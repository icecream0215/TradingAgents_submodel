"""
æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨
åŸºäºä»»åŠ¡ç‰¹å¾ã€æ€§èƒ½éœ€æ±‚å’Œå†å²æ•°æ®è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹
"""

import os
import json
import time
from typing import Any, Dict, List, Optional, Tuple
from enum import Enum
from dataclasses import dataclass, asdict
from pathlib import Path

from .multi_model_adapter import TaskType, MODEL_CONFIGURATIONS, ModelCapability
from tradingagents.utils.logging_manager import get_logger

logger = get_logger('agents')


@dataclass
class TaskCharacteristics:
    """ä»»åŠ¡ç‰¹å¾"""
    task_type: TaskType
    complexity: int  # 1-5å¤æ‚åº¦
    urgency: int     # 1-5ç´§æ€¥åº¦
    quality_requirement: int  # 1-5è´¨é‡è¦æ±‚
    expected_length: int      # é¢„æœŸè¾“å‡ºé•¿åº¦
    requires_reasoning: bool
    requires_creativity: bool
    requires_precision: bool
    context_length_needed: int


@dataclass
class ModelPerformance:
    """æ¨¡å‹æ€§èƒ½è®°å½•"""
    model_name: str
    task_type: TaskType
    avg_response_time: float
    success_rate: float
    quality_score: float
    user_satisfaction: float
    usage_count: int
    last_updated: float


class IntelligentModelSelector:
    """æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨"""
    
    def __init__(self, performance_cache_file: str = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½é€‰æ‹©å™¨
        
        Args:
            performance_cache_file: æ€§èƒ½ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        self.cache_file = performance_cache_file or os.path.join(
            os.path.dirname(__file__), "model_performance_cache.json"
        )
        self.performance_history: Dict[str, ModelPerformance] = {}
        self.load_performance_history()
    
    def load_performance_history(self):
        """åŠ è½½æ€§èƒ½å†å²æ•°æ®"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                for key, value in data.items():
                    self.performance_history[key] = ModelPerformance(**value)
                    
                logger.info(f"âœ… åŠ è½½äº† {len(self.performance_history)} æ¡æ€§èƒ½è®°å½•")
            else:
                logger.info("ğŸ”„ åˆ›å»ºæ–°çš„æ€§èƒ½ç¼“å­˜")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ€§èƒ½å†å²å¤±è´¥: {e}")
    
    def save_performance_history(self):
        """ä¿å­˜æ€§èƒ½å†å²æ•°æ®"""
        try:
            os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)
            
            data = {}
            for key, performance in self.performance_history.items():
                data[key] = asdict(performance)
            
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
            logger.debug(f"ğŸ’¾ ä¿å­˜äº† {len(self.performance_history)} æ¡æ€§èƒ½è®°å½•")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ€§èƒ½å†å²å¤±è´¥: {e}")
    
    def analyze_task_characteristics(
        self, 
        task_description: str,
        task_type: TaskType = None,
        context_length: int = None
    ) -> TaskCharacteristics:
        """
        åˆ†æä»»åŠ¡ç‰¹å¾
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¦‚æœæœªæŒ‡å®šåˆ™è‡ªåŠ¨æ¨æ–­ï¼‰
            context_length: ä¸Šä¸‹æ–‡é•¿åº¦
            
        Returns:
            ä»»åŠ¡ç‰¹å¾å¯¹è±¡
        """
        
        # è‡ªåŠ¨æ¨æ–­ä»»åŠ¡ç±»å‹
        if task_type is None:
            task_type = self._infer_task_type(task_description)
        
        # åˆ†æå¤æ‚åº¦
        complexity = self._analyze_complexity(task_description)
        
        # åˆ†æç´§æ€¥åº¦
        urgency = self._analyze_urgency(task_description)
        
        # åˆ†æè´¨é‡è¦æ±‚
        quality_requirement = self._analyze_quality_requirement(task_description)
        
        # åˆ†æé¢„æœŸè¾“å‡ºé•¿åº¦
        expected_length = self._estimate_output_length(task_description)
        
        # åˆ†æç‰¹æ®Šè¦æ±‚
        requires_reasoning = self._requires_reasoning(task_description)
        requires_creativity = self._requires_creativity(task_description)
        requires_precision = self._requires_precision(task_description)
        
        # ä¼°ç®—ä¸Šä¸‹æ–‡é•¿åº¦éœ€æ±‚
        context_length_needed = context_length or self._estimate_context_length(task_description)
        
        return TaskCharacteristics(
            task_type=task_type,
            complexity=complexity,
            urgency=urgency,
            quality_requirement=quality_requirement,
            expected_length=expected_length,
            requires_reasoning=requires_reasoning,
            requires_creativity=requires_creativity,
            requires_precision=requires_precision,
            context_length_needed=context_length_needed
        )
    
    def _infer_task_type(self, description: str) -> TaskType:
        """æ¨æ–­ä»»åŠ¡ç±»å‹"""
        description_lower = description.lower()
        
        # å…³é”®è¯æ˜ å°„
        keywords = {
            TaskType.CODING: ['ä»£ç ', 'ç¼–ç¨‹', 'å‡½æ•°', 'ç®—æ³•', 'code', 'function', 'programming', 'å¼€å‘'],
            TaskType.REASONING: ['åˆ†æ', 'æ¨ç†', 'é€»è¾‘', 'åˆ¤æ–­', 'analysis', 'reasoning', 'logic'],
            TaskType.THINKING: ['æ€è€ƒ', 'æ·±å…¥', 'å¤æ‚', 'å¤šè§’åº¦', 'thinking', 'complex', 'deep'],
            TaskType.FINANCIAL: ['è‚¡ç¥¨', 'æŠ•èµ„', 'è´¢åŠ¡', 'é‡‘è', 'åˆ†æ', 'stock', 'investment', 'financial'],
            TaskType.CONVERSATION: ['å¯¹è¯', 'èŠå¤©', 'äº¤æµ', 'chat', 'conversation', 'discuss'],
            TaskType.SPEED: ['å¿«é€Ÿ', 'ç´§æ€¥', 'ç«‹å³', 'quick', 'fast', 'urgent', 'immediate']
        }
        
        scores = {}
        for task_type, words in keywords.items():
            score = sum(1 for word in words if word in description_lower)
            if score > 0:
                scores[task_type] = score
        
        if scores:
            return max(scores, key=scores.get)
        
        return TaskType.GENERAL
    
    def _analyze_complexity(self, description: str) -> int:
        """åˆ†æä»»åŠ¡å¤æ‚åº¦ (1-5)"""
        complexity_indicators = {
            5: ['éå¸¸å¤æ‚', 'å¤šæ­¥éª¤', 'æ·±å…¥åˆ†æ', 'very complex', 'multi-step', 'comprehensive'],
            4: ['å¤æ‚', 'è¯¦ç»†', 'å…¨é¢', 'complex', 'detailed', 'thorough'],
            3: ['ä¸­ç­‰', 'æ ‡å‡†', 'moderate', 'standard'],
            2: ['ç®€å•', 'åŸºç¡€', 'simple', 'basic'],
            1: ['éå¸¸ç®€å•', 'å¿«é€Ÿ', 'very simple', 'quick']
        }
        
        description_lower = description.lower()
        for level, indicators in complexity_indicators.items():
            if any(indicator in description_lower for indicator in indicators):
                return level
        
        # æ ¹æ®æè¿°é•¿åº¦æ¨æ–­å¤æ‚åº¦
        if len(description) > 500:
            return 4
        elif len(description) > 200:
            return 3
        elif len(description) > 50:
            return 2
        else:
            return 1
    
    def _analyze_urgency(self, description: str) -> int:
        """åˆ†æä»»åŠ¡ç´§æ€¥åº¦ (1-5)"""
        urgency_indicators = {
            5: ['ç´§æ€¥', 'ç«‹å³', 'é©¬ä¸Š', 'urgent', 'immediate', 'asap'],
            4: ['å°½å¿«', 'å¿«é€Ÿ', 'quickly', 'fast'],
            3: ['åŠæ—¶', 'æ­£å¸¸', 'timely', 'normal'],
            2: ['ä¸æ€¥', 'æ…¢æ…¢', 'no rush'],
            1: ['ä¸ç´§æ€¥', 'æœ‰æ—¶é—´', 'not urgent']
        }
        
        description_lower = description.lower()
        for level, indicators in urgency_indicators.items():
            if any(indicator in description_lower for indicator in indicators):
                return level
        
        return 3  # é»˜è®¤ä¸­ç­‰ç´§æ€¥åº¦
    
    def _analyze_quality_requirement(self, description: str) -> int:
        """åˆ†æè´¨é‡è¦æ±‚ (1-5)"""
        quality_indicators = {
            5: ['é«˜è´¨é‡', 'ç²¾ç¡®', 'ä¸“ä¸š', 'high quality', 'precise', 'professional'],
            4: ['è´¨é‡å¥½', 'å‡†ç¡®', 'good quality', 'accurate'],
            3: ['æ ‡å‡†', 'æ­£å¸¸', 'standard', 'normal'],
            2: ['åŸºæœ¬', 'å¤Ÿç”¨', 'basic', 'sufficient'],
            1: ['ç®€å•', 'å¿«é€Ÿ', 'simple', 'quick']
        }
        
        description_lower = description.lower()
        for level, indicators in quality_indicators.items():
            if any(indicator in description_lower for indicator in indicators):
                return level
        
        return 3  # é»˜è®¤ä¸­ç­‰è´¨é‡è¦æ±‚
    
    def _estimate_output_length(self, description: str) -> int:
        """ä¼°ç®—é¢„æœŸè¾“å‡ºé•¿åº¦"""
        length_indicators = {
            4000: ['è¯¦ç»†æŠ¥å‘Š', 'å…¨é¢åˆ†æ', 'detailed report', 'comprehensive analysis'],
            2000: ['è¯¦ç»†', 'å®Œæ•´', 'detailed', 'complete'],
            1000: ['æ ‡å‡†', 'æ­£å¸¸', 'standard', 'normal'],
            500: ['ç®€çŸ­', 'æ¦‚è¦', 'brief', 'summary'],
            200: ['å¾ˆçŸ­', 'å¿«é€Ÿ', 'very short', 'quick']
        }
        
        description_lower = description.lower()
        for length, indicators in length_indicators.items():
            if any(indicator in description_lower for indicator in indicators):
                return length
        
        # æ ¹æ®æè¿°é•¿åº¦æ¨æ–­æœŸæœ›è¾“å‡ºé•¿åº¦
        return min(2000, max(500, len(description) * 3))
    
    def _requires_reasoning(self, description: str) -> bool:
        """æ˜¯å¦éœ€è¦æ¨ç†èƒ½åŠ›"""
        reasoning_keywords = [
            'åˆ†æ', 'æ¨ç†', 'åˆ¤æ–­', 'æ¯”è¾ƒ', 'è¯„ä¼°', 
            'analysis', 'reasoning', 'evaluate', 'compare', 'assess'
        ]
        description_lower = description.lower()
        return any(keyword in description_lower for keyword in reasoning_keywords)
    
    def _requires_creativity(self, description: str) -> bool:
        """æ˜¯å¦éœ€è¦åˆ›é€ åŠ›"""
        creativity_keywords = [
            'åˆ›æ„', 'åˆ›æ–°', 'è®¾è®¡', 'æƒ³è±¡', 
            'creative', 'innovative', 'design', 'imagine'
        ]
        description_lower = description.lower()
        return any(keyword in description_lower for keyword in creativity_keywords)
    
    def _requires_precision(self, description: str) -> bool:
        """æ˜¯å¦éœ€è¦ç²¾ç¡®æ€§"""
        precision_keywords = [
            'ç²¾ç¡®', 'å‡†ç¡®', 'å…·ä½“', 'æ•°å­—', 'è®¡ç®—',
            'precise', 'accurate', 'specific', 'calculation'
        ]
        description_lower = description.lower()
        return any(keyword in description_lower for keyword in precision_keywords)
    
    def _estimate_context_length(self, description: str) -> int:
        """ä¼°ç®—ä¸Šä¸‹æ–‡é•¿åº¦éœ€æ±‚"""
        if len(description) > 2000:
            return 16384
        elif len(description) > 1000:
            return 8192
        elif len(description) > 500:
            return 4096
        else:
            return 2048
    
    def select_optimal_model(
        self,
        task_characteristics: TaskCharacteristics,
        exclude_models: List[str] = None,
        consider_history: bool = True
    ) -> Tuple[str, float]:
        """
        é€‰æ‹©æœ€ä¼˜æ¨¡å‹
        
        Args:
            task_characteristics: ä»»åŠ¡ç‰¹å¾
            exclude_models: æ’é™¤çš„æ¨¡å‹åˆ—è¡¨
            consider_history: æ˜¯å¦è€ƒè™‘å†å²æ€§èƒ½
            
        Returns:
            (æ¨¡å‹åç§°, åŒ¹é…åˆ†æ•°)
        """
        exclude_models = exclude_models or []
        
        model_scores = {}
        
        for model_name, config in MODEL_CONFIGURATIONS.items():
            if model_name in exclude_models:
                continue
            
            score = self._calculate_model_score(config, task_characteristics, consider_history)
            model_scores[model_name] = score
        
        if not model_scores:
            return "qwen-instruct", 0.0  # é»˜è®¤æ¨¡å‹
        
        best_model = max(model_scores, key=model_scores.get)
        best_score = model_scores[best_model]
        
        logger.info(f"ğŸ¯ æ™ºèƒ½é€‰æ‹©: {best_model} (åˆ†æ•°: {best_score:.2f})")
        return best_model, best_score
    
    def _calculate_model_score(
        self,
        config: ModelCapability,
        characteristics: TaskCharacteristics,
        consider_history: bool
    ) -> float:
        """è®¡ç®—æ¨¡å‹é€‚é…åˆ†æ•°"""
        
        score = 0.0
        
        # 1. ä»»åŠ¡ç±»å‹åŒ¹é… (æƒé‡: 30%)
        if characteristics.task_type in config.task_strengths:
            score += 30.0
        elif TaskType.GENERAL in config.task_strengths:
            score += 15.0
        
        # 2. è´¨é‡vsé€Ÿåº¦å¹³è¡¡ (æƒé‡: 25%)
        if characteristics.urgency >= 4:  # é«˜ç´§æ€¥åº¦ï¼Œä¼˜å…ˆé€Ÿåº¦
            score += config.speed_score * 2.5
        elif characteristics.quality_requirement >= 4:  # é«˜è´¨é‡è¦æ±‚
            score += config.quality_score * 2.5
        else:  # å¹³è¡¡
            score += (config.quality_score + config.speed_score) * 1.25
        
        # 3. å¤æ‚åº¦åŒ¹é… (æƒé‡: 20%)
        if characteristics.complexity >= 4 and config.quality_score >= 8.5:
            score += 20.0
        elif characteristics.complexity <= 2 and config.speed_score >= 8.0:
            score += 20.0
        elif characteristics.complexity == 3:
            score += 15.0
        
        # 4. ä¸Šä¸‹æ–‡é•¿åº¦è¦æ±‚ (æƒé‡: 10%)
        if characteristics.context_length_needed <= config.context_length:
            score += 10.0
        else:
            score += 5.0  # éƒ¨åˆ†å‡åˆ†
        
        # 5. ç‰¹æ®Šèƒ½åŠ›è¦æ±‚ (æƒé‡: 10%)
        special_score = 0.0
        if characteristics.requires_reasoning and characteristics.task_type == TaskType.REASONING:
            special_score += 3.0
        if characteristics.requires_precision and config.quality_score >= 9.0:
            special_score += 3.0
        if characteristics.requires_creativity and config.model_id.startswith("Qwen"):
            special_score += 2.0
        score += min(10.0, special_score)
        
        # 6. å†å²æ€§èƒ½ (æƒé‡: 5%)
        if consider_history:
            history_key = f"{config.name}_{characteristics.task_type.value}"
            if history_key in self.performance_history:
                perf = self.performance_history[history_key]
                history_score = (perf.success_rate * 2 + perf.quality_score + perf.user_satisfaction) / 4
                score += history_score * 5.0
        
        return score
    
    def record_performance(
        self,
        model_name: str,
        task_type: TaskType,
        response_time: float,
        success: bool,
        quality_score: float = None,
        user_satisfaction: float = None
    ):
        """è®°å½•æ¨¡å‹æ€§èƒ½"""
        
        history_key = f"{model_name}_{task_type.value}"
        
        if history_key in self.performance_history:
            perf = self.performance_history[history_key]
            
            # æ›´æ–°å¹³å‡å€¼
            perf.avg_response_time = (perf.avg_response_time * perf.usage_count + response_time) / (perf.usage_count + 1)
            perf.success_rate = (perf.success_rate * perf.usage_count + (1.0 if success else 0.0)) / (perf.usage_count + 1)
            
            if quality_score is not None:
                perf.quality_score = (perf.quality_score * perf.usage_count + quality_score) / (perf.usage_count + 1)
            
            if user_satisfaction is not None:
                perf.user_satisfaction = (perf.user_satisfaction * perf.usage_count + user_satisfaction) / (perf.usage_count + 1)
            
            perf.usage_count += 1
            perf.last_updated = time.time()
        else:
            # åˆ›å»ºæ–°è®°å½•
            self.performance_history[history_key] = ModelPerformance(
                model_name=model_name,
                task_type=task_type,
                avg_response_time=response_time,
                success_rate=1.0 if success else 0.0,
                quality_score=quality_score or 5.0,
                user_satisfaction=user_satisfaction or 5.0,
                usage_count=1,
                last_updated=time.time()
            )
        
        # å®šæœŸä¿å­˜
        if len(self.performance_history) % 10 == 0:
            self.save_performance_history()
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        report = {
            "total_records": len(self.performance_history),
            "by_model": {},
            "by_task": {},
            "top_performers": {
                "speed": [],
                "quality": [],
                "reliability": []
            }
        }
        
        # æŒ‰æ¨¡å‹ç»Ÿè®¡
        model_stats = {}
        task_stats = {}
        
        for key, perf in self.performance_history.items():
            # æŒ‰æ¨¡å‹
            if perf.model_name not in model_stats:
                model_stats[perf.model_name] = {
                    "usage_count": 0,
                    "avg_response_time": 0.0,
                    "success_rate": 0.0,
                    "quality_score": 0.0
                }
            
            stats = model_stats[perf.model_name]
            stats["usage_count"] += perf.usage_count
            stats["avg_response_time"] = (stats["avg_response_time"] + perf.avg_response_time) / 2
            stats["success_rate"] = (stats["success_rate"] + perf.success_rate) / 2
            stats["quality_score"] = (stats["quality_score"] + perf.quality_score) / 2
            
            # æŒ‰ä»»åŠ¡ç±»å‹
            task_name = perf.task_type.value
            if task_name not in task_stats:
                task_stats[task_name] = {
                    "usage_count": 0,
                    "best_model": "",
                    "best_score": 0.0
                }
            
            task_stats[task_name]["usage_count"] += perf.usage_count
            combined_score = (perf.success_rate + perf.quality_score + perf.user_satisfaction) / 3
            if combined_score > task_stats[task_name]["best_score"]:
                task_stats[task_name]["best_model"] = perf.model_name
                task_stats[task_name]["best_score"] = combined_score
        
        report["by_model"] = model_stats
        report["by_task"] = task_stats
        
        # é¡¶çº§è¡¨ç°è€…
        all_performances = list(self.performance_history.values())
        
        # é€Ÿåº¦æœ€å¿«
        speed_sorted = sorted(all_performances, key=lambda x: x.avg_response_time)
        report["top_performers"]["speed"] = [
            {"model": p.model_name, "task": p.task_type.value, "time": p.avg_response_time}
            for p in speed_sorted[:5]
        ]
        
        # è´¨é‡æœ€é«˜
        quality_sorted = sorted(all_performances, key=lambda x: x.quality_score, reverse=True)
        report["top_performers"]["quality"] = [
            {"model": p.model_name, "task": p.task_type.value, "score": p.quality_score}
            for p in quality_sorted[:5]
        ]
        
        # å¯é æ€§æœ€é«˜
        reliability_sorted = sorted(all_performances, key=lambda x: x.success_rate, reverse=True)
        report["top_performers"]["reliability"] = [
            {"model": p.model_name, "task": p.task_type.value, "rate": p.success_rate}
            for p in reliability_sorted[:5]
        ]
        
        return report


# å…¨å±€æ™ºèƒ½é€‰æ‹©å™¨å®ä¾‹
_global_selector = None

def get_intelligent_selector() -> IntelligentModelSelector:
    """è·å–å…¨å±€æ™ºèƒ½é€‰æ‹©å™¨å®ä¾‹"""
    global _global_selector
    if _global_selector is None:
        _global_selector = IntelligentModelSelector()
    return _global_selector


def smart_select_model(
    task_description: str,
    task_type: TaskType = None,
    priority: str = "balanced",
    context_length: int = None
) -> str:
    """
    æ™ºèƒ½é€‰æ‹©æ¨¡å‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        task_description: ä»»åŠ¡æè¿°
        task_type: ä»»åŠ¡ç±»å‹
        priority: ä¼˜å…ˆçº§
        context_length: ä¸Šä¸‹æ–‡é•¿åº¦
        
    Returns:
        æœ€ä½³æ¨¡å‹åç§°
    """
    
    selector = get_intelligent_selector()
    
    # åˆ†æä»»åŠ¡ç‰¹å¾
    characteristics = selector.analyze_task_characteristics(
        task_description, task_type, context_length
    )
    
    # é€‰æ‹©æœ€ä¼˜æ¨¡å‹
    model_name, score = selector.select_optimal_model(characteristics)
    
    logger.info(f"ğŸ§  æ™ºèƒ½é€‰æ‹©ç»“æœ: {model_name} (åŒ¹é…åº¦: {score:.1f}%)")
    return model_name


def test_intelligent_selector():
    """æµ‹è¯•æ™ºèƒ½é€‰æ‹©å™¨"""
    logger.info("ğŸ§  æµ‹è¯•æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨")
    logger.info("=" * 50)
    
    selector = IntelligentModelSelector()
    
    test_cases = [
        ("è¯·å¸®æˆ‘å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—", None),
        ("æˆ‘éœ€è¦å¯¹è‹¹æœå…¬å¸çš„è‚¡ç¥¨è¿›è¡Œæ·±å…¥çš„è´¢åŠ¡åˆ†æ", None),
        ("è¯·å¿«é€Ÿå›ç­”ï¼šä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ", None),
        ("è¯·è¯¦ç»†åˆ†æäººå·¥æ™ºèƒ½å¯¹æœªæ¥ç¤¾ä¼šçš„å½±å“ï¼Œéœ€è¦æ·±å…¥æ€è€ƒ", None),
        ("å¸®æˆ‘debugè¿™æ®µä»£ç çš„é—®é¢˜", TaskType.CODING),
        ("åˆ†æä¸€ä¸‹æ¯”ç‰¹å¸ä»·æ ¼çš„èµ°åŠ¿å’ŒæŠ•èµ„å»ºè®®", TaskType.FINANCIAL)
    ]
    
    for task_desc, task_type in test_cases:
        try:
            characteristics = selector.analyze_task_characteristics(task_desc, task_type)
            model_name, score = selector.select_optimal_model(characteristics)
            
            logger.info(f"ğŸ“ ä»»åŠ¡: {task_desc[:50]}...")
            logger.info(f"   ç±»å‹: {characteristics.task_type.value}")
            logger.info(f"   å¤æ‚åº¦: {characteristics.complexity}/5")
            logger.info(f"   é€‰æ‹©: {model_name} (åˆ†æ•°: {score:.1f})")
            logger.info("")
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    report = selector.get_performance_report()
    logger.info(f"ğŸ“Š æ€§èƒ½æŠ¥å‘Š: å…± {report['total_records']} æ¡è®°å½•")


if __name__ == "__main__":
    test_intelligent_selector()