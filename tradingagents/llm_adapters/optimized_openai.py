#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„OpenAIé€‚é…å™¨ - è§£å†³500é”™è¯¯å’Œ"è¯•é”™"é—®é¢˜
ç›´æ¥ä½¿ç”¨æœ€ä½³å®ç°ï¼Œé¿å…LangChainæµå¼å¤„ç†é—®é¢˜
"""

from tradingagents.llm_adapters.third_party_openai import ThirdPartyOpenAI
from tradingagents.utils.logging_manager import get_logger
from datetime import datetime

logger = get_logger("optimized_openai")

class OptimizedOpenAI(ThirdPartyOpenAI):
    """
    ä¼˜åŒ–çš„OpenAIé€‚é…å™¨ - è§£å†³å…³é”®é—®é¢˜ï¼š
    1. é¿å…500é”™è¯¯ï¼šä¸å†ä½¿ç”¨LangChainçš„"è¯•é”™"æœºåˆ¶
    2. èŠ‚çœæ—¶é—´ï¼šç›´æ¥ä½¿ç”¨æœ€ä½³å®ç°
    3. æ›´ç¨³å®šï¼šé»˜è®¤éæµå¼æ¨¡å¼
    4. å‡†ç¡®ç»Ÿè®¡ï¼šä¿æŒtokenç»Ÿè®¡åŠŸèƒ½
    """
    
    def __init__(self, *args, **kwargs):
        """åˆå§‹åŒ–ä¼˜åŒ–é€‚é…å™¨ï¼Œé»˜è®¤ä½¿ç”¨æ›´ç¨³å®šçš„éæµå¼æ¨¡å¼"""
        # é»˜è®¤è®¾ä¸ºéæµå¼æ¨¡å¼ï¼ˆæ›´ç¨³å®šï¼Œé¿å…500é”™è¯¯ï¼‰
        kwargs.setdefault('streaming', False)
        
        super().__init__(*args, **kwargs)
        logger.info("âœ… ä¼˜åŒ–OpenAIé€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        """
        ğŸ¯ æ ¸å¿ƒä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨æœ€ä½³å®ç°
        
        åŸæ¥çš„æµç¨‹ï¼ˆæœ‰é—®é¢˜ï¼‰ï¼š
        1. å°è¯• super()._generate() [LangChainæ ‡å‡†æ–¹æ³•]
        2. å¤±è´¥ â†’ 500é”™è¯¯
        3. åˆ‡æ¢åˆ° self._direct_api_call() [æˆ‘ä»¬çš„æœ€ä½³å®ç°]
        
        ä¼˜åŒ–åçš„æµç¨‹ï¼š
        1. ç›´æ¥ä½¿ç”¨ self._direct_api_call() [æœ€ä½³å®ç°]
        2. æˆåŠŸ âœ…
        """
        
        # ä¿å­˜ä¼šè¯ä¿¡æ¯ç”¨äºtokenè·Ÿè¸ª
        session_id = kwargs.pop('session_id', None)
        analysis_type = kwargs.pop('analysis_type', 'stock_analysis')
        
        try:
            # ğŸ¯ æ ¸å¿ƒæ”¹è¿›ï¼šç›´æ¥ä½¿ç”¨æˆ‘ä»¬çš„_direct_api_callæ–¹æ³•
            # é¿å…LangChainæ ‡å‡†æ–¹æ³•çš„500é”™è¯¯é—®é¢˜
            
            stream_mode = kwargs.get('stream', getattr(self, 'streaming', False))
            
            logger.info(f"ğŸš€ ç›´æ¥APIè°ƒç”¨ (stream={stream_mode}, è·³è¿‡LangChainè¯•é”™)")
            
            # ç›´æ¥è°ƒç”¨æœ€ä½³å®ç°ï¼Œæ— éœ€"è¯•é”™"
            result = self._direct_api_call(messages, stream=stream_mode)
            
            # Tokenè·Ÿè¸ªï¼ˆä½¿ç”¨ç°æœ‰æœºåˆ¶ï¼‰
            self._track_optimized_usage(session_id, analysis_type)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–é€‚é…å™¨è°ƒç”¨å¤±è´¥: {e}")
            return self._create_error_response(f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
    
    def _track_optimized_usage(self, session_id, analysis_type):
        """ä¼˜åŒ–çš„tokenä½¿ç”¨è·Ÿè¸ª"""
        try:
            if hasattr(self, '_last_api_usage') and self._last_api_usage:
                usage = self._last_api_usage
                input_tokens = usage.get('prompt_tokens', 0)
                output_tokens = usage.get('completion_tokens', 0)
                
                if input_tokens > 0 or output_tokens > 0:
                    # ä½¿ç”¨ç°æœ‰çš„tokenè·Ÿè¸ªæœºåˆ¶
                    effective_session_id = (
                        session_id or 
                        getattr(self, 'session_id', None) or 
                        f"opt_{hash(datetime.now())%10000}"
                    )
                    
                    from tradingagents.config.config_manager import token_tracker
                    
                    token_tracker.track_usage(
                        provider="optimized_openai",
                        model_name=getattr(self, 'model_name', 'unknown'),
                        input_tokens=input_tokens,
                        output_tokens=output_tokens,
                        session_id=effective_session_id,
                        analysis_type=analysis_type
                    )
                    
                    logger.info(f"ğŸ“Š Tokenç»Ÿè®¡: {input_tokens}+{output_tokens}={input_tokens+output_tokens}")
                    
        except Exception as e:
            logger.warning(f"âš ï¸ Tokenè·Ÿè¸ªå¼‚å¸¸: {e}")

# ä¾¿æ·åˆ›å»ºå‡½æ•°
def create_optimized_llm(**kwargs):
    """
    åˆ›å»ºä¼˜åŒ–çš„LLMå®ä¾‹çš„ä¾¿æ·å‡½æ•°
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    llm = create_optimized_llm(
        model_name='Qwen/Qwen3-235B-A22B-Instruct-2507',
        temperature=0.7,
        streaming=False  # æ¨èï¼šéæµå¼æ›´ç¨³å®š
    )
    """
    import os
    
    defaults = {
        'api_key': os.getenv('OPENAI_API_KEY'),
        'base_url': os.getenv('OPENAI_API_BASE', 'https://llm.submodel.ai/v1'),
        'model': kwargs.pop('model_name', os.getenv('OPENAI_MODEL_NAME', 'Qwen/Qwen3-235B-A22B-Instruct-2507')),
        'temperature': 0.7,
        'streaming': False,  # é»˜è®¤éæµå¼ï¼Œæ›´ç¨³å®š
        'max_tokens': 2000
    }
    
    defaults.update(kwargs)
    
    return OptimizedOpenAI(**defaults)

if __name__ == "__main__":
    """ç®€å•æµ‹è¯•ä¼˜åŒ–é€‚é…å™¨"""
    import os
    from dotenv import load_dotenv
    from langchain_core.messages import HumanMessage
    
    load_dotenv()
    
    print("ğŸ¯ æµ‹è¯•ä¼˜åŒ–OpenAIé€‚é…å™¨")
    
    # åˆ›å»ºä¼˜åŒ–çš„LLMå®ä¾‹
    llm = create_optimized_llm(streaming=False)
    
    # æµ‹è¯•è°ƒç”¨
    messages = [HumanMessage(content="ç®€è¦åˆ†ææ¯”ç‰¹å¸è¶‹åŠ¿ï¼Œä¸è¶…è¿‡20å­—")]
    
    start_time = datetime.now()
    result = llm._generate(messages)
    duration = (datetime.now() - start_time).total_seconds()
    
    if result and result.generations:
        response = result.generations[0].message.content
        print(f"âœ… ä¼˜åŒ–é€‚é…å™¨æˆåŠŸ ({duration:.2f}ç§’)")
        print(f"ğŸ“„ å“åº”: {response}")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥")