import os
import json
from typing import Any, Dict, List, Optional, Union
from functools import lru_cache
from pydantic import Field

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯å¯¼å…¥é—®é¢˜
try:
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import BaseMessage
    from langchain_core.outputs import ChatResult
    from langchain_core.callbacks import CallbackManagerForLLMRun
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨å ä½ç¬¦ç±»
    class ChatOpenAI:
        pass

    class BaseMessage:
        pass

    class ChatResult:
        pass

    class CallbackManagerForLLMRun:
        pass

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')

# å¯¼å…¥tokenè·Ÿè¸ªå™¨
try:
    from tradingagents.config.config_manager import token_tracker
    TOKEN_TRACKING_ENABLED = True
    logger.info("âœ… Tokenè·Ÿè¸ªåŠŸèƒ½å·²å¯ç”¨")
except ImportError:
    TOKEN_TRACKING_ENABLED = False
    logger.warning("âš ï¸ Tokenè·Ÿè¸ªåŠŸèƒ½æœªå¯ç”¨")


class ThirdPartyOpenAI(ChatOpenAI):
    """
    ç¬¬ä¸‰æ–¹OpenAIæœåŠ¡é€‚é…å™¨

    ä¸“é—¨å¤„ç†ç¬¬ä¸‰æ–¹OpenAIå…¼å®¹æœåŠ¡çš„å…¼å®¹æ€§é—®é¢˜ï¼Œ
    é€šè¿‡è¿‡æ»¤å’Œä¼˜åŒ–è¯·æ±‚å‚æ•°æ¥é¿å…500é”™è¯¯
    """

    # æ·»åŠ session_idä½œä¸ºPydanticå­—æ®µ
    session_id: Optional[str] = Field(default=None, description="ä¼šè¯IDç”¨äºtokenè·Ÿè¸ª")
    
    # ğŸ”§ æ·»åŠ æµå¼é…ç½®å­—æ®µï¼Œè§£å†³Pydanticå±æ€§é—®é¢˜ï¼ˆä¸èƒ½ä»¥ä¸‹åˆ’çº¿å¼€å¤´ï¼‰
    user_wants_streaming: bool = Field(default=False, description="ç”¨æˆ·æ˜¯å¦éœ€è¦æµå¼å“åº”")
    user_defined_model: Optional[str] = Field(default=None, description="ç”¨æˆ·å®šä¹‰çš„æ¨¡å‹å")
    
    # æ·»åŠ æ¨¡å‹å‚æ•°ç¼“å­˜
    _model_param_cache = {}
    
    def __init__(
        self,
        model: str = "gpt-3.5-turbo",
        api_key: Optional[str] = None,
        base_url: str = "https://llm.submodel.ai/v1",
        temperature: float = 0.1,
        max_tokens: Optional[int] = 2000,
        session_id: Optional[str] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–ç¬¬ä¸‰æ–¹OpenAIé€‚é…å™¨
        
        Args:
            model: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            session_id: ä¼šè¯IDç”¨äºtokenè·Ÿè¸ª
            **kwargs: å…¶ä»–å‚æ•°
        """
        
        # ğŸ”§ ä¿®å¤æµå¼å‚æ•°å†²çªé—®é¢˜
        # å¤„ç†LangChainçš„streamå‚æ•°è½¬ç§»é—®é¢˜
        default_streaming = False  # æ”¹ä¸ºFalseï¼Œä¼˜å…ˆè·å–å‡†ç¡®tokenæ•°æ®
        user_streaming = kwargs.get('streaming', default_streaming)
        
        # ğŸ”‘ é‡è¦ï¼šæ­£ç¡®å¤„ç†streamå‚æ•°ï¼ˆé¿å…ä¸LangChainçš„streamæ–¹æ³•å†²çªï¼‰
        user_stream = None
        if 'stream' in kwargs:
            user_stream = kwargs.get('stream', default_streaming)
            # åªæœ‰å½“streamæ˜¯å¸ƒå°”å€¼æ—¶æ‰è®¤ä¸ºæ˜¯æµå¼å‚æ•°
            if not isinstance(user_stream, bool):
                user_stream = default_streaming
        else:
            user_stream = default_streaming
        
        # ç§»é™¤å¯èƒ½å†²çªçš„å‚æ•°ï¼ˆé¿å…ä¼ é€’ç»™çˆ¶ç±»ï¼‰
        kwargs.pop('streaming', None)
        kwargs.pop('stream', None)
        
        # æ·»åŠ session_idä½œä¸ºPydanticå­—æ®µ
        super().__init__(
            model=model,
            api_key=api_key,
            base_url=base_url,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=120,
            max_retries=2,
            streaming=user_streaming,  # ä½¿ç”¨å¤„ç†åçš„æµå¼å‚æ•°
            # ğŸ”§ é‡è¦ï¼šä¸ä¼ é€’streamç»™çˆ¶ç±»ï¼Œé¿å…LangChainè­¦å‘Š
            session_id=session_id,
            user_wants_streaming=user_streaming or user_stream,  # ğŸ”§ é€šè¿‡æ„é€ å‡½æ•°è®¾ç½®
            user_defined_model=model,  # ğŸ”§ é€šè¿‡æ„é€ å‡½æ•°è®¾ç½®
            **kwargs  # ä¼ é€’å…¶ä»–å‚æ•°ï¼Œä½†å·²ç»ç§»é™¤äº†å†²çªçš„å‚æ•°
        )
        
        # ğŸ”‘ é‡è¦ï¼šå¼ºåˆ¶è®¾ç½®model_nameä¸ºä¼ å…¥çš„modelå‚æ•°
        self.model_name = model
        # self.user_defined_model = model  # å·²åœ¨æ„é€ å‡½æ•°ä¸­è®¾ç½®
        
        # ğŸ”§ ä¿å­˜ç”¨æˆ·çš„æµå¼åå¥½ï¼ˆé¿å…ä¼ é€’ç»™çˆ¶ç±»å¼•èµ·è­¦å‘Šï¼‰
        self._user_stream_setting = user_stream
        
        # åˆå§‹åŒ–tokenä½¿ç”¨ä¿¡æ¯ä¸´æ—¶å­˜å‚¨
        self._last_api_usage = None
        
        logger.info(f"âœ… ç¬¬ä¸‰æ–¹OpenAIé€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"   æ¨¡å‹: {self.model_name}")  # æ˜¾ç¤ºå¼ºåˆ¶è®¾ç½®åçš„æ¨¡å‹å
        logger.info(f"   ç«¯ç‚¹: {base_url}")
        logger.info(f"   æµå¼æ¨¡å¼: {self.user_wants_streaming} (æ–¹æ¡ˆ3={'å¯ç”¨' if self.user_wants_streaming else 'ç¦ç”¨'})")
        # å®‰å…¨åœ°æ˜¾ç¤ºAPIå¯†é’¥ï¼ˆå¤„ç†SecretStrç±»å‹ï¼‰
        if api_key:
            if hasattr(api_key, 'get_secret_value'):
                key_display = api_key.get_secret_value()[:20] if api_key.get_secret_value() else 'None'
            else:
                key_display = str(api_key)[:20] if api_key else 'None'
        else:
            key_display = 'None'
        logger.info(f"   APIå¯†é’¥: {key_display}...")
        if session_id:
            logger.info(f"   ä¼šè¯ID: {session_id}")
    
    def _filter_safe_kwargs(self, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸å†è¿›è¡Œå‚æ•°è¿‡æ»¤ï¼Œç›´æ¥è¿”å›æ‰€æœ‰å‚æ•°
        
        Args:
            kwargs: åŸå§‹å‚æ•°
            
        Returns:
            åŸå§‹å‚æ•°ï¼ˆä¸è¿›è¡Œè¿‡æ»¤ï¼‰
        """
        
        # ä¸å†è¿›è¡Œå‚æ•°è¿‡æ»¤ï¼Œç›´æ¥è¿”å›æ‰€æœ‰å‚æ•°
        logger.debug(f"ğŸ”“ è·³è¿‡å‚æ•°è¿‡æ»¤ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰å‚æ•°: {list(kwargs.keys())}")
        return kwargs
    
    def _get_model_supported_params(self, model_name: str) -> Dict[str, Any]:
        """
        é€šè¿‡APIæŸ¥è¯¢æ¨¡å‹æ”¯æŒçš„å‚æ•°ï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            æ¨¡å‹æ”¯æŒçš„å‚æ•°å­—å…¸
        """
        # æ£€æŸ¥ç¼“å­˜
        if model_name in self._model_param_cache:
            return self._model_param_cache[model_name]
        
        try:
            # å®‰å…¨åœ°è·å–APIå¯†é’¥å’ŒåŸºç¡€URL
            api_key = self._get_api_key()
            base_url = self._get_base_url()
            
            # æ„é€ æ¨¡å‹ä¿¡æ¯æŸ¥è¯¢URL
            api_url = f"{base_url}/models/{model_name}" if "openai" in base_url.lower() else f"{base_url}/models"
            
            # è¯·æ±‚å¤´
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {api_key}'
            }
            
            import requests
            response = requests.get(api_url, headers=headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                # è§£ææ¨¡å‹æ”¯æŒçš„å‚æ•°
                supported_params = self._parse_model_params(data, model_name)
                # ç¼“å­˜ç»“æœ
                self._model_param_cache[model_name] = supported_params
                logger.debug(f"ğŸ“¥ è·å–æ¨¡å‹ {model_name} æ”¯æŒçš„å‚æ•°: {list(supported_params.keys())}")
                return supported_params
            else:
                logger.warning(f"âš ï¸ æ— æ³•è·å–æ¨¡å‹ {model_name} ä¿¡æ¯: {response.status_code}")
        except Exception as e:
            logger.warning(f"âš ï¸ æŸ¥è¯¢æ¨¡å‹å‚æ•°å¤±è´¥: {e}")
        
        # è¿”å›é»˜è®¤æ”¯æŒçš„å‚æ•°
        return self._get_default_supported_params(model_name)
    
    def _parse_model_params(self, model_data: Dict[str, Any], model_name: str) -> Dict[str, Any]:
        """
        è§£ææ¨¡å‹æ”¯æŒçš„å‚æ•°
        
        Args:
            model_data: æ¨¡å‹æ•°æ®
            model_name: æ¨¡å‹åç§°
            
        Returns:
            æ”¯æŒçš„å‚æ•°å­—å…¸
        """
        # é»˜è®¤æ”¯æŒçš„åŸºæœ¬å‚æ•°
        supported = {
            'model', 'messages', 'temperature', 'max_tokens',
            'top_p', 'frequency_penalty', 'presence_penalty'
        }
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´æ”¯æŒçš„å‚æ•°
        if 'deepseek' in model_name.lower():
            # DeepSeekæ”¯æŒçš„å‚æ•°
            supported = {'model', 'messages', 'temperature', 'max_tokens'}
        elif 'gpt' in model_name.lower():
            # OpenAI GPTç³»åˆ—æ”¯æŒæ›´å¤šå‚æ•°
            supported = {
                'model', 'messages', 'temperature', 'max_tokens',
                'top_p', 'frequency_penalty', 'presence_penalty',
                'stop', 'stream'
            }
        
        return {param: True for param in supported}
    
    def _clean_message_content(self, content: str) -> str:
        """
        æ¸…ç†æ¶ˆæ¯å†…å®¹ï¼Œç§»é™¤å¯èƒ½å¯¼è‡´tokenè§£æé”™è¯¯çš„å­—ç¬¦å’Œæ ¼å¼
        
        Args:
            content: åŸå§‹æ¶ˆæ¯å†…å®¹
            
        Returns:
            æ¸…ç†åçš„æ¶ˆæ¯å†…å®¹
        """
        if not isinstance(content, str):
            content = str(content)
        
        # ç§»é™¤å¯èƒ½å¯¼è‡´tokenè§£æé—®é¢˜çš„ç‰¹æ®Šå­—ç¬¦
        # 1. ç§»é™¤é›¶å®½åº¦å­—ç¬¦
        content = content.replace('\u200b', '')  # é›¶å®½åº¦ç©ºæ ¼
        content = content.replace('\ufeff', '')  # BOMå­—ç¬¦
        
        # 2. è§„èŒƒåŒ–æ¢è¡Œç¬¦
        content = content.replace('\r\n', '\n').replace('\r', '\n')
        
        # 3. ç§»é™¤è¿‡å¤šçš„è¿ç»­ç©ºç™½å­—ç¬¦
        import re
        content = re.sub(r'\n{3,}', '\n\n', content)  # æœ€å¤šä¿ç•™ä¸¤ä¸ªè¿ç»­æ¢è¡Œ
        content = re.sub(r'[ \t]{3,}', '  ', content)  # æœ€å¤šä¿ç•™ä¸¤ä¸ªè¿ç»­ç©ºæ ¼
        
        # 4. ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
        content = content.strip()
        if not content:
            content = "è¯·æä¾›åˆ†æå»ºè®®ã€‚"  # é»˜è®¤å†…å®¹
        
        # 5. é™åˆ¶å•ä¸ªæ¶ˆæ¯çš„æœ€å¤§é•¿åº¦ï¼ˆé¿å…è¶…é•¿è¾“å…¥ï¼‰
        max_length = 8000  # ä¿å®ˆçš„æœ€å¤§é•¿åº¦
        if len(content) > max_length:
            content = content[:max_length] + "...(å†…å®¹è¿‡é•¿å·²æˆªæ–­)"
            logger.warning(f"âš ï¸ æ¶ˆæ¯å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­åˆ°{max_length}å­—ç¬¦")
        
        return content
    
    def _aggressive_clean_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        æ¿€è¿›çš„æ¶ˆæ¯æ¸…ç†ï¼Œç”¨äºå¤„ç†tokenè§£æé”™è¯¯çš„é‡è¯•
        
        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            æ¿€è¿›æ¸…ç†åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        cleaned_messages = []
        
        for msg in messages:
            content = msg['content']
            
            # æ›´æ¿€è¿›çš„æ¸…ç†
            import re
            
            # 1. ç§»é™¤æ‰€æœ‰éASCIIå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ï¼‰
            content = re.sub(r'[^\u4e00-\u9fff\u3400-\u4dbf\u0020-\u007E\n]', '', content)
            
            # 2. ç§»é™¤markdownæ ¼å¼
            content = re.sub(r'\*\*([^*]+)\*\*', r'\1', content)  # ç²—ä½“
            content = re.sub(r'\*([^*]+)\*', r'\1', content)      # æ–œä½“
            content = re.sub(r'`([^`]+)`', r'\1', content)       # ä»£ç 
            content = re.sub(r'#+\s*', '', content)              # æ ‡é¢˜
            
            # 3. ç®€åŒ–æ ‡ç‚¹ç¬¦å·
            content = re.sub(r'[ã€‚ï¼ï¼Ÿ]{2,}', 'ã€‚', content)      # å¤šä¸ªå¥å·
            content = re.sub(r'[ï¼Œã€ï¼›ï¼š]{2,}', 'ï¼Œ', content)     # å¤šä¸ªé€—å·
            
            # 4. ç¡®ä¿åŸºæœ¬å†…å®¹
            content = content.strip()
            if not content:
                content = "è¯·åˆ†æ" if msg['role'] == 'user' else "å¥½çš„"
            
            # 5. é™åˆ¶é•¿åº¦
            if len(content) > 1000:
                content = content[:1000] + "..."
            
            cleaned_messages.append({
                'role': msg['role'],
                'content': content
            })
        
        return cleaned_messages
    
    def _get_default_supported_params(self, model_name: str) -> Dict[str, Any]:
        """
        è·å–é»˜è®¤æ”¯æŒçš„å‚æ•°ï¼ˆåŸºäºæ¨¡å‹åç§°ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            é»˜è®¤æ”¯æŒçš„å‚æ•°å­—å…¸
        """
        # åŸºç¡€å‚æ•°ï¼ˆæ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒï¼‰
        base_params = {'model', 'messages', 'temperature'}
        
        # æ ¹æ®æ¨¡å‹åç§°ç¡®å®šé¢å¤–æ”¯æŒçš„å‚æ•°
        if 'deepseek' in model_name.lower():
            # DeepSeekæ¨¡å‹è¾ƒä¸ºä¿å®ˆ
            extra_params = {'max_tokens'}
        elif 'gpt' in model_name.lower() or 'openai' in model_name.lower():
            # OpenAIç³»åˆ—æ¨¡å‹æ”¯æŒæ›´å¤šå‚æ•°
            extra_params = {'max_tokens', 'top_p', 'frequency_penalty', 'presence_penalty'}
        else:
            # å…¶ä»–æ¨¡å‹ä½¿ç”¨æœ€å°åŒ–å‚æ•°é›†
            extra_params = {'max_tokens'}
        
        all_params = base_params.union(extra_params)
        return {param: True for param in all_params}
    
    def _filter_model_kwargs(self, model_kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸å†è¿›è¡Œæ¨¡å‹å‚æ•°è¿‡æ»¤ï¼Œç›´æ¥è¿”å›æ‰€æœ‰å‚æ•°
        
        Args:
            model_kwargs: æ¨¡å‹å…³é”®å­—å‚æ•°
            
        Returns:
            åŸå§‹å‚æ•°ï¼ˆä¸è¿›è¡Œè¿‡æ»¤ï¼‰
        """
        if not model_kwargs:
            return {}
        
        # è·å–æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        model_name = getattr(self, 'model_name', '')
        
        logger.debug(f"ï¿½ è·³è¿‡æ¨¡å‹å‚æ•°è¿‡æ»¤: {model_name}")
        logger.debug(f"   ç›´æ¥ä½¿ç”¨æ‰€æœ‰å‚æ•°: {list(model_kwargs.keys())}")
        
        # ç›´æ¥è¿”å›æ‰€æœ‰å‚æ•°ï¼Œä¸è¿›è¡Œè¿‡æ»¤
        return model_kwargs
    
    def _filter_model_kwargs_predefined(self, model_kwargs: Dict[str, Any], model_name: str) -> Dict[str, Any]:
        """
        åºŸå¼ƒçš„æ–¹æ³• - ä¸å†è¿›è¡Œå‚æ•°è¿‡æ»¤
        ä¸ºäº†å‘åå…¼å®¹è€Œä¿ç•™ï¼Œç›´æ¥è¿”å›æ‰€æœ‰å‚æ•°
        
        Args:
            model_kwargs: æ¨¡å‹å…³é”®å­—å‚æ•°
            model_name: æ¨¡å‹åç§°
            
        Returns:
            åŸå§‹å‚æ•°ï¼ˆä¸è¿›è¡Œè¿‡æ»¤ï¼‰
        """
        logger.debug(f"ğŸ”“ åºŸå¼ƒçš„å‚æ•°è¿‡æ»¤æ–¹æ³•ï¼Œç›´æ¥è¿”å›æ‰€æœ‰å‚æ•°")
        return model_kwargs
    
    def _get_api_key(self) -> str:
        """
        å®‰å…¨åœ°è·å–APIå¯†é’¥ï¼Œå¤„ç†ä¸åŒçš„å±æ€§åç§°å’ŒSecretStrç±»å‹
        
        Returns:
            str: APIå¯†é’¥
            
        Raises:
            ValueError: å¦‚æœæ— æ³•è·å–APIå¯†é’¥
        """
        # å°è¯•ä¸åŒçš„å¯èƒ½å±æ€§åç§°
        possible_attrs = ['openai_api_key', 'api_key', '_api_key']
        
        for attr in possible_attrs:
            if hasattr(self, attr):
                api_key = getattr(self, attr)
                if api_key:
                    # å¤„ç†SecretStrç±»å‹
                    if hasattr(api_key, 'get_secret_value'):
                        return api_key.get_secret_value()
                    return str(api_key)
        
        # æœ€åå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
        import os
        env_key = os.getenv('OPENAI_API_KEY')
        if env_key:
            return env_key
            
        raise ValueError("æ— æ³•è·å–APIå¯†é’¥ï¼šè¯·æ£€æŸ¥åˆå§‹åŒ–å‚æ•°æˆ–ç¯å¢ƒå˜é‡OPENAI_API_KEY")
    
    def _get_base_url(self) -> str:
        """
        å®‰å…¨åœ°è·å–åŸºç¡€URL
        
        Returns:
            str: åŸºç¡€URL
        """
        # å°è¯•ä¸åŒçš„å¯èƒ½å±æ€§åç§°
        possible_attrs = ['base_url', 'openai_api_base', '_base_url']
        
        for attr in possible_attrs:
            if hasattr(self, attr):
                base_url = getattr(self, attr)
                if base_url:
                    return base_url
                    
        # é»˜è®¤åŸºç¡€URL
        return "https://llm.submodel.ai/v1"
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        ç”ŸæˆèŠå¤©å“åº”ï¼Œä½¿ç”¨ä¼˜åŒ–çš„å‚æ•°å’Œç›´æ¥APIè°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            stop: åœæ­¢è¯
            run_manager: å›è°ƒç®¡ç†å™¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            èŠå¤©ç»“æœ
        """
        
        # ä¿å­˜è‡ªå®šä¹‰å‚æ•°ç”¨äºtokenè·Ÿè¸ªï¼Œå¹¶ä»kwargsä¸­ç§»é™¤
        custom_kwargs = {
            'session_id': kwargs.pop('session_id', None),
            'analysis_type': kwargs.pop('analysis_type', 'stock_analysis')
        }
        
        try:
            # ğŸ”‘ é‡è¦ï¼šå¦‚æœæ˜¯æµå¼å“åº”ï¼Œå¼ºåˆ¶ä½¿ç”¨æˆ‘ä»¬çš„_direct_api_callä»¥ç¡®ä¿æ–¹æ¡ˆ3ç”Ÿæ•ˆ
            # ä½¿ç”¨ä¿å­˜çš„ç”¨æˆ·æµå¼æ„å›¾ï¼Œé¿å…å‚æ•°å†²çª
            wants_streaming = getattr(self, 'user_wants_streaming', False)
            kwargs_stream = kwargs.get('stream', False)
            # åªæœ‰å½“streamæ˜¯å¸ƒå°”å€¼æ—¶æ‰è€ƒè™‘
            if isinstance(kwargs_stream, bool) and kwargs_stream:
                wants_streaming = True
                
            if wants_streaming:
                logger.info(f"ğŸŒŠ æ£€æµ‹åˆ°æµå¼æ¨¡å¼ï¼Œä½¿ç”¨æ–¹æ¡ˆ3çš„_direct_api_call")
                result = self._direct_api_call(messages, stream=True)
            else:
                # ç›´æ¥ä½¿ç”¨LangChainæ ‡å‡†æ–¹æ³•ï¼Œä¸è¿›è¡Œå‚æ•°è¿‡æ»¤
                logger.debug(f"ğŸ”„ ç¬¬ä¸‰æ–¹é€‚é…å™¨ï¼šä½¿ç”¨æ ‡å‡†LangChainè°ƒç”¨ï¼ˆæ— å‚æ•°è¿‡æ»¤ï¼‰")
                result = super()._generate(messages, stop, run_manager, **kwargs)
        except Exception as e:
            # ç‰¹åˆ«å¤„ç†tokenè§£æé”™è¯¯
            if "Unexpected token" in str(e) and "while expecting start token" in str(e):
                logger.warning(f"ğŸ”§ æ£€æµ‹åˆ°tokenè§£æé”™è¯¯ï¼Œå°è¯•æ¸…ç†æ¶ˆæ¯åé‡è¯•: {e}")
                try:
                    # æ¸…ç†æ¶ˆæ¯å†…å®¹
                    cleaned_messages = []
                    for msg in messages:
                        cleaned_content = self._clean_message_content(msg.content)
                        # åˆ›å»ºæ–°çš„æ¶ˆæ¯å¯¹è±¡ï¼Œä¿æŒåŸæœ‰ç±»å‹
                        if hasattr(msg, 'type'):
                            # ä½¿ç”¨LangChainçš„æ¶ˆæ¯ç±»
                            if msg.type == 'human':
                                from langchain_core.messages import HumanMessage
                                cleaned_msg = HumanMessage(content=cleaned_content)
                            else:
                                from langchain_core.messages import AIMessage
                                cleaned_msg = AIMessage(content=cleaned_content)
                        else:
                            # ä¿æŒåŸå§‹æ¶ˆæ¯ç±»å‹
                            cleaned_msg = type(msg)(content=cleaned_content)
                        cleaned_messages.append(cleaned_msg)
                    
                    # ä½¿ç”¨æ¸…ç†åçš„æ¶ˆæ¯é‡è¯•
                    logger.info(f"ğŸ”„ ä½¿ç”¨æ¸…ç†åçš„æ¶ˆæ¯é‡è¯•æ ‡å‡†è°ƒç”¨")
                    result = super()._generate(cleaned_messages, stop, run_manager, **kwargs)
                except Exception as retry_error:
                    logger.warning(f"âš ï¸ æ¸…ç†åé‡è¯•ä»å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ç›´æ¥APIè°ƒç”¨: {retry_error}")
                    result = self._direct_api_call(messages)
            # å¦‚æœå‡ºç°500é”™è¯¯ï¼Œä½¿ç”¨ç›´æ¥APIè°ƒç”¨æ–¹å¼
            elif "500" in str(e) or "Internal server error" in str(e).lower():
                logger.warning(f"âš ï¸ æ ‡å‡†è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨ç›´æ¥APIæ–¹æ³•: {e}")
                logger.info(f"ğŸ”„ åˆ‡æ¢åˆ°ç›´æ¥APIè°ƒç”¨æ¨¡å¼")
                
                try:
                    result = self._direct_api_call(messages)
                except Exception as direct_error:
                    logger.error(f"âŒ ç›´æ¥APIè°ƒç”¨ä¹Ÿå¤±è´¥: {direct_error}")
                    # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                    logger.error(f"   æ¨¡å‹: {getattr(self, 'model_name', 'unknown')}")
                    logger.error(f"   æ¶ˆæ¯æ•°é‡: {len(messages)}")
                    logger.error(f"   æ¶ˆæ¯å†…å®¹: {[m.content[:50] for m in messages]}")
                    
                    # ä¼˜é›…åœ°å¤„ç†å¤±è´¥ï¼Œè¿”å›æœ‰æ„ä¹‰çš„é”™è¯¯ä¿¡æ¯
                    return self._create_error_response(
                        "ğŸš¨ APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚åŸå› ï¼šä¸Šæ¸¸APIæœåŠ¡å‡ºç°é”™è¯¯ï¼ŒåŒæ—¶å¤‡ç”¨è°ƒç”¨æœºåˆ¶ä¹Ÿå¤±è´¥ã€‚"
                    )
            else:
                # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                raise e
        
        # è¿½è¸ª token ä½¿ç”¨é‡
        try:
            # å°è¯•å¤šç§æ–¹å¼è·å–tokenä½¿ç”¨ä¿¡æ¯
            input_tokens = 0
            output_tokens = 0
            
            # æ–¹æ³•1ï¼šæ£€æŸ¥LangChainæ ‡å‡†çš„llm_output
            if hasattr(result, 'llm_output') and result.llm_output:
                token_usage = result.llm_output.get('token_usage', {})
                input_tokens = token_usage.get('prompt_tokens', 0)
                output_tokens = token_usage.get('completion_tokens', 0)
                logger.debug(f"ğŸ” [token] ä»llm_outputè·å–: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}")
            
            # æ–¹æ³•2ï¼šæ£€æŸ¥response_metadataä¸­çš„tokenä¿¡æ¯ï¼ˆæ–°ç‰ˆLangChainï¼‰
            if (input_tokens == 0 and output_tokens == 0 and 
                hasattr(result, 'response_metadata') and result.response_metadata):
                token_usage = result.response_metadata.get('token_usage', {})
                input_tokens = token_usage.get('prompt_tokens', 0)
                output_tokens = token_usage.get('completion_tokens', 0)
                logger.debug(f"ğŸ” [token] ä»response_metadataè·å–: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}")
            
            # æ–¹æ³•3ï¼šæ£€æŸ¥usage_metadataï¼ˆLangChainæ–°å±æ€§ï¼‰
            if (input_tokens == 0 and output_tokens == 0 and 
                hasattr(result, 'usage_metadata') and result.usage_metadata):
                input_tokens = getattr(result.usage_metadata, 'input_tokens', 0)
                output_tokens = getattr(result.usage_metadata, 'output_tokens', 0)
                logger.debug(f"ğŸ” [token] ä»usage_metadataè·å–: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}")
            
            # æ–¹æ³•4ï¼šå¦‚æœä»¥ä¸Šéƒ½æ²¡æœ‰è·å–åˆ°ï¼Œä¸”è¿™æ˜¯æˆ‘ä»¬çš„ç›´æ¥APIè°ƒç”¨ï¼Œå°è¯•è§£ætokenä¿¡æ¯
            if (input_tokens == 0 and output_tokens == 0 and 
                hasattr(self, '_last_api_usage') and self._last_api_usage):
                input_tokens = self._last_api_usage.get('prompt_tokens', 0)
                output_tokens = self._last_api_usage.get('completion_tokens', 0)
                logger.debug(f"ğŸ” [token] ä»_last_api_usageè·å–: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}")
                # æ¸…é™¤ä¸´æ—¶å­˜å‚¨
                self._last_api_usage = None
            
            # æ–¹æ³•5ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œä¸”æ¶ˆæ¯ä¸ä¸ºç©ºï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•ï¼ˆæœ€åæ‰‹æ®µï¼‰
            if input_tokens == 0 and output_tokens == 0 and messages:
                # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦çº¦ç­‰äº1.5ä¸ªtokenï¼Œè‹±æ–‡å•è¯çº¦ç­‰äº1.3ä¸ªtoken
                input_text = " ".join([msg.content for msg in messages])
                output_text = result.content if hasattr(result, 'content') else ""
                
                # ç²—ç•¥ä¼°ç®—
                input_tokens = max(1, int(len(input_text) * 0.75))  # ä¿å®ˆä¼°ç®—
                output_tokens = max(1, int(len(output_text) * 0.75))
                logger.warning(f"âš ï¸ [token] ä½¿ç”¨ä¼°ç®—æ–¹æ³•: è¾“å…¥â‰ˆ{input_tokens}, è¾“å‡ºâ‰ˆ{output_tokens}")
            
            # è®°å½•tokenä½¿ç”¨ï¼ˆåªè¦æœ‰ä»»ä½•ä¸€ä¸ªå€¼å¤§äº0ï¼‰
            if input_tokens > 0 or output_tokens > 0:
                # ä½¿ç”¨åˆå§‹åŒ–æ—¶ä¿å­˜çš„session_id
                session_id = self.session_id or custom_kwargs.get('session_id') or f"thirdparty_openai_{hash(str(messages))%10000}"
                analysis_type = custom_kwargs.get('analysis_type', 'stock_analysis')
                
                # ä½¿ç”¨ TokenTracker è®°å½•ä½¿ç”¨é‡
                if TOKEN_TRACKING_ENABLED:
                    logger.info(f"ğŸ“Š [token] è®°å½•ä½¿ç”¨é‡: {input_tokens}+{output_tokens}={input_tokens+output_tokens} tokens")
                    token_tracker.track_usage(
                        provider="custom_openai",
                        model_name=getattr(self, 'model_name', 'unknown'),
                        input_tokens=input_tokens,
                        output_tokens=output_tokens,
                        session_id=session_id,
                        analysis_type=analysis_type
                    )
                else:
                    logger.warning(f"âš ï¸ [token] Tokenè·Ÿè¸ªæœªå¯ç”¨")
            else:
                logger.warning(f"âš ï¸ [token] æ— æ³•è·å–tokenä½¿ç”¨é‡ä¿¡æ¯")
                        
        except Exception as track_error:
            # token è¿½è¸ªå¤±è´¥ä¸åº”è¯¥å½±å“ä¸»è¦åŠŸèƒ½
            logger.error(f"âš ï¸ Token è¿½è¸ªå¤±è´¥: {track_error}")
        
        return result
    
    def _create_error_response(self, error_message: str) -> ChatResult:
        """
        åˆ›å»ºé”™è¯¯å“åº”ï¼Œç”¨äºä¼˜é›…åœ°å¤„ç†å¤±è´¥
        
        Args:
            error_message: é”™è¯¯ä¿¡æ¯
            
        Returns:
            ChatResult: åŒ…å«é”™è¯¯ä¿¡æ¯çš„å“åº”
        """
        try:
            from langchain_core.outputs import ChatResult, ChatGeneration
            from langchain_core.messages import AIMessage
            
            ai_message = AIMessage(content=error_message)
            generation = ChatGeneration(message=ai_message)
            return ChatResult(generations=[generation])
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„å“åº”å¯¹è±¡
            class MockChatResult:
                def __init__(self, generations):
                    self.generations = generations
                    
            class MockChatGeneration:
                def __init__(self, message):
                    self.message = message
                    
            class MockAIMessage:
                def __init__(self, content):
                    self.content = content
            
            ai_message = MockAIMessage(content=error_message)
            generation = MockChatGeneration(message=ai_message)
            return MockChatResult(generations=[generation])
    
    def _direct_api_call(self, messages: List[BaseMessage], stream: bool = True) -> ChatResult:
        """
        ç›´æ¥ä½¿ç”¨requestsè°ƒç”¨APIï¼Œæ”¯æŒæµå¼å’Œéæµå¼å“åº”
        å¢å¼ºäº†é”™è¯¯å¤„ç†å’Œå±æ€§å®‰å…¨æ€§ï¼ŒåŒ…æ‹¬tokenæ ¼å¼å…¼å®¹æ€§å¤„ç†
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            stream: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”
            
        Returns:
            èŠå¤©ç»“æœ
            
        Raises:
            Exception: å„ç§é”™è¯¯æƒ…å†µ
        """
        import requests
        try:
            from langchain_core.outputs import ChatResult, ChatGeneration
            from langchain_core.messages import AIMessage
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨å ä½ç¬¦
            class ChatResult:
                def __init__(self, generations):
                    self.generations = generations
                    
            class ChatGeneration:
                def __init__(self, message):
                    self.message = message
                    
            class AIMessage:
                def __init__(self, content):
                    self.content = content
        
        try:
            # å®‰å…¨åœ°è·å–APIå¯†é’¥å’ŒåŸºç¡€URL
            api_key = self._get_api_key()
            base_url = self._get_base_url()
            
            logger.debug(f"ğŸ”‘ ä½¿ç”¨APIå¯†é’¥: {api_key[:20] if api_key else 'None'}...")
            logger.debug(f"ğŸŒ ä½¿ç”¨åŸºç¡€URL: {base_url}")
            
        except ValueError as e:
            logger.error(f"âŒ é…ç½®é”™è¯¯: {e}")
            raise Exception(f"é…ç½®é”™è¯¯ï¼š{e}")
        
        # è½¬æ¢å’Œæ¸…ç†æ¶ˆæ¯æ ¼å¼
        api_messages = []
        for msg in messages:
            if hasattr(msg, 'role'):
                role = msg.role
            elif hasattr(msg, 'type'):
                role = 'user' if msg.type == 'human' else 'assistant'
            else:
                role = 'user'
            
            # æ¸…ç†æ¶ˆæ¯å†…å®¹ï¼Œå¤„ç†å¯èƒ½å¯¼è‡´tokenè§£æé”™è¯¯çš„å­—ç¬¦
            content = self._clean_message_content(msg.content)
            
            api_messages.append({
                'role': role,
                'content': content
            })
        
        # å®‰å…¨åœ°è·å–æ¨¡å‹å‚æ•° - ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·å®šä¹‰çš„æ¨¡å‹å
        model_name = getattr(self, 'user_defined_model', None) or getattr(self, 'model', getattr(self, 'model_name', 'gpt-3.5-turbo'))
        logger.info(f"ğŸ¤– å®é™…ä½¿ç”¨çš„æ¨¡å‹å: {model_name}")
        logger.info(f"ğŸ” self.model: {getattr(self, 'model', 'N/A')}")
        logger.info(f"ğŸ” self.model_name: {getattr(self, 'model_name', 'N/A')}")
        logger.info(f"ğŸ” self.user_defined_model: {getattr(self, 'user_defined_model', 'N/A')}")
        temperature = getattr(self, 'temperature', 0.1)
        max_tokens = getattr(self, 'max_tokens', 2000)
        request_timeout = getattr(self, 'request_timeout', 120)
        
        # æ„å»ºè¯·æ±‚æ•°æ® - ä¸è¿›è¡Œå‚æ•°è¿‡æ»¤
        request_data = {
            'model': model_name,
            'messages': api_messages,
            'temperature': temperature,
            'stream': stream  # é»˜è®¤ä½¿ç”¨æµå¼è¯·æ±‚
        }
        
        # ç›´æ¥æ·»åŠ max_tokenså‚æ•°ï¼Œä¸è¿›è¡Œè¿‡æ»¤æ£€æŸ¥
        if max_tokens and max_tokens > 0:
            request_data['max_tokens'] = max_tokens
            
        # å¯ä»¥æ·»åŠ å…¶ä»–å¸¸ç”¨å‚æ•°ï¼ˆä¸è¿›è¡Œå…¼å®¹æ€§æ£€æŸ¥ï¼‰
        # è®©æœåŠ¡ç«¯è‡ªè¡Œå¤„ç†ä¸æ”¯æŒçš„å‚æ•°
        logger.debug(f"ğŸ“‹ ä½¿ç”¨å®Œæ•´å‚æ•°é›†ï¼Œè®©æœåŠ¡ç«¯å¤„ç†å…¼å®¹æ€§")
        
        # è¯·æ±‚å¤´
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {api_key}'
        }
        
        api_url = f"{base_url}/chat/completions"
        logger.debug(f"ğŸŒ ç›´æ¥APIè°ƒç”¨: {api_url}")
        logger.debug(f"ğŸ“ è¯·æ±‚æ•°æ®: {request_data}")
        
        try:
            if stream:
                # æµå¼è¯·æ±‚å¤„ç†
                response = requests.post(
                    api_url,
                    headers=headers,
                    json=request_data,
                    timeout=request_timeout,
                    stream=True
                )
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code != 200:
                    error_msg = f"HTTP {response.status_code}: {response.text}"
                    logger.error(f"âŒ ç›´æ¥APIè°ƒç”¨å¤±è´¥: {error_msg}")
                    raise Exception(f"APIè°ƒç”¨å¤±è´¥: {error_msg}")
                
                # å¤„ç†æµå¼å“åº”
                full_content = ""
                for line in response.iter_lines():
                    if line:
                        decoded_line = line.decode('utf-8')
                        if decoded_line.startswith('data: '):
                            data_str = decoded_line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                            if data_str == '[DONE]':
                                break
                            try:
                                import json
                                chunk_data = json.loads(data_str)
                                if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                                    delta = chunk_data['choices'][0].get('delta', {})
                                    content = delta.get('content', '')
                                    if content:
                                        full_content += content
                                        # è¿™é‡Œå¯ä»¥æ·»åŠ å›è°ƒå¤„ç†ï¼Œå¦‚æœéœ€è¦å®æ—¶å¤„ç†æ¯ä¸ªchunk
                            except json.JSONDecodeError:
                                logger.warning(f"âš ï¸ æ— æ³•è§£ææµå¼æ•°æ®: {data_str}")
                
                logger.info(f"âœ… æµå¼APIè°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(full_content)}")
                
                # æ–¹æ¡ˆ3ï¼šé€šè¿‡é‡æ–°å‘é€å®Œæ•´å¯¹è¯è·å–100%å‡†ç¡®çš„tokenç»Ÿè®¡
                # å°†åŸå§‹è¾“å…¥+AIå®Œæ•´å›å¤ä½œä¸ºå®Œæ•´å¯¹è¯ï¼Œç”¨éæµå¼è¯·æ±‚è·å–ç²¾ç¡®tokenè®¡æ•°
                logger.info(f"ğŸ” [token] å¼€å§‹æ‰§è¡Œæ–¹æ¡ˆ3ï¼šå®Œæ•´å¯¹è¯tokenç»Ÿè®¡...")
                accurate_usage = self._get_accurate_tokens_via_complete_conversation(api_messages, full_content, headers, api_url, request_timeout)
                
                if accurate_usage:
                    # ä½¿ç”¨100%å‡†ç¡®çš„tokenæ•°æ®
                    self._last_api_usage = accurate_usage
                    logger.info(f"âœ… [token] è·å–åˆ°100%å‡†ç¡®çš„tokenç»Ÿè®¡: è¾“å…¥={accurate_usage['prompt_tokens']}, è¾“å‡º={accurate_usage['completion_tokens']}")
                else:
                    # å¦‚æœæ— æ³•è·å–å‡†ç¡®æ•°æ®ï¼Œå›é€€åˆ°ä¼°ç®—
                    logger.warning(f"âš ï¸ [token] æ–¹æ¡ˆ3å¤±è´¥ï¼Œå›é€€åˆ°ä¼°ç®—æ–¹æ³•")
                    input_text = " ".join([msg['content'] for msg in api_messages])
                    input_tokens = max(1, int(len(input_text) * 0.75))
                    output_tokens = max(1, int(len(full_content) * 0.75))
                    
                    self._last_api_usage = {
                        'prompt_tokens': input_tokens,
                        'completion_tokens': output_tokens,
                        'total_tokens': input_tokens + output_tokens
                    }
                    logger.warning(f"âš ï¸ [token] å›é€€åˆ°ä¼°ç®—æ–¹æ³•: è¾“å…¥â‰ˆ{input_tokens}, è¾“å‡ºâ‰ˆ{output_tokens}")
                
                # æ„é€ LangChainæ ¼å¼çš„å“åº”
                ai_message = AIMessage(content=full_content)
                generation = ChatGeneration(message=ai_message)
                return ChatResult(generations=[generation])
            else:
                # éæµå¼è¯·æ±‚å¤„ç†ï¼ˆä¿æŒåŸæœ‰çš„å¤„ç†é€»è¾‘ï¼‰
                response = requests.post(
                    api_url,
                    headers=headers,
                    json=request_data,
                    timeout=request_timeout
                )
                
                # æ£€æŸ¥å“åº”
                if response.status_code == 200:
                    data = response.json()
                    content = data['choices'][0]['message']['content']
                    
                    # æå–çœŸå®çš„tokenä½¿ç”¨ä¿¡æ¯
                    if 'usage' in data:
                        usage = data['usage']
                        self._last_api_usage = {
                            'prompt_tokens': usage.get('prompt_tokens', 0),
                            'completion_tokens': usage.get('completion_tokens', 0),
                            'total_tokens': usage.get('total_tokens', 0)
                        }
                        logger.debug(f"ğŸ” [token] éæµå¼å“åº”çœŸå®tokenä½¿ç”¨: {self._last_api_usage}")
                    else:
                        # å¦‚æœæ²¡æœ‰usageå­—æ®µï¼Œè¿›è¡Œä¼°ç®—
                        input_text = " ".join([msg['content'] for msg in api_messages])
                        input_tokens = max(1, int(len(input_text) * 0.75))
                        output_tokens = max(1, int(len(content) * 0.75))
                        self._last_api_usage = {
                            'prompt_tokens': input_tokens,
                            'completion_tokens': output_tokens,
                            'total_tokens': input_tokens + output_tokens
                        }
                        logger.debug(f"ğŸ” [token] éæµå¼å“åº”ä¼°ç®—tokenä½¿ç”¨: {self._last_api_usage}")
                    
                    logger.info(f"âœ… ç›´æ¥APIè°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(content)}")
                    
                    # æ„é€ LangChainæ ¼å¼çš„å“åº”
                    ai_message = AIMessage(content=content)
                    generation = ChatGeneration(message=ai_message)
                    return ChatResult(generations=[generation])
                else:
                    error_msg = f"HTTP {response.status_code}: {response.text}"
                    logger.error(f"âŒ ç›´æ¥APIè°ƒç”¨å¤±è´¥: {error_msg}")
                    
                    # ç‰¹åˆ«å¤„ç†tokenè§£æé”™è¯¯
                    if "Unexpected token" in response.text and "while expecting start token" in response.text:
                        logger.warning(f"ğŸ”§ æ£€æµ‹åˆ°tokenè§£æé”™è¯¯ï¼Œå°è¯•é‡æ–°æ¸…ç†æ¶ˆæ¯æ ¼å¼")
                        # å°è¯•æ›´æ¿€è¿›çš„æ¶ˆæ¯æ¸…ç†
                        cleaned_messages = self._aggressive_clean_messages(api_messages)
                        if cleaned_messages != api_messages:
                            logger.info(f"ğŸ”„ ä½¿ç”¨æ¿€è¿›æ¸…ç†åçš„æ¶ˆæ¯é‡è¯•")
                            request_data['messages'] = cleaned_messages
                            retry_response = requests.post(
                                api_url,
                                headers=headers,
                                json=request_data,
                                timeout=request_timeout
                            )
                            if retry_response.status_code == 200:
                                data = retry_response.json()
                                content = data['choices'][0]['message']['content']
                                logger.info(f"âœ… é‡è¯•æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(content)}")
                                ai_message = AIMessage(content=content)
                                generation = ChatGeneration(message=ai_message)
                                return ChatResult(generations=[generation])
                    
                    # æ ¹æ®ä¸åŒçš„é”™è¯¯ä»£ç æä¾›å‹å¥½çš„é”™è¯¯ä¿¡æ¯
                    if response.status_code == 400:
                        if "Unexpected token" in response.text:
                            raise Exception("è¾“å…¥æ ¼å¼é”™è¯¯ï¼šæ¶ˆæ¯åŒ…å«æ¨¡å‹æ— æ³•è§£æçš„ç‰¹æ®Šå­—ç¬¦æˆ–æ ¼å¼ï¼Œè¯·ç®€åŒ–è¾“å…¥å†…å®¹")
                        else:
                            raise Exception(f"è¯·æ±‚æ ¼å¼é”™è¯¯ï¼š{response.text}")
                    elif response.status_code == 401:
                        raise Exception("èº«ä»½éªŒè¯å¤±è´¥ï¼šè¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®")
                    elif response.status_code == 403:
                        raise Exception("è®¿é—®è¢«æ‹’ç»ï¼šè¯·æ£€æŸ¥APIå¯†é’¥æƒé™")
                    elif response.status_code == 429:
                        raise Exception("è¯·æ±‚é¢‘ç‡è¶…é™ï¼šè¯·ç¨åå†è¯•")
                    elif response.status_code >= 500:
                        raise Exception(f"æœåŠ¡å™¨é”™è¯¯ ({response.status_code})ï¼šä¸Šæ¸¸APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
                    else:
                        raise Exception(f"æœªçŸ¥é”™è¯¯ ({response.status_code})ï¼š{response.text}")
                        
        except requests.exceptions.Timeout:
            raise Exception("è¯·æ±‚è¶…æ—¶ï¼šç½‘ç»œè¿æ¥é—®é¢˜æˆ–æœåŠ¡å™¨å“åº”è¿‡æ…¢")
        except requests.exceptions.ConnectionError:
            raise Exception("è¿æ¥é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨")
        except requests.exceptions.RequestException as e:
            raise Exception(f"ç½‘ç»œè¯·æ±‚é”™è¯¯ï¼š{str(e)}")
    
    def _get_accurate_tokens_via_complete_conversation(self, original_messages, ai_response_content, headers, api_url, request_timeout):
        """
        æ–¹æ¡ˆ3ï¼šé€šè¿‡é‡æ–°å‘é€å®Œæ•´å¯¹è¯è·å–100%å‡†ç¡®çš„tokenç»Ÿè®¡
        å°†åŸå§‹è¾“å…¥+AIå®Œæ•´å›å¤æ„å»ºæˆå®Œæ•´å¯¹è¯ï¼Œç”¨éæµå¼è¯·æ±‚ç²¾ç¡®è®¡ç®—tokenæ•°é‡
        
        Args:
            original_messages: åŸå§‹ç”¨æˆ·è¾“å…¥æ¶ˆæ¯
            ai_response_content: AIçš„å®Œæ•´æµå¼å›å¤å†…å®¹
            headers: è¯·æ±‚å¤´
            api_url: API URL
            request_timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            dict: åŒ…å«100%å‡†ç¡®tokenä½¿ç”¨ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        import requests  # ç¡®ä¿å¯¼å…¥requests
        
        try:
            # æ„å»ºå®Œæ•´çš„å¯¹è¯å†å²ï¼ˆç”¨æˆ·è¾“å…¥ + AIå®Œæ•´å›å¤ï¼‰
            complete_conversation = []
            
            # 1. æ·»åŠ åŸå§‹ç”¨æˆ·æ¶ˆæ¯
            for msg in original_messages:
                complete_conversation.append({
                    'role': msg['role'],
                    'content': msg['content']
                })
            
            # 2. æ·»åŠ AIçš„å®Œæ•´å›å¤
            complete_conversation.append({
                'role': 'assistant',
                'content': ai_response_content
            })
            
            # 3. æ„å»ºç»Ÿè®¡è¯·æ±‚ï¼šå°†å®Œæ•´å¯¹è¯ä½œä¸ºè¾“å…¥ï¼Œç”¨æå°è¾“å‡ºè·å–tokenç»Ÿè®¡
            token_counting_request = {
                'model': getattr(self, 'model', getattr(self, 'model_name', 'gpt-3.5-turbo')),
                'messages': complete_conversation,  # ğŸ”‘ å…³é”®ï¼šå®Œæ•´å¯¹è¯ä½œä¸ºè¾“å…¥
                'temperature': 0.1,
                'max_tokens': 1,      # ğŸ”‘ æœ€å°è¾“å‡ºï¼Œä»…ä¸ºè·å–tokenç»Ÿè®¡
                'stream': False       # ğŸ”‘ éæµå¼è·å–usageä¿¡æ¯
            }
            
            logger.debug(f"ğŸ” [token] å‘é€å®Œæ•´å¯¹è¯è¿›è¡Œtokenç»Ÿè®¡...")
            logger.debug(f"   å¯¹è¯è½®æ¬¡: {len(complete_conversation)}")
            logger.debug(f"   æœ€åAIå›å¤é•¿åº¦: {len(ai_response_content)}å­—ç¬¦")
            
            response = requests.post(
                api_url,
                headers=headers,
                json=token_counting_request,
                timeout=min(15, request_timeout)  # ä½¿ç”¨é€‚ä¸­çš„è¶…æ—¶æ—¶é—´
            )
            
            if response.status_code == 200:
                data = response.json()
                if 'usage' in data:
                    usage = data['usage']
                    total_conversation_tokens = usage.get('prompt_tokens', 0)
                    
                    # ğŸ¯ å…³é”®è®¡ç®—ï¼šä»æ€»tokenæ•°ä¸­åˆ†ç¦»å‡ºåŸå§‹è¾“å…¥å’ŒAIè¾“å‡ºçš„tokenæ•°
                    # å‘é€ä¸€ä¸ªåªåŒ…å«åŸå§‹è¾“å…¥çš„è¯·æ±‚ï¼Œè·å–çº¯è¾“å…¥çš„tokenæ•°
                    input_only_usage = self._get_input_only_tokens(original_messages, headers, api_url, request_timeout)
                    
                    if input_only_usage:
                        accurate_input_tokens = input_only_usage.get('prompt_tokens', 0)
                        # AIè¾“å‡ºçš„tokenæ•° = å®Œæ•´å¯¹è¯çš„è¾“å…¥tokenæ•° - åŸå§‹è¾“å…¥çš„tokenæ•°
                        accurate_output_tokens = total_conversation_tokens - accurate_input_tokens
                        
                        # éªŒè¯è®¡ç®—ç»“æœçš„åˆç†æ€§
                        if accurate_output_tokens > 0:
                            result = {
                                'prompt_tokens': accurate_input_tokens,      # åŸå§‹ç”¨æˆ·è¾“å…¥çš„å‡†ç¡®tokenæ•°
                                'completion_tokens': accurate_output_tokens, # AIè¾“å‡ºçš„å‡†ç¡®tokenæ•°
                                'total_tokens': accurate_input_tokens + accurate_output_tokens
                            }
                            
                            logger.info(f"ğŸ¯ [token] å®Œæ•´å¯¹è¯tokenåˆ†æ:")
                            logger.info(f"   å®Œæ•´å¯¹è¯è¾“å…¥tokens: {total_conversation_tokens}")
                            logger.info(f"   åŸå§‹è¾“å…¥tokens: {accurate_input_tokens}")
                            logger.info(f"   AIè¾“å‡ºtokens: {accurate_output_tokens}")
                            logger.info(f"   è®¡ç®—éªŒè¯: {accurate_input_tokens} + {accurate_output_tokens} = {result['total_tokens']}")
                            
                            return result
                        else:
                            logger.warning(f"âš ï¸ [token] è®¡ç®—å‡ºçš„è¾“å‡ºtokenæ•°å¼‚å¸¸: {accurate_output_tokens}")
                    else:
                        logger.warning(f"âš ï¸ [token] æ— æ³•è·å–åŸå§‹è¾“å…¥çš„tokenæ•°")
                else:
                    logger.warning(f"âš ï¸ [token] å®Œæ•´å¯¹è¯è¯·æ±‚æœªè¿”å›usageä¿¡æ¯")
            else:
                logger.warning(f"âš ï¸ [token] å®Œæ•´å¯¹è¯è¯·æ±‚å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            logger.warning(f"âš ï¸ [token] å®Œæ•´å¯¹è¯tokenç»Ÿè®¡å¤±è´¥: {e}")
        
        return None
    
    def _get_input_only_tokens(self, original_messages, headers, api_url, request_timeout):
        """
        è·å–çº¯è¾“å…¥æ¶ˆæ¯çš„tokenæ•°é‡
        
        Args:
            original_messages: åŸå§‹ç”¨æˆ·è¾“å…¥æ¶ˆæ¯
            headers: è¯·æ±‚å¤´
            api_url: API URL
            request_timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            dict: åŒ…å«è¾“å…¥tokenä¿¡æ¯ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        import requests  # ç¡®ä¿å¯¼å…¥requests
        
        try:
            # æ„å»ºåªåŒ…å«åŸå§‹è¾“å…¥çš„è¯·æ±‚
            input_only_request = {
                'model': getattr(self, 'model', getattr(self, 'model_name', 'gpt-3.5-turbo')),
                'messages': original_messages,  # ğŸ”‘ åªæœ‰åŸå§‹ç”¨æˆ·è¾“å…¥
                'temperature': 0.1,
                'max_tokens': 1,     # æœ€å°è¾“å‡º
                'stream': False
            }
            
            logger.debug(f"ğŸ” [token] è®¡ç®—åŸå§‹è¾“å…¥tokenæ•°...")
            
            response = requests.post(
                api_url,
                headers=headers,
                json=input_only_request,
                timeout=min(10, request_timeout)
            )
            
            if response.status_code == 200:
                data = response.json()
                if 'usage' in data:
                    return data['usage']
                    
        except Exception as e:
            logger.warning(f"âš ï¸ [token] è·å–è¾“å…¥tokenæ•°å¤±è´¥: {e}")
        
        return None
    
    def _get_accurate_usage_for_streaming(self, api_messages, full_content, headers, api_url, request_timeout):
        """
        ä¸ºæµå¼å“åº”è·å–å‡†ç¡®çš„tokenä½¿ç”¨ç»Ÿè®¡
        é€šè¿‡å‘é€ä¸€ä¸ªç®€åŒ–çš„éæµå¼è¯·æ±‚æ¥è·å–å‡†ç¡®çš„tokenè®¡è´¹ä¿¡æ¯
        
        Args:
            api_messages: åŸå§‹æ¶ˆæ¯
            full_content: æµå¼å“åº”çš„å®Œæ•´å†…å®¹
            headers: è¯·æ±‚å¤´
            api_url: API URL
            request_timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            dict: åŒ…å«å‡†ç¡®tokenä½¿ç”¨ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            # æ„å»ºä¸€ä¸ªç®€åŒ–çš„éæµå¼è¯·æ±‚æ¥è·å–tokenä½¿ç”¨ä¿¡æ¯
            # ä½¿ç”¨ç›¸åŒçš„è¾“å…¥å’Œä¸€ä¸ªç®€çŸ­çš„è¾“å‡ºé™åˆ¶ï¼Œå¿«é€Ÿè·å–å‡†ç¡®çš„è¾“å…¥tokenæ•°
            simplified_request = {
                'model': getattr(self, 'model_name', 'gpt-3.5-turbo'),
                'messages': api_messages,  # ä½¿ç”¨ç›¸åŒçš„è¾“å…¥
                'temperature': 0.1,  # é™ä½éšæœºæ€§
                'max_tokens': 1,      # æœ€å°è¾“å‡ºï¼Œä»…ä¸ºè·å–tokenç»Ÿè®¡
                'stream': False       # éæµå¼è¯·æ±‚
            }
            
            logger.debug(f"ğŸ” [token] å‘é€ç®€åŒ–è¯·æ±‚è·å–å‡†ç¡®tokenç»Ÿè®¡...")
            
            response = requests.post(
                api_url,
                headers=headers,
                json=simplified_request,
                timeout=min(10, request_timeout)  # ä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶
            )
            
            if response.status_code == 200:
                data = response.json()
                if 'usage' in data:
                    usage = data['usage']
                    accurate_input_tokens = usage.get('prompt_tokens', 0)
                    
                    # å¯¹äºè¾“å‡ºtokenï¼Œæˆ‘ä»¬éœ€è¦ä¼°ç®—å®é™…æµå¼è¾“å‡ºçš„tokenæ•°
                    # å› ä¸ºç®€åŒ–è¯·æ±‚åªè¾“å‡ºäº†1ä¸ªtokenï¼Œä½†å®é™…è¾“å‡ºæ›´å¤š
                    # ä½¿ç”¨æ›´ç²¾ç¡®çš„ä¼°ç®—æ–¹æ³•
                    actual_output_tokens = self._estimate_output_tokens_accurately(full_content)
                    
                    return {
                        'prompt_tokens': accurate_input_tokens,  # ä½¿ç”¨APIè¿”å›çš„å‡†ç¡®è¾“å…¥tokenæ•°
                        'completion_tokens': actual_output_tokens,  # ä½¿ç”¨æ”¹è¿›çš„è¾“å‡ºtokenä¼°ç®—
                        'total_tokens': accurate_input_tokens + actual_output_tokens
                    }
                else:
                    logger.warning(f"âš ï¸ [token] ç®€åŒ–è¯·æ±‚æœªè¿”å›usageä¿¡æ¯")
            else:
                logger.warning(f"âš ï¸ [token] ç®€åŒ–è¯·æ±‚å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            logger.warning(f"âš ï¸ [token] è·å–å‡†ç¡®tokenç»Ÿè®¡å¤±è´¥: {e}")
        
        return None
    
    def _estimate_output_tokens_accurately(self, content):
        """
        æ›´å‡†ç¡®åœ°ä¼°ç®—è¾“å‡ºtokenæ•°é‡
        åŸºäºæ”¹è¿›çš„ç®—æ³•ï¼Œè€ƒè™‘ä¸­è‹±æ–‡æ··åˆã€æ ‡ç‚¹ç¬¦å·ç­‰å› ç´ 
        
        Args:
            content: è¾“å‡ºå†…å®¹
            
        Returns:
            int: ä¼°ç®—çš„tokenæ•°é‡
        """
        if not content:
            return 0
        
        # åˆ†åˆ«å¤„ç†ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€æ ‡ç‚¹ç¬¦å·
        import re
        
        # ä¸­æ–‡å­—ç¬¦ï¼ˆåŒ…æ‹¬ä¸­æ–‡æ ‡ç‚¹ï¼‰
        chinese_chars = re.findall(r'[\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]', content)
        chinese_count = len(chinese_chars)
        
        # è‹±æ–‡å•è¯ï¼ˆè¿ç»­çš„å­—æ¯ï¼‰
        english_words = re.findall(r'[a-zA-Z]+', content)
        english_word_count = len(english_words)
        
        # æ•°å­—
        numbers = re.findall(r'\d+', content)
        number_count = len(numbers)
        
        # ç‰¹æ®Šç¬¦å·å’Œemoji
        emojis = re.findall(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', content)
        emoji_count = len(emojis)
        
        # æ ‡ç‚¹ç¬¦å·
        punctuation = re.findall(r'[^\w\s\u4e00-\u9fff]', content)
        punct_count = len(punctuation) - emoji_count  # æ’é™¤emoji
        
        # æ ¹æ®ä¸åŒç±»å‹å†…å®¹è®¡ç®—tokenæ•°
        estimated_tokens = 0
        estimated_tokens += chinese_count * 1.2    # ä¸­æ–‡å­—ç¬¦ï¼Œçº¦1.2 token/å­—ç¬¦
        estimated_tokens += english_word_count * 1.3  # è‹±æ–‡å•è¯ï¼Œçº¦1.3 token/è¯
        estimated_tokens += number_count * 0.8    # æ•°å­—ï¼Œçº¦0.8 token/æ•°å­—
        estimated_tokens += emoji_count * 1.5     # emojiï¼Œçº¦1.5 token/emoji
        estimated_tokens += punct_count * 0.5     # æ ‡ç‚¹ç¬¦å·ï¼Œçº¦0.5 token/ç¬¦å·
        
        result = max(1, int(estimated_tokens))
        
        logger.debug(f"ğŸ” [token] è¯¦ç»†è¾“å‡ºä¼°ç®—: ä¸­æ–‡{chinese_count}*1.2 + è‹±æ–‡{english_word_count}*1.3 + æ•°å­—{number_count}*0.8 + emoji{emoji_count}*1.5 + æ ‡ç‚¹{punct_count}*0.5 = {result}")
        
        return result


def create_third_party_openai(
    model: str,
    api_key: str,
    base_url: str = "https://llm.submodel.ai/v1",
    temperature: float = 0.1,
    max_tokens: int = 2000,
    session_id: Optional[str] = None
) -> ThirdPartyOpenAI:
    """
    åˆ›å»ºç¬¬ä¸‰æ–¹OpenAIé€‚é…å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model: æ¨¡å‹åç§°
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URL
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        session_id: ä¼šè¯IDç”¨äºtokenè·Ÿè¸ª
        
    Returns:
        ç¬¬ä¸‰æ–¹OpenAIé€‚é…å™¨å®ä¾‹
    """
    
    return ThirdPartyOpenAI(
        model=model,
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
        max_tokens=max_tokens,
        session_id=session_id
    )


# ä¸ºäº†å‘åå…¼å®¹
CompatibleChatOpenAI = ThirdPartyOpenAI
