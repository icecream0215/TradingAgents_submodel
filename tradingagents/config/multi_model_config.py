"""
å¤šæ¨¡å‹é…ç½®ç®¡ç†å™¨
æ”¯æŒ9ä¸ªæ¨¡å‹çš„é…ç½®ç®¡ç†ã€åŠ¨æ€åˆ‡æ¢å’Œæ€§èƒ½ç›‘æ§
"""

import os
import json
import time
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
from enum import Enum

from tradingagents.utils.logging_manager import get_logger

logger = get_logger('agents')


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    CODING = "coding"                   # ä»£ç ç›¸å…³ä»»åŠ¡
    REASONING = "reasoning"             # æ¨ç†åˆ†æä»»åŠ¡  
    CONVERSATION = "conversation"       # å¯¹è¯äº¤äº’ä»»åŠ¡
    THINKING = "thinking"              # æ€ç»´é“¾ä»»åŠ¡
    SPEED = "speed"                    # å¿«é€Ÿå“åº”ä»»åŠ¡
    QUALITY = "quality"                # é«˜è´¨é‡è¾“å‡ºä»»åŠ¡
    GENERAL = "general"                # é€šç”¨ä»»åŠ¡
    FINANCIAL = "financial"            # é‡‘èåˆ†æä»»åŠ¡


@dataclass
class ModelCapability:
    """æ¨¡å‹èƒ½åŠ›é…ç½®"""
    name: str
    provider: str
    model_id: str
    base_url: str
    api_key_env: str
    context_length: int
    supports_function_calling: bool
    supports_streaming: bool
    avg_response_time: float
    task_strengths: List[TaskType]
    quality_score: float  # 1-10è¯„åˆ†
    speed_score: float    # 1-10è¯„åˆ†
    cost_score: float     # 1-10è¯„åˆ†ï¼Œ10ä¸ºæœ€ä¾¿å®œ
    description: str


# åŸºç¡€æ¨¡å‹é…ç½®
MODEL_CONFIGURATIONS = {
    "qwen-instruct": ModelCapability(
        name="Qwen Instruct",
        provider="qwen",
        model_id="qwen-instruct",
        base_url="https://llm.submodel.ai/v1",
        api_key_env="OPENAI_API_KEY",
        context_length=32768,
        supports_function_calling=True,
        supports_streaming=True,
        avg_response_time=2.8,
        task_strengths=[TaskType.CONVERSATION, TaskType.GENERAL, TaskType.FINANCIAL],
        quality_score=9.0,
        speed_score=7.0,
        cost_score=6.0,
        description="é€šç”¨å¯¹è¯æ¨¡å‹"
    ),
    "deepseek-v31": ModelCapability(
        name="DeepSeek V3.1",
        provider="deepseek",
        model_id="deepseek-v31",
        base_url="https://llm.submodel.ai/v1",
        api_key_env="OPENAI_API_KEY",
        context_length=32768,
        supports_function_calling=True,
        supports_streaming=True,
        avg_response_time=2.2,
        task_strengths=[TaskType.CODING, TaskType.FINANCIAL, TaskType.GENERAL],
        quality_score=9.2,
        speed_score=8.0,
        cost_score=7.5,
        description="å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦"
    )
}


@dataclass 
class MultiModelSettings:
    """å¤šæ¨¡å‹è®¾ç½®"""
    enabled_models: List[str]                    # å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    default_model: str                          # é»˜è®¤æ¨¡å‹
    auto_selection_enabled: bool                # æ˜¯å¦å¯ç”¨è‡ªåŠ¨é€‰æ‹©
    fallback_models: Dict[str, List[str]]       # å¤‡ç”¨æ¨¡å‹æ˜ å°„
    performance_tracking_enabled: bool         # æ˜¯å¦å¯ç”¨æ€§èƒ½è·Ÿè¸ª
    cost_tracking_enabled: bool                # æ˜¯å¦å¯ç”¨æˆæœ¬è·Ÿè¸ª
    task_model_mapping: Dict[str, str]          # ä»»åŠ¡ç±»å‹åˆ°æ¨¡å‹çš„æ˜ å°„
    priority_settings: Dict[str, str]           # ä¼˜å…ˆçº§è®¾ç½®


@dataclass
class ModelEndpointConfig:
    """æ¨¡å‹ç«¯ç‚¹é…ç½®"""
    model_name: str
    model_id: str
    provider: str
    base_url: str
    api_key_env: str
    enabled: bool = True
    custom_headers: Dict[str, str] = None
    timeout: int = 120
    max_retries: int = 3


class MultiModelConfigManager:
    """å¤šæ¨¡å‹é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_dir: str = None):
        """
        åˆå§‹åŒ–å¤šæ¨¡å‹é…ç½®ç®¡ç†å™¨
        
        Args:
            config_dir: é…ç½®ç›®å½•è·¯å¾„
        """
        self.config_dir = Path(config_dir or "config/multi_model")
        self.config_dir.mkdir(parents=True, exist_ok=True)
        
        self.settings_file = self.config_dir / "multi_model_settings.json"
        self.endpoints_file = self.config_dir / "model_endpoints.json"
        self.performance_file = self.config_dir / "performance_cache.json"
        
        self.settings: Optional[MultiModelSettings] = None
        self.endpoints: Dict[str, ModelEndpointConfig] = {}
        
        self._load_configurations()
        self._ensure_default_configs()
    
    def _load_configurations(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            # åŠ è½½è®¾ç½®
            if self.settings_file.exists():
                with open(self.settings_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.settings = MultiModelSettings(**data)
            
            # åŠ è½½ç«¯ç‚¹é…ç½®
            if self.endpoints_file.exists():
                with open(self.endpoints_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.endpoints = {
                        name: ModelEndpointConfig(**config)
                        for name, config in data.items()
                    }
            
            logger.info(f"âœ… å¤šæ¨¡å‹é…ç½®åŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å¤šæ¨¡å‹é…ç½®å¤±è´¥: {e}")
            self._create_default_configs()
    
    def _ensure_default_configs(self):
        """ç¡®ä¿é»˜è®¤é…ç½®å­˜åœ¨"""
        if self.settings is None:
            self._create_default_configs()
        
        if not self.endpoints:
            self._create_default_endpoints()
    
    def _create_default_configs(self):
        """åˆ›å»ºé»˜è®¤é…ç½®"""
        self.settings = MultiModelSettings(
            enabled_models=list(MODEL_CONFIGURATIONS.keys()),
            default_model="qwen-instruct",
            auto_selection_enabled=True,
            fallback_models={
                "qwen-coder": ["deepseek-v31", "qwen-instruct"],
                "qwen-instruct": ["gpt-oss", "glm-4.5"],
                "glm-4.5": ["qwen-instruct", "gpt-oss"],
                "gpt-oss": ["qwen-instruct", "deepseek-v31"],
                "deepseek-r1": ["qwen-thinking", "qwen-instruct"],
                "qwen-thinking": ["deepseek-r1", "qwen-instruct"],
                "deepseek-v31": ["qwen-instruct", "gpt-oss"]
            },
            performance_tracking_enabled=True,
            cost_tracking_enabled=True,
            task_model_mapping={
                TaskType.CODING.value: "qwen-coder",
                TaskType.REASONING.value: "deepseek-r1",
                TaskType.THINKING.value: "qwen-thinking",
                TaskType.CONVERSATION.value: "qwen-instruct",
                TaskType.SPEED.value: "glm-4.5",
                TaskType.QUALITY.value: "qwen-thinking",
                TaskType.GENERAL.value: "qwen-instruct",
                TaskType.FINANCIAL.value: "deepseek-v31"
            },
            priority_settings={
                "development": "quality",
                "production": "balanced",
                "testing": "speed"
            }
        )
        
        self._save_settings()
    
    def _create_default_endpoints(self):
        """åˆ›å»ºé»˜è®¤ç«¯ç‚¹é…ç½®"""
        for model_name, config in MODEL_CONFIGURATIONS.items():
            self.endpoints[model_name] = ModelEndpointConfig(
                model_name=model_name,
                model_id=config.model_id,
                provider=config.provider,
                base_url=config.base_url,
                api_key_env=config.api_key_env,
                enabled=True,
                timeout=120,
                max_retries=3
            )
        
        self._save_endpoints()
    
    def _save_settings(self):
        """ä¿å­˜è®¾ç½®"""
        try:
            with open(self.settings_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(self.settings), f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¤šæ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
    
    def _save_endpoints(self):
        """ä¿å­˜ç«¯ç‚¹é…ç½®"""
        try:
            data = {
                name: asdict(config) for name, config in self.endpoints.items()
            }
            with open(self.endpoints_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç«¯ç‚¹é…ç½®å¤±è´¥: {e}")
    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return [
            model_name for model_name, config in self.endpoints.items()
            if config.enabled and model_name in self.settings.enabled_models
        ]
    
    def get_model_for_task(
        self,
        task_type: TaskType,
        use_intelligent_selection: bool = None,
        priority: str = None
    ) -> str:
        """
        è·å–ä»»åŠ¡çš„æœ€ä½³æ¨¡å‹
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            use_intelligent_selection: æ˜¯å¦ä½¿ç”¨æ™ºèƒ½é€‰æ‹©
            priority: ä¼˜å…ˆçº§è®¾ç½®
            
        Returns:
            æ¨¡å‹åç§°
        """
        
        # é¢„è®¾æ˜ å°„é€‰æ‹©
        mapped_model = self.settings.task_model_mapping.get(task_type.value)
        if mapped_model and mapped_model in self.get_available_models():
            return mapped_model
        
        # é»˜è®¤æ¨¡å‹
        if self.settings.default_model in self.get_available_models():
            return self.settings.default_model
        
        # å…œåº•ï¼šç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
        available = self.get_available_models()
        if available:
            return available[0]
        
        raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
    
    def get_fallback_models(self, primary_model: str) -> List[str]:
        """è·å–å¤‡ç”¨æ¨¡å‹åˆ—è¡¨"""
        fallbacks = self.settings.fallback_models.get(primary_model, [])
        available_models = self.get_available_models()
        
        return [model for model in fallbacks if model in available_models]
    
    def create_model_adapter(
        self,
        model_name: str = None,
        task_type: TaskType = TaskType.GENERAL,
        **kwargs
    ):
        """
        åˆ›å»ºæ¨¡å‹é€‚é…å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨é€‰æ‹©
            task_type: ä»»åŠ¡ç±»å‹
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            æ¨¡å‹é€‚é…å™¨å®ä¾‹
        """
        
        # è‡ªåŠ¨é€‰æ‹©æ¨¡å‹
        if model_name is None:
            model_name = self.get_model_for_task(task_type)
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
        if model_name not in self.get_available_models():
            raise ValueError(f"æ¨¡å‹ {model_name} ä¸å¯ç”¨")
        
        # è·å–ç«¯ç‚¹é…ç½®
        endpoint_config = self.endpoints[model_name]
        
        # æ£€æŸ¥APIå¯†é’¥
        api_key = os.getenv(endpoint_config.api_key_env)
        if not api_key:
            raise ValueError(
                f"æ¨¡å‹ {model_name} çš„APIå¯†é’¥æœªæ‰¾åˆ°ã€‚"
                f"è¯·è®¾ç½®ç¯å¢ƒå˜é‡ {endpoint_config.api_key_env}"
            )
        
        # åˆ›å»ºé€‚é…å™¨
        from langchain_openai import ChatOpenAI
        
        adapter = ChatOpenAI(
            model=endpoint_config.model_id,
            api_key=api_key,
            base_url=endpoint_config.base_url,
            temperature=kwargs.get('temperature', 0.1),
            max_tokens=kwargs.get('max_tokens', 2000),
            timeout=endpoint_config.timeout,
            max_retries=endpoint_config.max_retries
        )
        
        logger.info(f"âœ… åˆ›å»ºæ¨¡å‹é€‚é…å™¨: {model_name}")
        return adapter
    
    def enable_model(self, model_name: str):
        """å¯ç”¨æ¨¡å‹"""
        if model_name in MODEL_CONFIGURATIONS:
            if model_name not in self.settings.enabled_models:
                self.settings.enabled_models.append(model_name)
            
            if model_name in self.endpoints:
                self.endpoints[model_name].enabled = True
            
            self._save_settings()
            self._save_endpoints()
            logger.info(f"âœ… å¯ç”¨æ¨¡å‹: {model_name}")
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_name}")
    
    def disable_model(self, model_name: str):
        """ç¦ç”¨æ¨¡å‹"""
        if model_name in self.settings.enabled_models:
            self.settings.enabled_models.remove(model_name)
        
        if model_name in self.endpoints:
            self.endpoints[model_name].enabled = False
        
        self._save_settings()
        self._save_endpoints()
        logger.info(f"âŒ ç¦ç”¨æ¨¡å‹: {model_name}")
    
    def set_task_model_mapping(self, task_type: TaskType, model_name: str):
        """è®¾ç½®ä»»åŠ¡ç±»å‹åˆ°æ¨¡å‹çš„æ˜ å°„"""
        if model_name not in MODEL_CONFIGURATIONS:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_name}")
        
        self.settings.task_model_mapping[task_type.value] = model_name
        self._save_settings()
        logger.info(f"âœ… è®¾ç½®ä»»åŠ¡æ˜ å°„: {task_type.value} -> {model_name}")
    
    def update_endpoint_config(
        self,
        model_name: str,
        base_url: str = None,
        api_key_env: str = None,
        timeout: int = None,
        max_retries: int = None
    ):
        """æ›´æ–°ç«¯ç‚¹é…ç½®"""
        if model_name not in self.endpoints:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹ç«¯ç‚¹: {model_name}")
        
        config = self.endpoints[model_name]
        
        if base_url is not None:
            config.base_url = base_url
        if api_key_env is not None:
            config.api_key_env = api_key_env
        if timeout is not None:
            config.timeout = timeout
        if max_retries is not None:
            config.max_retries = max_retries
        
        self._save_endpoints()
        logger.info(f"âœ… æ›´æ–°ç«¯ç‚¹é…ç½®: {model_name}")
    
    def get_model_status(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹çŠ¶æ€"""
        status = {
            "total_models": len(MODEL_CONFIGURATIONS),
            "enabled_models": len(self.get_available_models()),
            "models": {}
        }
        
        for model_name, config in MODEL_CONFIGURATIONS.items():
            endpoint_config = self.endpoints.get(model_name)
            
            # æ£€æŸ¥APIå¯†é’¥
            api_key_available = bool(os.getenv(config.api_key_env)) if endpoint_config else False
            
            status["models"][model_name] = {
                "name": config.name,
                "provider": config.provider,
                "enabled": model_name in self.settings.enabled_models,
                "api_key_available": api_key_available,
                "task_strengths": [t.value for t in config.task_strengths],
                "quality_score": config.quality_score,
                "speed_score": config.speed_score,
                "avg_response_time": config.avg_response_time
            }
        
        return status
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {"message": "æ€§èƒ½è·Ÿè¸ªåŠŸèƒ½å·²ç§»é™¤"}
    
    def record_model_performance(
        self,
        model_name: str,
        task_type: TaskType,
        response_time: float,
        success: bool,
        quality_score: float = None,
        user_satisfaction: float = None
    ):
        """è®°å½•æ¨¡å‹æ€§èƒ½"""
        # æ€§èƒ½è®°å½•åŠŸèƒ½å·²ç§»é™¤
        pass
    
    def export_config(self) -> Dict[str, Any]:
        """å¯¼å‡ºé…ç½®"""
        return {
            "settings": asdict(self.settings),
            "endpoints": {name: asdict(config) for name, config in self.endpoints.items()},
            "model_configurations": {
                name: {
                    "name": config.name,
                    "description": config.description,
                    "provider": config.provider,
                    "task_strengths": [t.value for t in config.task_strengths],
                    "quality_score": config.quality_score,
                    "speed_score": config.speed_score,
                    "cost_score": config.cost_score
                }
                for name, config in MODEL_CONFIGURATIONS.items()
            }
        }
    
    def import_config(self, config_data: Dict[str, Any]):
        """å¯¼å…¥é…ç½®"""
        try:
            if "settings" in config_data:
                self.settings = MultiModelSettings(**config_data["settings"])
                self._save_settings()
            
            if "endpoints" in config_data:
                self.endpoints = {
                    name: ModelEndpointConfig(**config)
                    for name, config in config_data["endpoints"].items()
                }
                self._save_endpoints()
            
            logger.info("âœ… é…ç½®å¯¼å…¥æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ é…ç½®å¯¼å…¥å¤±è´¥: {e}")
            raise
    
    def test_all_models(self) -> Dict[str, Dict[str, Any]]:
        """æµ‹è¯•æ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        results = {}
        available_models = self.get_available_models()
        
        for model_name in available_models:
            try:
                start_time = time.time()
                
                # åˆ›å»ºé€‚é…å™¨
                adapter = self.create_model_adapter(model_name)
                
                # ç®€å•æµ‹è¯•
                from langchain_core.messages import HumanMessage
                messages = [HumanMessage(content="è¯·ç®€å•å›ç­”ï¼šä½ å¥½")]
                
                result = adapter._generate(messages)
                
                end_time = time.time()
                response_time = end_time - start_time
                
                results[model_name] = {
                    "status": "success",
                    "response_time": response_time,
                    "model_info": adapter.get_model_info(),
                    "error": None
                }
                
                logger.info(f"âœ… æ¨¡å‹æµ‹è¯•æˆåŠŸ: {model_name} ({response_time:.2f}s)")
                
            except Exception as e:
                results[model_name] = {
                    "status": "failed",
                    "response_time": None,
                    "model_info": None,
                    "error": str(e)
                }
                
                logger.error(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {model_name} - {e}")
        
        return results


# å…¨å±€å¤šæ¨¡å‹é…ç½®ç®¡ç†å™¨å®ä¾‹
_global_multi_config = None

def get_multi_model_config() -> MultiModelConfigManager:
    """è·å–å…¨å±€å¤šæ¨¡å‹é…ç½®ç®¡ç†å™¨å®ä¾‹"""
    global _global_multi_config
    if _global_multi_config is None:
        _global_multi_config = MultiModelConfigManager()
    return _global_multi_config


def create_smart_llm(
    task_description: str = None,
    task_type: TaskType = TaskType.GENERAL,
    priority: str = "balanced",
    **kwargs
):
    """
    æ™ºèƒ½åˆ›å»ºLLMé€‚é…å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        task_description: ä»»åŠ¡æè¿°ï¼ˆç”¨äºæ™ºèƒ½é€‰æ‹©ï¼‰
        task_type: ä»»åŠ¡ç±»å‹
        priority: ä¼˜å…ˆçº§
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        LLMé€‚é…å™¨å®ä¾‹
    """
    
    config_manager = get_multi_model_config()
    
    # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©
    return config_manager.create_model_adapter(None, task_type, **kwargs)


def test_multi_model_config():
    """æµ‹è¯•å¤šæ¨¡å‹é…ç½®ç³»ç»Ÿ"""
    logger.info("ğŸ§ª æµ‹è¯•å¤šæ¨¡å‹é…ç½®ç³»ç»Ÿ")
    logger.info("=" * 50)
    
    config_manager = get_multi_model_config()
    
    # è·å–æ¨¡å‹çŠ¶æ€
    status = config_manager.get_model_status()
    logger.info(f"ğŸ“Š æ€»æ¨¡å‹æ•°: {status['total_models']}")
    logger.info(f"ğŸ“Š å¯ç”¨æ¨¡å‹æ•°: {status['enabled_models']}")
    
    # æµ‹è¯•ä»»åŠ¡æ¨¡å‹é€‰æ‹©
    test_cases = [
        (TaskType.CODING, "å†™ä¸€ä¸ªPythonæ’åºç®—æ³•"),
        (TaskType.FINANCIAL, "åˆ†æè‹¹æœå…¬å¸çš„è‚¡ç¥¨"),
        (TaskType.SPEED, "å¿«é€Ÿå›ç­”å½“å‰æ—¶é—´"),
        (TaskType.THINKING, "æ·±åº¦åˆ†æäººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•")
    ]
    
    for task_type, description in test_cases:
        try:
            model_name = config_manager.get_model_for_task(task_type)
            logger.info(f"âœ… {task_type.value}: {model_name}")
            
        except Exception as e:
            logger.error(f"âŒ {task_type.value}: {e}")


if __name__ == "__main__":
    test_multi_model_config()