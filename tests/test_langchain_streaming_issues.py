#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æŸ¥LangChainæµå¼æ”¯æŒçš„æ½œåœ¨é—®é¢˜
æ¯”è¾ƒLangChainæ ‡å‡†æ–¹æ³•å’Œæˆ‘ä»¬çš„ç›´æ¥APIè°ƒç”¨æ–¹æ³•
"""

import os
import sys
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from tradingagents.config.config_manager import config_manager, token_tracker
from tradingagents.utils.logging_manager import get_logger
from tradingagents.llm_adapters.third_party_openai import ThirdPartyOpenAI

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)-20s | %(levelname)-8s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

logger = get_logger('langchain_test')

def test_langchain_streaming_issues():
    """æµ‹è¯•LangChainæµå¼æ”¯æŒçš„æ½œåœ¨é—®é¢˜"""
    
    print("ğŸ” LangChainæµå¼æ”¯æŒé—®é¢˜æ£€æŸ¥")
    print("=" * 60)
    
    try:
        # 1. æµ‹è¯•LangChainæ ‡å‡†æµå¼æ–¹æ³•
        print("\n1ï¸âƒ£ æµ‹è¯•LangChainæ ‡å‡†æµå¼æ–¹æ³•...")
        
        llm_standard = ThirdPartyOpenAI(
            model=os.getenv('OPENAI_MODEL_NAME', 'Qwen/Qwen3-235B-A22B-Instruct-2507'),
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=os.getenv('OPENAI_API_BASE', 'https://llm.submodel.ai/v1'),
            temperature=0.7,
            streaming=False  # ğŸ”‘ å…³é—­æµå¼ï¼Œä½¿ç”¨LangChainæ ‡å‡†æ–¹æ³•
        )
        
        from langchain_core.messages import HumanMessage
        test_query = "ç®€è¦è¯´æ˜åŒºå—é“¾æŠ€æœ¯ä¼˜åŠ¿ï¼Œä¸è¶…è¿‡20å­—ã€‚"
        messages = [HumanMessage(content=test_query)]
        
        logger.info(f"ğŸ“ æµ‹è¯•æ¶ˆæ¯: {test_query}")
        logger.info(f"ğŸ¯ LangChainæ ‡å‡†æ–¹æ³•ï¼ˆéæµå¼ï¼‰...")
        
        before_count = len(config_manager.load_usage_records())
        result1 = llm_standard.generate([messages])
        after_count = len(config_manager.load_usage_records())
        
        logger.info(f"âœ… LangChainæ ‡å‡†å“åº”: {result1.generations[0][0].text}")
        logger.info(f"ğŸ“Š æ–°å¢è®°å½•æ•°: {after_count - before_count}")
        
        # 2. æµ‹è¯•æˆ‘ä»¬çš„ç›´æ¥APIè°ƒç”¨æ–¹æ³•ï¼ˆæµå¼ï¼‰
        print("\n2ï¸âƒ£ æµ‹è¯•æˆ‘ä»¬çš„ç›´æ¥APIæµå¼æ–¹æ³•...")
        
        llm_direct = ThirdPartyOpenAI(
            model=os.getenv('OPENAI_MODEL_NAME', 'Qwen/Qwen3-235B-A22B-Instruct-2507'),
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=os.getenv('OPENAI_API_BASE', 'https://llm.submodel.ai/v1'),
            temperature=0.7,
            streaming=True  # ğŸ”‘ å¼€å¯æµå¼ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•
        )
        
        test_query2 = "ç®€è¦è¯´æ˜äººå·¥æ™ºèƒ½æŠ€æœ¯ä¼˜åŠ¿ï¼Œä¸è¶…è¿‡20å­—ã€‚"
        messages2 = [HumanMessage(content=test_query2)]
        
        logger.info(f"ğŸ“ æµ‹è¯•æ¶ˆæ¯: {test_query2}")
        logger.info(f"ğŸ¯ æˆ‘ä»¬çš„æµå¼æ–¹æ³•...")
        
        before_count2 = len(config_manager.load_usage_records())
        result2 = llm_direct.generate([messages2])
        after_count2 = len(config_manager.load_usage_records())
        
        logger.info(f"âœ… æˆ‘ä»¬çš„æµå¼å“åº”: {result2.generations[0][0].text}")
        logger.info(f"ğŸ“Š æ–°å¢è®°å½•æ•°: {after_count2 - before_count2}")
        
        # 3. æµ‹è¯•LangChainçš„æµå¼å›è°ƒæœºåˆ¶
        print("\n3ï¸âƒ£ æµ‹è¯•LangChainæµå¼å›è°ƒæœºåˆ¶...")
        
        class StreamingHandler:
            def __init__(self):
                self.tokens = []
                self.complete_text = ""
                
            def on_llm_new_token(self, token: str, **kwargs):
                self.tokens.append(token)
                self.complete_text += token
                print(f"ğŸ”„ æµå¼Token: {repr(token)}")
        
        handler = StreamingHandler()
        
        try:
            # æµ‹è¯•LangChainçš„æµå¼å›è°ƒ
            from langchain.callbacks import CallbackManager
            from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
            
            llm_callback = ThirdPartyOpenAI(
                model=os.getenv('OPENAI_MODEL_NAME', 'Qwen/Qwen3-235B-A22B-Instruct-2507'),
                api_key=os.getenv('OPENAI_API_KEY'),
                base_url=os.getenv('OPENAI_API_BASE', 'https://llm.submodel.ai/v1'),
                temperature=0.7,
                streaming=True,
                callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])
            )
            
            test_query3 = "ç®€è¦è¯´æ˜æœºå™¨å­¦ä¹ åŸºæœ¬åŸç†ï¼Œä¸è¶…è¿‡20å­—ã€‚"
            messages3 = [HumanMessage(content=test_query3)]
            
            logger.info(f"ğŸ“ æµ‹è¯•æ¶ˆæ¯: {test_query3}")
            logger.info(f"ğŸ¯ LangChainæµå¼å›è°ƒ...")
            
            before_count3 = len(config_manager.load_usage_records())
            result3 = llm_callback.generate([messages3])
            after_count3 = len(config_manager.load_usage_records())
            
            logger.info(f"âœ… å›è°ƒæµå¼å“åº”: {result3.generations[0][0].text}")
            logger.info(f"ğŸ“Š æ–°å¢è®°å½•æ•°: {after_count3 - before_count3}")
            logger.info(f"ğŸ”„ æ•è·çš„tokenæ•°: {len(handler.tokens)}")
            
        except Exception as callback_error:
            logger.warning(f"âš ï¸ LangChainæµå¼å›è°ƒæµ‹è¯•å¤±è´¥: {callback_error}")
        
        # 4. åˆ†æé—®é¢˜
        print("\n4ï¸âƒ£ é—®é¢˜åˆ†æ...")
        
        # æ£€æŸ¥æœ€è¿‘çš„è®°å½•
        recent_records = config_manager.load_usage_records()[-6:]
        logger.info(f"ğŸ“Š æœ€è¿‘6æ¡è®°å½•åˆ†æ:")
        
        for i, record in enumerate(recent_records[-3:], 1):
            logger.info(f"   è®°å½•{i}: è¾“å…¥={record.input_tokens}, è¾“å‡º={record.output_tokens}, æ¨¡å‹={record.model_name}")
            logger.info(f"          æ—¶é—´={record.timestamp}, æˆæœ¬=Â¥{record.cost:.6f}")
        
        # 5. LangChainç‰ˆæœ¬æ£€æŸ¥
        print("\n5ï¸âƒ£ LangChainç‰ˆæœ¬æ£€æŸ¥...")
        
        try:
            import langchain
            logger.info(f"ğŸ“¦ LangChainç‰ˆæœ¬: {langchain.__version__}")
        except:
            logger.warning("âš ï¸ æ— æ³•è·å–LangChainç‰ˆæœ¬")
            
        try:
            import langchain_openai
            logger.info(f"ğŸ“¦ LangChain-OpenAIç‰ˆæœ¬: {langchain_openai.__version__}")
        except:
            logger.warning("âš ï¸ æ— æ³•è·å–LangChain-OpenAIç‰ˆæœ¬")
        
        # 6. æ£€æŸ¥æµå¼å‚æ•°å¤„ç†
        print("\n6ï¸âƒ£ æµå¼å‚æ•°å¤„ç†æ£€æŸ¥...")
        
        # æ£€æŸ¥æˆ‘ä»¬çš„å‚æ•°å¤„ç†é€»è¾‘
        test_llm = ThirdPartyOpenAI(
            model="test-model",
            streaming=True,
            stream=True  # åŒæ—¶è®¾ç½®ä¸¤ä¸ªå‚æ•°
        )
        
        logger.info(f"ğŸ” streamingå±æ€§: {getattr(test_llm, 'streaming', 'N/A')}")
        logger.info(f"ğŸ” streamå±æ€§: {getattr(test_llm, 'stream', 'N/A')}")
        
        # æ£€æŸ¥å¯èƒ½çš„å†²çª
        if hasattr(test_llm, 'streaming') and hasattr(test_llm, 'stream'):
            if test_llm.streaming != test_llm.stream:
                logger.warning(f"âš ï¸ æµå¼å‚æ•°å†²çª: streaming={test_llm.streaming}, stream={test_llm.stream}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    try:
        # åŠ è½½ç¯å¢ƒå˜é‡
        from dotenv import load_dotenv
        load_dotenv()
        
        if not os.getenv('OPENAI_API_KEY'):
            logger.error("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
            sys.exit(1)
        
        logger.info("âœ… ç¯å¢ƒé…ç½®åŠ è½½æˆåŠŸ")
        
        # è¿è¡Œæµ‹è¯•
        test_result = test_langchain_streaming_issues()
        
        print("\n" + "=" * 60)
        print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
        
        if test_result:
            print("âœ… LangChainæµå¼æ”¯æŒæ£€æŸ¥å®Œæˆ")
            print("ğŸ“š å¯èƒ½çš„é—®é¢˜ç‚¹:")
            print("   1. LangChainæ ‡å‡†æµå¼ vs æˆ‘ä»¬çš„ç›´æ¥APIè°ƒç”¨")
            print("   2. æµå¼å›è°ƒæœºåˆ¶çš„å…¼å®¹æ€§")
            print("   3. streaming/streamå‚æ•°å†²çª")
            print("   4. tokenç»Ÿè®¡åœ¨ä¸åŒæ–¹æ³•ä¸‹çš„è¡¨ç°")
        else:
            print("âŒ æ£€æŸ¥è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
        
        print(f"\nğŸ“š ç›¸å…³æ–‡ä»¶:")
        print(f"   - ä»£ç : tradingagents/llm_adapters/third_party_openai.py")
        print(f"   - è®°å½•: config/usage.json")
        
    except KeyboardInterrupt:
        print(f"\n\nğŸ‘‹ æµ‹è¯•è¢«ç”¨æˆ·å–æ¶ˆ")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()