#!/usr/bin/env python3
"""
æµ‹è¯•LLMé€‚é…å™¨çš„æµå¼è¯·æ±‚åŠŸèƒ½
éªŒè¯ç§»é™¤å‚æ•°è¿‡æ»¤åçš„LLMé€‚é…å™¨æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œ
"""

import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('test')

def test_third_party_openai_streaming():
    """æµ‹è¯•ç¬¬ä¸‰æ–¹OpenAIé€‚é…å™¨çš„æµå¼è¯·æ±‚"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç¬¬ä¸‰æ–¹OpenAIé€‚é…å™¨ - æµå¼è¯·æ±‚")
    print("=" * 60)
    
    try:
        from tradingagents.llm_adapters.third_party_openai import ThirdPartyOpenAI
        from langchain_core.messages import HumanMessage
        
        # æ£€æŸ¥æ˜¯å¦æœ‰APIå¯†é’¥
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âš ï¸ æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼Œè·³è¿‡æµ‹è¯•")
            return False
        
        print(f"ğŸ”§ åˆ›å»ºç¬¬ä¸‰æ–¹OpenAIé€‚é…å™¨...")
        llm = ThirdPartyOpenAI(
            model="gpt-3.5-turbo", 
            api_key=api_key,
            base_url="https://llm.submodel.ai/v1",
            temperature=0.7,
            max_tokens=100,
            # æ·»åŠ ä¸€äº›é€šå¸¸ä¼šè¢«è¿‡æ»¤çš„å‚æ•°æ¥æµ‹è¯•
            top_p=0.9,
            presence_penalty=0.1,
            frequency_penalty=0.1
        )
        
        print(f"âœ… é€‚é…å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ç”Ÿæˆå“åº”
        print(f"ğŸ“ æµ‹è¯•ç”Ÿæˆå“åº”...")
        messages = [HumanMessage(content="è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½ï¼Œç”¨ä¸­æ–‡å›ç­”")]
        
        result = llm._generate(messages)
        
        if result and result.generations:
            response_content = result.generations[0].message.content
            print(f"âœ… ç”ŸæˆæˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response_content)}")
            print(f"ğŸ“„ å“åº”é¢„è§ˆ: {response_content[:100]}...")
            return True
        else:
            print(f"âŒ ç”Ÿæˆå¤±è´¥ï¼Œæœªæ”¶åˆ°æœ‰æ•ˆå“åº”")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_dashscope_streaming():
    """æµ‹è¯•DashScopeé€‚é…å™¨çš„æµå¼è¯·æ±‚"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•DashScopeé€‚é…å™¨ - æµå¼è¯·æ±‚")
    print("=" * 60)
    
    try:
        from tradingagents.llm_adapters.dashscope_adapter import ChatDashScope
        from langchain_core.messages import HumanMessage
        
        # æ£€æŸ¥æ˜¯å¦æœ‰APIå¯†é’¥
        api_key = os.getenv("DASHSCOPE_API_KEY")
        if not api_key:
            print("âš ï¸ æœªè®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡ï¼Œè·³è¿‡æµ‹è¯•")
            return False
        
        print(f"ğŸ”§ åˆ›å»ºDashScopeé€‚é…å™¨...")
        llm = ChatDashScope(
            model="qwen-turbo",
            api_key=api_key,
            temperature=0.7,
            max_tokens=100,
            # æµ‹è¯•é¢å¤–å‚æ•°ä¼ é€’
            top_p=0.9
        )
        
        print(f"âœ… é€‚é…å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ç”Ÿæˆå“åº”
        print(f"ğŸ“ æµ‹è¯•ç”Ÿæˆå“åº”...")
        messages = [HumanMessage(content="è¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‚¡ç¥¨æŠ•èµ„ï¼Œç”¨ä¸­æ–‡å›ç­”")]
        
        result = llm._generate(messages)
        
        if result and result.generations:
            response_content = result.generations[0].message.content
            print(f"âœ… ç”ŸæˆæˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response_content)}")
            print(f"ğŸ“„ å“åº”é¢„è§ˆ: {response_content[:100]}...")
            return True
        else:
            print(f"âŒ ç”Ÿæˆå¤±è´¥ï¼Œæœªæ”¶åˆ°æœ‰æ•ˆå“åº”")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_deepseek_streaming():
    """æµ‹è¯•DeepSeekç›´æ¥é€‚é…å™¨çš„æµå¼è¯·æ±‚"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•DeepSeekç›´æ¥é€‚é…å™¨ - æµå¼è¯·æ±‚")
    print("=" * 60)
    
    try:
        from tradingagents.llm_adapters.deepseek_direct_adapter import DeepSeekDirectAdapter
        
        # æ£€æŸ¥æ˜¯å¦æœ‰APIå¯†é’¥
        api_key = os.getenv("DEEPSEEK_API_KEY")
        if not api_key:
            print("âš ï¸ æœªè®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡ï¼Œè·³è¿‡æµ‹è¯•")
            return False
        
        print(f"ğŸ”§ åˆ›å»ºDeepSeekç›´æ¥é€‚é…å™¨...")
        adapter = DeepSeekDirectAdapter(
            model="deepseek-chat",
            temperature=0.7,
            max_tokens=100,
            api_key=api_key,
            stream=True  # é»˜è®¤ä½¿ç”¨æµå¼è¯·æ±‚
        )
        
        print(f"âœ… é€‚é…å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ç”Ÿæˆå“åº”
        print(f"ğŸ“ æµ‹è¯•ç”Ÿæˆå“åº”...")
        result = adapter.invoke("è¯·ç®€å•ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ ï¼Œç”¨ä¸­æ–‡å›ç­”", stream=True)
        
        if result:
            print(f"âœ… ç”ŸæˆæˆåŠŸï¼Œå“åº”é•¿åº¦: {len(result)}")
            print(f"ğŸ“„ å“åº”é¢„è§ˆ: {result[:100]}...")
            return True
        else:
            print(f"âŒ ç”Ÿæˆå¤±è´¥ï¼Œæœªæ”¶åˆ°æœ‰æ•ˆå“åº”")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_openai_compatible_base():
    """æµ‹è¯•OpenAIå…¼å®¹åŸºç±»çš„æµå¼è¯·æ±‚"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•OpenAIå…¼å®¹åŸºç±»é€‚é…å™¨ - æµå¼è¯·æ±‚")
    print("=" * 60)
    
    try:
        from tradingagents.llm_adapters.openai_compatible_base import ChatDeepSeekOpenAI
        from langchain_core.messages import HumanMessage
        
        # æ£€æŸ¥æ˜¯å¦æœ‰APIå¯†é’¥
        api_key = os.getenv("DEEPSEEK_API_KEY")
        if not api_key:
            print("âš ï¸ æœªè®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡ï¼Œè·³è¿‡æµ‹è¯•")
            return False
        
        print(f"ğŸ”§ åˆ›å»ºOpenAIå…¼å®¹é€‚é…å™¨...")
        llm = ChatDeepSeekOpenAI(
            model="deepseek-chat",
            api_key=api_key,
            temperature=0.7,
            max_tokens=100,
            # æµ‹è¯•é¢å¤–å‚æ•°ä¼ é€’
            top_p=0.9
        )
        
        print(f"âœ… é€‚é…å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ç”Ÿæˆå“åº”
        print(f"ğŸ“ æµ‹è¯•ç”Ÿæˆå“åº”...")
        messages = [HumanMessage(content="è¯·ç®€å•ä»‹ç»ä¸€ä¸‹æ·±åº¦å­¦ä¹ ï¼Œç”¨ä¸­æ–‡å›ç­”")]
        
        result = llm._generate(messages)
        
        if result and result.generations:
            response_content = result.generations[0].message.content
            print(f"âœ… ç”ŸæˆæˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response_content)}")
            print(f"ğŸ“„ å“åº”é¢„è§ˆ: {response_content[:100]}...")
            return True
        else:
            print(f"âŒ ç”Ÿæˆå¤±è´¥ï¼Œæœªæ”¶åˆ°æœ‰æ•ˆå“åº”")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•LLMé€‚é…å™¨æµå¼è¯·æ±‚åŠŸèƒ½")
    print("æ—¶é—´:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    
    tests = [
        ("ç¬¬ä¸‰æ–¹OpenAIé€‚é…å™¨", test_third_party_openai_streaming),
        ("DashScopeé€‚é…å™¨", test_dashscope_streaming),
        ("DeepSeekç›´æ¥é€‚é…å™¨", test_deepseek_streaming),
        ("OpenAIå…¼å®¹åŸºç±»", test_openai_compatible_base),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            success = test_func()
            if success:
                passed += 1
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å‡ºç°å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 60)
    print(f"æ€»æµ‹è¯•æ•°: {total}")
    print(f"é€šè¿‡æµ‹è¯•: {passed}")
    print(f"å¤±è´¥æµ‹è¯•: {total - passed}")
    print(f"æˆåŠŸç‡: {passed/total*100:.1f}%")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰LLMé€‚é…å™¨æµå¼è¯·æ±‚æµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
        print("âœ… å‚æ•°è¿‡æ»¤å·²æˆåŠŸç§»é™¤")
        print("âœ… æµå¼è¯·æ±‚å·²æˆåŠŸå¯ç”¨")
    else:
        print(f"\nâš ï¸ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)