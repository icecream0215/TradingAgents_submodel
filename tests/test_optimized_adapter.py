#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ä¼˜åŒ–åçš„é€‚é…å™¨ - é¿å…LangChainæµå¼é—®é¢˜
ç›´æ¥ä½¿ç”¨æœ€ä½³å®ç°ï¼Œä¸å†"è¯•é”™"
"""

import os
import sys
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from tradingagents.config.config_manager import config_manager, token_tracker
from tradingagents.utils.logging_manager import get_logger
from tradingagents.llm_adapters.third_party_openai import ThirdPartyOpenAI

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)-20s | %(levelname)-8s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

logger = logging.getLogger('optimized_test')

class OptimizedOpenAI(ThirdPartyOpenAI):
    """
    ä¼˜åŒ–çš„OpenAIé€‚é…å™¨ - é¿å…LangChainæµå¼é—®é¢˜
    ç›´æ¥ä½¿ç”¨æœ€ä½³å®ç°ï¼Œä¸å†"è¯•é”™"
    """
    
    def __init__(self, *args, **kwargs):
        # æå–è‡ªå®šä¹‰å‚æ•°ï¼Œé¿å…ä¼ é€’ç»™çˆ¶ç±»
        user_wants_streaming = kwargs.pop('user_wants_streaming', False)
        super().__init__(*args, **kwargs)
        # åœ¨çˆ¶ç±»åˆå§‹åŒ–åè®¾ç½®è‡ªå®šä¹‰å±æ€§
        object.__setattr__(self, 'user_wants_streaming', user_wants_streaming)
        logger.info("ğŸš€ åˆå§‹åŒ–ä¼˜åŒ–çš„OpenAIé€‚é…å™¨")
    
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        """
        ä¼˜åŒ–çš„ç”Ÿæˆæ–¹æ³•ï¼š
        1. ç›´æ¥ä½¿ç”¨æˆ‘ä»¬çš„_direct_api_callï¼Œé¿å…LangChainé—®é¢˜
        2. ä¸å†"è¯•é”™"ï¼Œç›´æ¥ä½¿ç”¨æœ€ä½³å®ç°
        3. æ”¯æŒæµå¼å’Œéæµå¼ï¼Œä½†é»˜è®¤ä½¿ç”¨éæµå¼ï¼ˆæ›´ç¨³å®šï¼‰
        """
        # ä¿å­˜è‡ªå®šä¹‰å‚æ•°ç”¨äºtokenè·Ÿè¸ª
        custom_kwargs = {
            'session_id': kwargs.pop('session_id', None),
            'analysis_type': kwargs.pop('analysis_type', 'stock_analysis')
        }
        
        try:
            # ğŸ¯ ç›´æ¥ä½¿ç”¨æˆ‘ä»¬çš„_direct_api_callæ–¹æ³•
            # æ ¹æ®ç”¨æˆ·æ„å›¾é€‰æ‹©æµå¼æˆ–éæµå¼
            wants_streaming = self.user_wants_streaming
            kwargs_stream = kwargs.get('stream', False)
            
            if isinstance(kwargs_stream, bool) and kwargs_stream:
                wants_streaming = True
                
            logger.info(f"ğŸš€ ä½¿ç”¨ä¼˜åŒ–çš„ç›´æ¥APIè°ƒç”¨ (stream={wants_streaming})")
            result = self._direct_api_call(messages, stream=wants_streaming)
            
            # Tokenè·Ÿè¸ª
            self._track_token_usage(custom_kwargs)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–é€‚é…å™¨å¤±è´¥: {e}")
            # åˆ›å»ºé”™è¯¯å“åº”
            return self._create_error_response(
                f"ğŸš¨ APIè°ƒç”¨å¤±è´¥: {str(e)}"
            )
    
    def _track_token_usage(self, custom_kwargs):
        """è·Ÿè¸ªtokenä½¿ç”¨é‡"""
        try:
            if hasattr(self, '_last_api_usage') and self._last_api_usage:
                usage = self._last_api_usage
                input_tokens = usage.get('prompt_tokens', 0)
                output_tokens = usage.get('completion_tokens', 0)
                
                if input_tokens > 0 or output_tokens > 0:
                    from tradingagents.config.config_manager import TOKEN_TRACKING_ENABLED
                    
                    if TOKEN_TRACKING_ENABLED:
                        session_id = self.session_id or custom_kwargs.get('session_id') or f"optimized_{hash(datetime.now())%10000}"
                        analysis_type = custom_kwargs.get('analysis_type', 'stock_analysis')
                        
                        token_tracker.track_usage(
                            provider="optimized_openai",
                            model_name=getattr(self, 'model_name', 'unknown'),
                            input_tokens=input_tokens,
                            output_tokens=output_tokens,
                            session_id=session_id,
                            analysis_type=analysis_type
                        )
                        logger.info(f"ğŸ“Š [token] ä¼˜åŒ–é€‚é…å™¨è®°å½•: {input_tokens}+{output_tokens}={input_tokens+output_tokens}")
                    
        except Exception as track_error:
            logger.error(f"âš ï¸ Token è¿½è¸ªå¤±è´¥: {track_error}")

def test_optimized_adapter():
    """æµ‹è¯•ä¼˜åŒ–çš„é€‚é…å™¨"""
    
    print("ğŸ¯ æµ‹è¯•ä¼˜åŒ–çš„OpenAIé€‚é…å™¨")
    print("=" * 50)
    
    try:
        # æ˜¾ç¤ºæµ‹è¯•å‰ç»Ÿè®¡
        def display_stats():
            try:
                stats = config_manager.get_usage_statistics(7)
                logger.info(f"ğŸ“Š æœ€è¿‘7å¤©: æˆæœ¬Â¥{stats['total_cost']:.6f}, è¯·æ±‚{stats['total_requests']}æ¬¡")
            except Exception as e:
                logger.error(f"âŒ æ˜¾ç¤ºç»Ÿè®¡å¤±è´¥: {e}")
        
        print("ğŸ“Š æµ‹è¯•å‰ç»Ÿè®¡:")
        display_stats()
        before_records = len(config_manager.load_usage_records())
        
        # 1. æµ‹è¯•éæµå¼æ¨¡å¼ï¼ˆæ¨èï¼Œæ›´ç¨³å®šï¼‰
        print("\n1ï¸âƒ£ æµ‹è¯•éæµå¼æ¨¡å¼ï¼ˆæ¨èï¼‰...")
        
        llm_stable = OptimizedOpenAI(
            model=os.getenv('OPENAI_MODEL_NAME', 'Qwen/Qwen3-235B-A22B-Instruct-2507'),
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=os.getenv('OPENAI_API_BASE', 'https://llm.submodel.ai/v1'),
            temperature=0.7,
            streaming=False,  # éæµå¼ï¼Œæ›´ç¨³å®š
            user_wants_streaming=False
        )
        
        from langchain_core.messages import HumanMessage
        test_query = "ç®€è¦åˆ†æå½“å‰ç§‘æŠ€è‚¡çš„å¸‚åœºè¡¨ç°ï¼Œä¸è¶…è¿‡30å­—ã€‚"
        messages = [HumanMessage(content=test_query)]
        
        logger.info(f"ğŸ“ æµ‹è¯•æŸ¥è¯¢: {test_query}")
        start_time = datetime.now()
        
        result = llm_stable._generate(messages)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        if result and result.generations:
            response = result.generations[0].message.content
            logger.info(f"âœ… éæµå¼å“åº”æˆåŠŸ ({duration:.2f}ç§’)")
            logger.info(f"ğŸ“„ å“åº”: {response}")
        else:
            logger.error("âŒ éæµå¼å“åº”å¤±è´¥")
        
        # 2. æµ‹è¯•æµå¼æ¨¡å¼
        print("\n2ï¸âƒ£ æµ‹è¯•æµå¼æ¨¡å¼...")
        
        llm_stream = OptimizedOpenAI(
            model=os.getenv('OPENAI_MODEL_NAME', 'Qwen/Qwen3-235B-A22B-Instruct-2507'),
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=os.getenv('OPENAI_API_BASE', 'https://llm.submodel.ai/v1'),
            temperature=0.7,
            streaming=True,  # æµå¼
            user_wants_streaming=True
        )
        
        test_query2 = "ç®€è¦è¯´æ˜AIåœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨ï¼Œä¸è¶…è¿‡30å­—ã€‚"
        messages2 = [HumanMessage(content=test_query2)]
        
        logger.info(f"ğŸ“ æµ‹è¯•æŸ¥è¯¢: {test_query2}")
        start_time = datetime.now()
        
        result2 = llm_stream._generate(messages2, stream=True)
        
        end_time = datetime.now()
        duration2 = (end_time - start_time).total_seconds()
        
        if result2 and result2.generations:
            response2 = result2.generations[0].message.content
            logger.info(f"âœ… æµå¼å“åº”æˆåŠŸ ({duration2:.2f}ç§’)")
            logger.info(f"ğŸ“„ å“åº”: {response2}")
        else:
            logger.error("âŒ æµå¼å“åº”å¤±è´¥")
        
        # éªŒè¯æ”¹è¿›æ•ˆæœ
        import time
        time.sleep(2)
        
        after_records = len(config_manager.load_usage_records())
        new_records = after_records - before_records
        
        print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœ:")
        print(f"   â±ï¸ éæµå¼è€—æ—¶: {duration:.2f}ç§’")
        print(f"   â±ï¸ æµå¼è€—æ—¶: {duration2:.2f}ç§’") 
        print(f"   ğŸ“Š æ–°å¢è®°å½•: {new_records}æ¡")
        
        if new_records > 0:
            print("âœ… ä¼˜åŒ–æˆåŠŸï¼š")
            print("   ğŸš€ é¿å…äº†'è¯•é”™'æœºåˆ¶ï¼Œç›´æ¥ä½¿ç”¨æœ€ä½³å®ç°")
            print("   ğŸ“Š å‡†ç¡®çš„tokenç»Ÿè®¡")
            print("   âš¡ æ›´å¿«çš„å“åº”é€Ÿåº¦")
        
        print("\nğŸ“Š æµ‹è¯•åç»Ÿè®¡:")
        display_stats()
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_production_adapter():
    """åˆ›å»ºç”Ÿäº§ç¯å¢ƒç”¨çš„ä¼˜åŒ–é€‚é…å™¨"""
    
    print("\nğŸ”§ åˆ›å»ºç”Ÿäº§ç¯å¢ƒä¼˜åŒ–é€‚é…å™¨...")
    
    production_code = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–çš„OpenAIé€‚é…å™¨
ç›´æ¥ä½¿ç”¨æœ€ä½³å®ç°ï¼Œé¿å…LangChainæµå¼é—®é¢˜
"""

from tradingagents.llm_adapters.third_party_openai import ThirdPartyOpenAI
from tradingagents.config.config_manager import token_tracker, TOKEN_TRACKING_ENABLED
from tradingagents.utils.logging_manager import get_logger
from datetime import datetime

logger = get_logger("production_adapter")

class ProductionOpenAI(ThirdPartyOpenAI):
    """
    ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–çš„OpenAIé€‚é…å™¨
    - ç›´æ¥ä½¿ç”¨æœ€ä½³å®ç°ï¼Œä¸å†"è¯•é”™"
    - é»˜è®¤éæµå¼æ¨¡å¼ï¼Œæ›´ç¨³å®š
    - ä¿æŒå‡†ç¡®çš„tokenç»Ÿè®¡
    """
    
    def __init__(self, *args, **kwargs):
        # å¼ºåˆ¶è®¾ç½®ä¸ºéæµå¼æ¨¡å¼ï¼ˆæ›´ç¨³å®šï¼‰
        kwargs['streaming'] = kwargs.get('streaming', False)
        super().__init__(*args, **kwargs)
        
        # æ ‡è®°è¿™æ˜¯ä¼˜åŒ–ç‰ˆæœ¬
        self.is_optimized = True
        logger.info("ğŸš€ åˆå§‹åŒ–ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–é€‚é…å™¨")
    
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        """
        ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–çš„ç”Ÿæˆæ–¹æ³•
        """
        # ä¿å­˜è‡ªå®šä¹‰å‚æ•°
        session_id = kwargs.pop('session_id', None)
        analysis_type = kwargs.pop('analysis_type', 'stock_analysis')
        
        try:
            # ğŸ¯ æ ¸å¿ƒä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨_direct_api_call
            # é¿å…LangChainçš„"è¯•é”™"æœºåˆ¶
            
            # æ ¹æ®é…ç½®é€‰æ‹©æµå¼æ¨¡å¼
            use_streaming = kwargs.get('stream', getattr(self, 'streaming', False))
            
            logger.debug(f"ğŸ”„ ç›´æ¥APIè°ƒç”¨ (stream={use_streaming})")
            
            # ç›´æ¥è°ƒç”¨æœ€ä½³å®ç°
            result = self._direct_api_call(messages, stream=use_streaming)
            
            # Tokenè·Ÿè¸ª
            self._track_production_usage(session_id, analysis_type)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿäº§é€‚é…å™¨è°ƒç”¨å¤±è´¥: {e}")
            return self._create_error_response(f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
    
    def _track_production_usage(self, session_id, analysis_type):
        """ç”Ÿäº§ç¯å¢ƒtokenä½¿ç”¨è·Ÿè¸ª"""
        try:
            if hasattr(self, '_last_api_usage') and self._last_api_usage:
                usage = self._last_api_usage
                input_tokens = usage.get('prompt_tokens', 0)
                output_tokens = usage.get('completion_tokens', 0)
                
                if (input_tokens > 0 or output_tokens > 0) and TOKEN_TRACKING_ENABLED:
                    effective_session_id = (
                        session_id or 
                        getattr(self, 'session_id', None) or 
                        f"prod_{hash(datetime.now())%10000}"
                    )
                    
                    token_tracker.track_usage(
                        provider="production_openai",
                        model_name=getattr(self, 'model_name', 'unknown'),
                        input_tokens=input_tokens,
                        output_tokens=output_tokens,
                        session_id=effective_session_id,
                        analysis_type=analysis_type
                    )
                    
                    logger.debug(f"ğŸ“Š tokenè®°å½•: {input_tokens}+{output_tokens}")
                    
        except Exception as e:
            logger.warning(f"âš ï¸ Tokenè·Ÿè¸ªå¤±è´¥: {e}")
'''
    
    # ä¿å­˜ç”Ÿäº§é€‚é…å™¨åˆ°æ–‡ä»¶
    production_file = '/root/TradingAgents/tradingagents/llm_adapters/production_openai.py'
    
    try:
        with open(production_file, 'w', encoding='utf-8') as f:
            f.write(production_code)
        
        logger.info(f"âœ… ç”Ÿäº§é€‚é…å™¨å·²ä¿å­˜åˆ°: {production_file}")
        
        usage_example = '''
# ä½¿ç”¨ç¤ºä¾‹ï¼š
from tradingagents.llm_adapters.production_openai import ProductionOpenAI

# åˆ›å»ºä¼˜åŒ–çš„é€‚é…å™¨å®ä¾‹
llm = ProductionOpenAI(
    model=os.getenv('OPENAI_MODEL_NAME'),
    api_key=os.getenv('OPENAI_API_KEY'),
    base_url=os.getenv('OPENAI_API_BASE'),
    temperature=0.7,
    streaming=False  # æ¨èéæµå¼æ¨¡å¼
)

# ç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€æ‹…å¿ƒ500é”™è¯¯å’Œ"è¯•é”™"é—®é¢˜
response = llm.invoke("åˆ†æè‚¡å¸‚è¶‹åŠ¿")
'''
        
        print("ğŸ“ ä½¿ç”¨ç¤ºä¾‹:")
        print(usage_example)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜ç”Ÿäº§é€‚é…å™¨å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    try:
        # åŠ è½½ç¯å¢ƒå˜é‡
        from dotenv import load_dotenv
        load_dotenv()
        
        if not os.getenv('OPENAI_API_KEY'):
            logger.error("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
            sys.exit(1)
        
        logger.info("âœ… ç¯å¢ƒé…ç½®åŠ è½½æˆåŠŸ")
        
        # è¿è¡Œæµ‹è¯•
        test_result = test_optimized_adapter()
        
        # åˆ›å»ºç”Ÿäº§é€‚é…å™¨
        production_result = create_production_adapter()
        
        print("\n" + "=" * 50)
        print("ğŸ“‹ ä¼˜åŒ–æ€»ç»“:")
        if test_result:
            print("ğŸ‰ ä¼˜åŒ–é€‚é…å™¨æµ‹è¯•å®Œæˆï¼")
            print("\nğŸ’¡ ä¸»è¦æ”¹è¿›:")
            print("   1. ğŸš€ é¿å…LangChainæµå¼å¤„ç†çš„ä¸ç¨³å®šæ€§")
            print("   2. ğŸ¯ ç›´æ¥ä½¿ç”¨æœ€ä½³å®ç°ï¼Œä¸å†'è¯•é”™'")
            print("   3. âš¡ æ”¯æŒæµå¼å’Œéæµå¼ï¼Œé»˜è®¤éæµå¼æ›´ç¨³å®š")
            print("   4. ğŸ“Š ä¿æŒå‡†ç¡®çš„tokenç»Ÿè®¡")
            print("\nğŸ”§ ä½¿ç”¨å»ºè®®:")
            print("   - ä¼˜å…ˆä½¿ç”¨éæµå¼æ¨¡å¼ï¼ˆæ›´ç¨³å®šï¼Œæ— 500é”™è¯¯ï¼‰")
            print("   - å¿…è¦æ—¶æ‰ä½¿ç”¨æµå¼æ¨¡å¼")
            print("   - æ›¿æ¢ç°æœ‰çš„ThirdPartyOpenAIä¸ºProductionOpenAI")
            
            if production_result:
                print("   - å·²åˆ›å»ºç”Ÿäº§ç¯å¢ƒé€‚é…å™¨ï¼šproduction_openai.py")
        else:
            print("âŒ æµ‹è¯•å¤±è´¥")
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æµ‹è¯•è¢«ç”¨æˆ·å–æ¶ˆ")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
