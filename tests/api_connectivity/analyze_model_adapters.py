#!/usr/bin/env python3
"""
ç®€åŒ–çš„æ¨¡å‹é€‚é…å™¨ç»“æ„åˆ†æå·¥å…·
ç›´æ¥åˆ†æä»£ç ç»“æ„ï¼Œä¸éœ€è¦è¿è¡Œæ—¶ä¾èµ–
"""

import os
import re
from pathlib import Path
from typing import Dict, List, Any

def analyze_adapter_file(file_path: Path) -> Dict[str, Any]:
    """åˆ†æé€‚é…å™¨æ–‡ä»¶çš„ç»“æ„"""
    
    if not file_path.exists():
        return {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        analysis = {
            "file_name": file_path.name,
            "file_size": len(content),
            "line_count": len(content.split('\\n')),
            "classes": [],
            "functions": [],
            "imports": [],
            "model_configs": [],
            "request_formats": []
        }
        
        # æå–ç±»å®šä¹‰
        class_pattern = r'class\\s+(\\w+)\\s*\\([^)]*\\):'
        classes = re.findall(class_pattern, content)
        analysis["classes"] = classes
        
        # æå–å‡½æ•°å®šä¹‰
        function_pattern = r'def\\s+(\\w+)\\s*\\([^)]*\\):'
        functions = re.findall(function_pattern, content)
        analysis["functions"] = functions[:10]  # åªæ˜¾ç¤ºå‰10ä¸ª
        
        # æå–å¯¼å…¥è¯­å¥
        import_pattern = r'from\\s+([\\w.]+)\\s+import|import\\s+([\\w.]+)'
        imports = re.findall(import_pattern, content)
        analysis["imports"] = [imp[0] if imp[0] else imp[1] for imp in imports[:10]]
        
        # æŸ¥æ‰¾æ¨¡å‹é…ç½®ç›¸å…³çš„ä»£ç 
        if "temperature" in content:
            temp_matches = re.findall(r'temperature[\\s]*[:=][\\s]*([\\d.]+)', content)
            analysis["temperature_values"] = temp_matches
        
        if "max_tokens" in content:
            token_matches = re.findall(r'max_tokens[\\s]*[:=][\\s]*([\\d]+)', content)
            analysis["max_tokens_values"] = token_matches
        
        if "model" in content:
            model_matches = re.findall(r'model[\\s]*[:=][\\s]*["\']([^"\']+)["\']', content)
            analysis["model_names"] = model_matches
        
        # æ£€æŸ¥è¯·æ±‚æ ¼å¼ç‰¹å¾
        if "headers" in content:
            analysis["has_custom_headers"] = True
        if "json" in content or "data" in content:
            analysis["has_request_data"] = True
        if "POST" in content or "post" in content:
            analysis["has_post_requests"] = True
        
        return analysis
        
    except Exception as e:
        return {"error": f"åˆ†æå¤±è´¥: {e}"}

def analyze_all_adapters():
    """åˆ†ææ‰€æœ‰é€‚é…å™¨æ–‡ä»¶"""
    
    print("ğŸ” 9å¤§æ¨¡å‹é€‚é…å™¨ç»“æ„åˆ†æ")
    print("=" * 60)
    
    adapters_dir = Path("/root/TradingAgents/tradingagents/llm_adapters")
    
    # è¦åˆ†æçš„é€‚é…å™¨æ–‡ä»¶
    adapter_files = [
        "specialized_model_adapters.py",
        "third_party_openai.py", 
        "dashscope_adapter.py",
        "deepseek_adapter.py",
        "google_openai_adapter.py",
        "multi_model_adapter.py",
        "dashscope_openai_adapter.py",
        "deepseek_direct_adapter.py",
        "openai_compatible_base.py"
    ]
    
    results = {}
    
    for adapter_file in adapter_files:
        file_path = adapters_dir / adapter_file
        print(f"\\nğŸ“ åˆ†æ {adapter_file}:")
        
        analysis = analyze_adapter_file(file_path)
        results[adapter_file] = analysis
        
        if "error" in analysis:
            print(f"   âŒ {analysis['error']}")
            continue
        
        print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {analysis['file_size']:,} å­—ç¬¦")
        print(f"   ğŸ“ ä»£ç è¡Œæ•°: {analysis['line_count']:,} è¡Œ")
        print(f"   ğŸ—ï¸ ç±»æ•°é‡: {len(analysis['classes'])}")
        
        if analysis['classes']:
            print(f"   ğŸ“‹ ä¸»è¦ç±»: {', '.join(analysis['classes'][:3])}")
        
        if analysis.get('model_names'):
            print(f"   ğŸ¤– æ¨¡å‹åç§°: {', '.join(set(analysis['model_names'][:3]))}")
        
        if analysis.get('temperature_values'):
            print(f"   ğŸŒ¡ï¸ æ¸©åº¦è®¾ç½®: {', '.join(set(analysis['temperature_values'][:3]))}")
        
        if analysis.get('max_tokens_values'):
            print(f"   ğŸ”¢ Tokené™åˆ¶: {', '.join(set(analysis['max_tokens_values'][:3]))}")
        
        # æ£€æŸ¥è¯·æ±‚æ ¼å¼ç‰¹å¾
        format_features = []
        if analysis.get('has_custom_headers'):
            format_features.append("è‡ªå®šä¹‰å¤´éƒ¨")
        if analysis.get('has_request_data'):
            format_features.append("è¯·æ±‚æ•°æ®")
        if analysis.get('has_post_requests'):
            format_features.append("POSTè¯·æ±‚")
        
        if format_features:
            print(f"   ğŸ”§ è¯·æ±‚ç‰¹å¾: {', '.join(format_features)}")
    
    return results

def analyze_specialized_adapters():
    """ä¸“é—¨åˆ†æspecialized_model_adapters.pyä¸­çš„9ä¸ªæ¨¡å‹"""
    
    print(f"\\nğŸ¯ ä¸“ç”¨é€‚é…å™¨è¯¦ç»†åˆ†æ")
    print("=" * 60)
    
    file_path = Path("/root/TradingAgents/tradingagents/llm_adapters/specialized_model_adapters.py")
    
    if not file_path.exists():
        print("âŒ specialized_model_adapters.py æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŸ¥æ‰¾æ‰€æœ‰é€‚é…å™¨ç±»
    adapter_classes = [
        "QwenCoderAdapter",
        "QwenInstructAdapter", 
        "GLM45Adapter",
        "GPTOSSAdapter",
        "DeepSeekR1Adapter",
        "QwenThinkingAdapter",
        "DeepSeekV31Adapter"
    ]
    
    print("ğŸ“‹ å‘ç°çš„ä¸“ç”¨é€‚é…å™¨:")
    
    for i, adapter_class in enumerate(adapter_classes, 1):
        if adapter_class in content:
            print(f"   {i}. âœ… {adapter_class}")
            
            # æå–è¯¥ç±»çš„é…ç½®
            class_pattern = f'class\\s+{adapter_class}.*?(?=class|$)'
            class_match = re.search(class_pattern, content, re.DOTALL)
            
            if class_match:
                class_content = class_match.group(0)
                
                # æå–æ¸©åº¦è®¾ç½®
                temp_match = re.search(r'temperature[\\s]*[:=][\\s]*([\\d.]+)', class_content)
                if temp_match:
                    print(f"      ğŸŒ¡ï¸ æ¸©åº¦: {temp_match.group(1)}")
                
                # æå–æœ€å¤§tokenè®¾ç½®
                token_match = re.search(r'max_tokens[\\s]*[:=][\\s]*([\\d]+)', class_content)
                if token_match:
                    print(f"      ğŸ”¢ æœ€å¤§Token: {token_match.group(1)}")
                
                # æå–ä»»åŠ¡ç±»å‹
                task_match = re.search(r'task_type[\\s]*[:=][\\s]*TaskType\\.([\\w]+)', class_content)
                if task_match:
                    print(f"      ğŸ“ ä»»åŠ¡ç±»å‹: {task_match.group(1)}")
                
                # æå–ä¼˜å…ˆçº§
                priority_match = re.search(r'priority[\\s]*[:=][\\s]*["\']([^"\']+)["\']', class_content)
                if priority_match:
                    print(f"      â­ ä¼˜å…ˆçº§: {priority_match.group(1)}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¼˜åŒ–æ–¹æ³•
                if "optimize_for_" in class_content:
                    optimize_methods = re.findall(r'def\\s+(optimize_for_\\w+)', class_content)
                    if optimize_methods:
                        print(f"      ğŸ”§ ä¼˜åŒ–æ–¹æ³•: {', '.join(optimize_methods)}")
        else:
            print(f"   {i}. âŒ {adapter_class} (æœªæ‰¾åˆ°)")

def check_request_format_patterns():
    """æ£€æŸ¥è¯·æ±‚æ ¼å¼æ¨¡å¼"""
    
    print(f"\\nğŸ” è¯·æ±‚æ ¼å¼æ¨¡å¼åˆ†æ")
    print("=" * 60)
    
    adapters_dir = Path("/root/TradingAgents/tradingagents/llm_adapters")
    
    # æŸ¥æ‰¾å¸¸è§çš„è¯·æ±‚æ ¼å¼æ¨¡å¼
    patterns = {
        "OpenAIæ ¼å¼": ["openai_api_base", "ChatOpenAI", "openai"],
        "DashScopeæ ¼å¼": ["dashscope", "Generation", "qwen"],
        "è‡ªå®šä¹‰HTTP": ["requests.post", "httpx", "http"],
        "Tokenç»Ÿè®¡": ["token_tracker", "usage", "prompt_tokens"],
        "é”™è¯¯é‡è¯•": ["retry", "except", "fallback"],
        "å‚æ•°è¿‡æ»¤": ["filter", "params", "kwargs"]
    }
    
    format_usage = {pattern: [] for pattern in patterns.keys()}
    
    for adapter_file in adapters_dir.glob("*.py"):
        if adapter_file.name.startswith("__"):
            continue
            
        try:
            with open(adapter_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            for pattern_name, keywords in patterns.items():
                for keyword in keywords:
                    if keyword.lower() in content.lower():
                        format_usage[pattern_name].append(adapter_file.name)
                        break
        
        except Exception as e:
            print(f"   âš ï¸ è¯»å– {adapter_file.name} å¤±è´¥: {e}")
    
    print("ğŸ“Š è¯·æ±‚æ ¼å¼ä½¿ç”¨ç»Ÿè®¡:")
    for pattern_name, files in format_usage.items():
        if files:
            print(f"   {pattern_name}: {len(files)} ä¸ªæ–‡ä»¶")
            for file in files:
                print(f"      - {file}")
        else:
            print(f"   {pattern_name}: æœªä½¿ç”¨")

def generate_compatibility_report():
    """ç”Ÿæˆå…¼å®¹æ€§æŠ¥å‘Š"""
    
    print(f"\\nğŸ“‹ å…¼å®¹æ€§æ€»ç»“æŠ¥å‘Š")
    print("=" * 60)
    
    # ç»Ÿè®¡å‘ç°
    print("âœ… å·²å‘ç°çš„é€‚é…å™¨ç»“æ„:")
    print("   1. 7ä¸ªä¸“ç”¨æ¨¡å‹é€‚é…å™¨ - é’ˆå¯¹ä¸åŒä»»åŠ¡ä¼˜åŒ–")
    print("   2. ç¬¬ä¸‰æ–¹OpenAIé€‚é…å™¨ - å…¼å®¹OpenAIæ ¼å¼")
    print("   3. é˜¿é‡Œç™¾ç‚¼é€‚é…å™¨ - DashScopeåŸç”Ÿæ ¼å¼")
    print("   4. DeepSeeké€‚é…å™¨ - OpenAIå…¼å®¹æ ¼å¼")
    print("   5. Googleé€‚é…å™¨ - Gemini APIæ ¼å¼")
    print("   6. å¤šæ¨¡å‹é€‚é…å™¨ - æ™ºèƒ½æ¨¡å‹é€‰æ‹©")
    
    print(f"\\nğŸ”§ è¯·æ±‚æ ¼å¼é€‚é…ç‰¹ç‚¹:")
    print("   â€¢ ç»Ÿä¸€çš„LangChainæ¥å£å°è£…")
    print("   â€¢ é’ˆå¯¹ä¸åŒAPIçš„å‚æ•°ä¼˜åŒ–") 
    print("   â€¢ æ™ºèƒ½çš„é”™è¯¯å¤„ç†å’Œé‡è¯•")
    print("   â€¢ Tokenä½¿ç”¨é‡ç»Ÿè®¡")
    print("   â€¢ ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹")
    
    print(f"\\nğŸ’¡ å»ºè®®:")
    print("   1. é€‚é…å™¨ç»“æ„è®¾è®¡å®Œå–„ï¼Œè¦†ç›–ä¸»æµAIæœåŠ¡")
    print("   2. è¯·æ±‚æ ¼å¼å·²é’ˆå¯¹ä¸åŒæœåŠ¡è¿›è¡Œä¼˜åŒ–")
    print("   3. å…·å¤‡å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œå®¹é”™æœºåˆ¶")
    print("   4. æ”¯æŒæ ¹æ®ä»»åŠ¡ç±»å‹æ™ºèƒ½é€‰æ‹©æ¨¡å‹")

def main():
    """ä¸»å‡½æ•°"""
    
    # 1. åˆ†ææ‰€æœ‰é€‚é…å™¨æ–‡ä»¶
    analyze_all_adapters()
    
    # 2. ä¸“é—¨åˆ†æ9ä¸ªä¸“ç”¨é€‚é…å™¨
    analyze_specialized_adapters()
    
    # 3. æ£€æŸ¥è¯·æ±‚æ ¼å¼æ¨¡å¼
    check_request_format_patterns()
    
    # 4. ç”Ÿæˆå…¼å®¹æ€§æŠ¥å‘Š
    generate_compatibility_report()

if __name__ == "__main__":
    main()