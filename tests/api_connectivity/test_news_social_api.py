#!/usr/bin/env python3
"""
æ–°é—»å’Œç¤¾äº¤åª’ä½“APIè¿é€šæ€§æµ‹è¯•
æµ‹è¯•Google Newså’ŒReddit APIçš„è®¿é—®æ˜¯å¦æ­£å¸¸

æ”¯æŒçš„æ•°æ®ç±»å‹ï¼š
- Google News: å…¨çƒæ–°é—» (Global news via web scraping)
- Reddit: ç¤¾äº¤åª’ä½“æƒ…ç»ª (Social media sentiment via PRAW)
"""

import os
import sys
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List
import warnings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

class NewsAndSocialAPITester:
    """æ–°é—»å’Œç¤¾äº¤åª’ä½“APIæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.reddit_client_id = os.getenv('REDDIT_CLIENT_ID')
        self.reddit_client_secret = os.getenv('REDDIT_CLIENT_SECRET')
        self.reddit_user_agent = os.getenv('REDDIT_USER_AGENT', 'TradingAgents-Test/1.0')
        
    def test_google_news_scraping(self, query: str = "AAPL stock") -> bool:
        """æµ‹è¯•Google Newsç½‘é¡µçˆ¬è™«"""
        print(f"\nğŸ“° æµ‹è¯•Google Newsçˆ¬è™« (æŸ¥è¯¢: {query})...")
        try:
            import requests
            from bs4 import BeautifulSoup
            
            # æ„å»ºGoogle Newsæœç´¢URL
            search_url = f"https://news.google.com/search?q={query}&hl=en-US&gl=US&ceid=US:en"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(search_url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾æ–°é—»æ ‡é¢˜ (Google Newsçš„HTMLç»“æ„å¯èƒ½ä¼šå˜åŒ–)
                articles = soup.find_all('article')
                
                if articles:
                    print(f"âœ… Google Newsçˆ¬è™«æˆåŠŸ")
                    print(f"   æ‰¾åˆ°æ–‡ç« æ•°: {len(articles)}")
                    
                    # å°è¯•æå–å‰å‡ ä¸ªæ ‡é¢˜
                    for i, article in enumerate(articles[:3]):
                        title_elem = article.find('a')
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                            print(f"   æ–‡ç«  {i+1}: {title[:60]}...")
                    
                    return True
                else:
                    print("âš ï¸ æœªæ‰¾åˆ°æ–°é—»æ–‡ç« ï¼Œå¯èƒ½ç½‘é¡µç»“æ„å·²å˜åŒ–")
                    return False
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
                
        except ImportError:
            print("âŒ ç¼ºå°‘ä¾èµ–åº“")
            print("ğŸ’¡ è¯·å®‰è£…: pip install beautifulsoup4 requests")
            return False
        except Exception as e:
            print(f"âŒ Google Newsæµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def test_alternative_news_api(self, query: str = "Apple stock") -> bool:
        """æµ‹è¯•æ›¿ä»£æ–°é—»API (ä½¿ç”¨NewsAPIæˆ–å…¶ä»–å…è´¹API)"""
        print(f"\nğŸ“° æµ‹è¯•æ›¿ä»£æ–°é—»æº...")
        try:
            import requests
            
            # ä½¿ç”¨ä¸€ä¸ªå…è´¹çš„æ–°é—»API (ä¾‹å¦‚GNews API)
            api_key = os.getenv('GNEWS_API_KEY')  # å¦‚æœæœ‰çš„è¯
            
            if api_key:
                url = "https://gnews.io/api/v4/search"
                params = {
                    'q': query,
                    'token': api_key,
                    'lang': 'en',
                    'max': 5
                }
                
                response = requests.get(url, params=params, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    articles = data.get('articles', [])
                    
                    if articles:
                        print(f"âœ… æ–°é—»APIè¿æ¥æˆåŠŸ")
                        print(f"   æ–‡ç« æ•°é‡: {len(articles)}")
                        
                        for i, article in enumerate(articles[:3]):
                            print(f"   æ–‡ç«  {i+1}: {article['title'][:60]}...")
                        
                        return True
                    else:
                        print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–°é—»")
                        return False
                else:
                    print(f"âŒ APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    return False
            else:
                print("âš ï¸ æœªé…ç½®æ–°é—»APIå¯†é’¥ï¼Œè·³è¿‡æ­¤æµ‹è¯•")
                print("ğŸ’¡ å¯åœ¨ .env ä¸­è®¾ç½® GNEWS_API_KEY")
                return True  # æ²¡æœ‰APIå¯†é’¥ä¸ç®—å¤±è´¥
                
        except Exception as e:
            print(f"âŒ æ–°é—»APIæµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def test_reddit_api(self, subreddit: str = "stocks", limit: int = 5) -> bool:
        """æµ‹è¯•Reddit APIè¿æ¥"""
        print(f"\nğŸ”´ æµ‹è¯•Reddit API (r/{subreddit})...")
        try:
            import praw
            
            # æ£€æŸ¥APIå‡­æ®
            if not self.reddit_client_id or not self.reddit_client_secret:
                print("âš ï¸ Reddit APIå‡­æ®æœªé…ç½®")
                print("ğŸ’¡ è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®:")
                print("   REDDIT_CLIENT_ID=your_client_id")
                print("   REDDIT_CLIENT_SECRET=your_client_secret")
                return True  # æ²¡æœ‰å‡­æ®ä¸ç®—å¤±è´¥
            
            # åˆ›å»ºRedditå®ä¾‹
            reddit = praw.Reddit(
                client_id=self.reddit_client_id,
                client_secret=self.reddit_client_secret,
                user_agent=self.reddit_user_agent
            )
            
            # æµ‹è¯•è¿æ¥
            subreddit_obj = reddit.subreddit(subreddit)
            posts = list(subreddit_obj.hot(limit=limit))
            
            if posts:
                print(f"âœ… Reddit APIè¿æ¥æˆåŠŸ")
                print(f"   è·å–å¸–å­æ•°: {len(posts)}")
                
                for i, post in enumerate(posts):
                    print(f"   å¸–å­ {i+1}: {post.title[:50]}... (ğŸ‘{post.score})")
                
                return True
            else:
                print(f"âŒ æœªèƒ½è·å–r/{subreddit}çš„å¸–å­")
                return False
                
        except ImportError:
            print("âŒ PRAWåº“æœªå®‰è£…")
            print("ğŸ’¡ è¯·è¿è¡Œ: pip install praw")
            return False
        except Exception as e:
            print(f"âŒ Reddit APIæµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def test_reddit_sentiment_analysis(self, subreddit: str = "investing", query: str = "AAPL") -> bool:
        """æµ‹è¯•Redditæƒ…ç»ªåˆ†æ"""
        print(f"\nğŸ’­ æµ‹è¯•Redditæƒ…ç»ªåˆ†æ (r/{subreddit}, å…³é”®è¯: {query})...")
        try:
            import praw
            
            if not self.reddit_client_id or not self.reddit_client_secret:
                print("âš ï¸ Reddit APIå‡­æ®æœªé…ç½®ï¼Œè·³è¿‡æƒ…ç»ªåˆ†ææµ‹è¯•")
                return True
            
            reddit = praw.Reddit(
                client_id=self.reddit_client_id,
                client_secret=self.reddit_client_secret,
                user_agent=self.reddit_user_agent
            )
            
            # æœç´¢ç›¸å…³å¸–å­
            subreddit_obj = reddit.subreddit(subreddit)
            posts = list(subreddit_obj.search(query, limit=10))
            
            if posts:
                print(f"âœ… æ‰¾åˆ° {len(posts)} ä¸ªç›¸å…³å¸–å­")
                
                # ç®€å•çš„æƒ…ç»ªåˆ†æ (åŸºäºåˆ†æ•°)
                total_score = sum(post.score for post in posts)
                avg_score = total_score / len(posts)
                
                print(f"   æ€»åˆ†æ•°: {total_score}")
                print(f"   å¹³å‡åˆ†æ•°: {avg_score:.1f}")
                
                sentiment = "ç§¯æ" if avg_score > 0 else "æ¶ˆæ" if avg_score < 0 else "ä¸­æ€§"
                print(f"   æ•´ä½“æƒ…ç»ª: {sentiment}")
                
                return True
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°å…³äº '{query}' çš„ç›¸å…³å¸–å­")
                return True  # æ²¡æœ‰æ‰¾åˆ°å¸–å­ä¸ç®—å¤±è´¥
                
        except Exception as e:
            print(f"âŒ Redditæƒ…ç»ªåˆ†ææµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def run_all_tests(self) -> Dict[str, bool]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ–°é—»å’Œç¤¾äº¤åª’ä½“APIè¿é€šæ€§æµ‹è¯•...")
        print("=" * 50)
        
        results = {}
        
        # 1. æµ‹è¯•Google Newsçˆ¬è™«
        results['google_news_scraping'] = self.test_google_news_scraping()
        
        # 2. æµ‹è¯•æ›¿ä»£æ–°é—»API
        results['alternative_news_api'] = self.test_alternative_news_api()
        
        # 3. æµ‹è¯•Reddit API
        results['reddit_api'] = self.test_reddit_api()
        
        # 4. æµ‹è¯•Redditæƒ…ç»ªåˆ†æ
        results['reddit_sentiment'] = self.test_reddit_sentiment_analysis()
        
        # è¾“å‡ºæ€»ç»“
        print("\n" + "=" * 50)
        print("ğŸ“Š æ–°é—»å’Œç¤¾äº¤åª’ä½“APIæµ‹è¯•ç»“æœæ€»ç»“:")
        for test_name, result in results.items():
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            print(f"   {test_name}: {status}")
        
        success_rate = sum(results.values()) / len(results) * 100
        print(f"\nğŸ¯ æ€»ä½“æˆåŠŸç‡: {success_rate:.1f}%")
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    tester = NewsAndSocialAPITester()
    results = tester.run_all_tests()
    
    # è¿”å›é€€å‡ºç 
    if all(results.values()):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        sys.exit(0)
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥")
        sys.exit(1)

if __name__ == "__main__":
    main()