#!/usr/bin/env python3
"""
æ¨¡å‹é€‚é…å™¨è¯¦ç»†åˆ†æå·¥å…·
æ£€æŸ¥9å¤§æ¨¡å‹é€‚é…å™¨çš„å…·ä½“å®ç°å’Œå…¼å®¹æ€§
"""

import os
import re
import json
from pathlib import Path
from typing import Dict, List, Any

def read_file_safely(file_path: Path) -> str:
    """å®‰å…¨è¯»å–æ–‡ä»¶å†…å®¹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        return f"è¯»å–å¤±è´¥: {e}"

def extract_class_details(content: str, class_name: str) -> Dict[str, Any]:
    """æå–ç±»çš„è¯¦ç»†ä¿¡æ¯"""
    
    # æŸ¥æ‰¾ç±»å®šä¹‰çš„å¼€å§‹ä½ç½®
    class_pattern = rf'class\s+{class_name}\s*\([^)]*\):'
    class_match = re.search(class_pattern, content)
    
    if not class_match:
        return {"found": False}
    
    start_pos = class_match.start()
    
    # æ‰¾åˆ°ç±»çš„ç»“æŸä½ç½®ï¼ˆä¸‹ä¸€ä¸ªç±»æˆ–æ–‡ä»¶ç»“æŸï¼‰
    next_class_pattern = r'\nclass\s+\w+'
    next_class_match = re.search(next_class_pattern, content[start_pos + 1:])
    
    if next_class_match:
        end_pos = start_pos + next_class_match.start() + 1
        class_content = content[start_pos:end_pos]
    else:
        class_content = content[start_pos:]
    
    details = {"found": True, "content_length": len(class_content)}
    
    # æå–æ„é€ å‡½æ•°å‚æ•°
    init_pattern = r'def\s+__init__\s*\([^)]*\):'
    init_match = re.search(init_pattern, class_content)
    if init_match:
        init_params = re.findall(r'(\w+):\s*[^,)=]+(?:\s*=\s*[^,)]+)?', init_match.group(0))
        details["init_params"] = init_params
    
    # æå–é»˜è®¤é…ç½®å€¼
    temperature_match = re.search(r'temperature:\s*float\s*=\s*([0-9.]+)', class_content)
    if temperature_match:
        details["default_temperature"] = float(temperature_match.group(1))
    
    max_tokens_match = re.search(r'max_tokens:\s*Optional\[int\]\s*=\s*(\d+)', class_content)
    if max_tokens_match:
        details["default_max_tokens"] = int(max_tokens_match.group(1))
    
    # æå–ä»»åŠ¡ç±»å‹
    task_type_match = re.search(r'task_type=TaskType\.(\w+)', class_content)
    if task_type_match:
        details["task_type"] = task_type_match.group(1)
    
    # æå–ä¼˜å…ˆçº§
    priority_match = re.search(r'priority=["\']([^"\']+)["\']', class_content)
    if priority_match:
        details["priority"] = priority_match.group(1)
    
    # æå–æ¨¡å‹åç§°
    model_name_match = re.search(r'model_name=["\']([^"\']+)["\']', class_content)
    if model_name_match:
        details["model_name"] = model_name_match.group(1)
    
    # æŸ¥æ‰¾ä¼˜åŒ–æ–¹æ³•
    optimize_methods = re.findall(r'def\s+(optimize_for_\w+)', class_content)
    if optimize_methods:
        details["optimize_methods"] = optimize_methods
    
    # æŸ¥æ‰¾é‡å†™çš„æ–¹æ³•
    override_methods = re.findall(r'def\s+(_generate|_call)', class_content)
    if override_methods:
        details["override_methods"] = override_methods
    
    return details

def analyze_specialized_adapters():
    """åˆ†æä¸“ç”¨é€‚é…å™¨æ–‡ä»¶"""
    
    print("ğŸ¯ ä¸“ç”¨æ¨¡å‹é€‚é…å™¨è¯¦ç»†åˆ†æ")
    print("=" * 70)
    
    file_path = Path("/root/TradingAgents/tradingagents/llm_adapters/specialized_model_adapters.py")
    
    if not file_path.exists():
        print("âŒ specialized_model_adapters.py æ–‡ä»¶ä¸å­˜åœ¨")
        return {}
    
    content = read_file_safely(file_path)
    if content.startswith("è¯»å–å¤±è´¥"):
        print(f"âŒ {content}")
        return {}
    
    # å®šä¹‰9ä¸ªä¸“ç”¨é€‚é…å™¨
    adapter_classes = [
        "QwenCoderAdapter",      # 1. Qwen3 Coder 480B - ä»£ç ä¸“å®¶
        "QwenInstructAdapter",   # 2. Qwen3 Instruct 72B - æŒ‡ä»¤è·Ÿéš
        "GLM45Adapter",          # 3. GLM-4.5 FP8 - é«˜æ•ˆå¹³è¡¡
        "GPTOSSAdapter",         # 4. GPT-OSS 8x7B - å¼€æºæ›¿ä»£
        "DeepSeekR1Adapter",     # 5. DeepSeek R1 671B - æ¨ç†ä¸“å®¶
        "QwenThinkingAdapter",   # 6. Qwen3.5 Thinking - æ€ç»´é“¾
        "DeepSeekV31Adapter"     # 7. DeepSeek V3.1 685B - å…¨èƒ½æ¨¡å‹
    ]
    
    analysis_results = {}
    
    for i, adapter_class in enumerate(adapter_classes, 1):
        print(f"\\n{i}. ğŸ” {adapter_class}")
        print("-" * 50)
        
        details = extract_class_details(content, adapter_class)
        analysis_results[adapter_class] = details
        
        if not details["found"]:
            print("   âŒ æœªæ‰¾åˆ°è¯¥é€‚é…å™¨ç±»")
            continue
        
        # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        print(f"   âœ… ç±»å®šä¹‰å·²æ‰¾åˆ° ({details['content_length']} å­—ç¬¦)")
        
        if "model_name" in details:
            print(f"   ğŸ¤– æ¨¡å‹åç§°: {details['model_name']}")
        
        if "task_type" in details:
            print(f"   ğŸ“ ä»»åŠ¡ç±»å‹: {details['task_type']}")
        
        if "priority" in details:
            print(f"   â­ ä¼˜å…ˆçº§: {details['priority']}")
        
        if "default_temperature" in details:
            print(f"   ğŸŒ¡ï¸ é»˜è®¤æ¸©åº¦: {details['default_temperature']}")
        
        if "default_max_tokens" in details:
            print(f"   ğŸ”¢ é»˜è®¤Token: {details['default_max_tokens']}")
        
        if "optimize_methods" in details:
            print(f"   ğŸ”§ ä¼˜åŒ–æ–¹æ³•: {', '.join(details['optimize_methods'])}")
        
        if "override_methods" in details:
            print(f"   âš™ï¸ é‡å†™æ–¹æ³•: {', '.join(details['override_methods'])}")
        
        if "init_params" in details:
            print(f"   ğŸ“‹ åˆå§‹åŒ–å‚æ•°: {', '.join(details['init_params'][:5])}")  # æ˜¾ç¤ºå‰5ä¸ª
    
    return analysis_results

def analyze_other_adapters():
    """åˆ†æå…¶ä»–é€‚é…å™¨æ–‡ä»¶"""
    
    print(f"\\n\\nğŸ”§ å…¶ä»–é€‚é…å™¨æ–‡ä»¶åˆ†æ")
    print("=" * 70)
    
    adapters_dir = Path("/root/TradingAgents/tradingagents/llm_adapters")
    
    other_adapters = [
        ("third_party_openai.py", "ç¬¬ä¸‰æ–¹OpenAIé€‚é…å™¨"),
        ("dashscope_adapter.py", "é˜¿é‡Œç™¾ç‚¼DashScopeé€‚é…å™¨"),
        ("deepseek_adapter.py", "DeepSeekæ ‡å‡†é€‚é…å™¨"),
        ("google_openai_adapter.py", "Google Geminié€‚é…å™¨"),
        ("multi_model_adapter.py", "å¤šæ¨¡å‹åŸºç¡€é€‚é…å™¨")
    ]
    
    for filename, description in other_adapters:
        file_path = adapters_dir / filename
        print(f"\\nğŸ“ {description} ({filename})")
        
        if not file_path.exists():
            print("   âŒ æ–‡ä»¶ä¸å­˜åœ¨")
            continue
        
        content = read_file_safely(file_path)
        if content.startswith("è¯»å–å¤±è´¥"):
            print(f"   âŒ {content}")
            continue
        
        # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
        lines = content.split('\\n')
        print(f"   ğŸ“ ä»£ç è¡Œæ•°: {len(lines)}")
        print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {len(content):,} å­—ç¬¦")
        
        # æŸ¥æ‰¾ç±»å®šä¹‰
        classes = re.findall(r'class\\s+(\\w+)', content)
        if classes:
            print(f"   ğŸ—ï¸ å®šä¹‰çš„ç±»: {', '.join(classes[:3])}")
        
        # æŸ¥æ‰¾å…³é”®ç‰¹å¾
        features = []
        if "async def" in content:
            features.append("å¼‚æ­¥æ”¯æŒ")
        if "retry" in content.lower():
            features.append("é‡è¯•æœºåˆ¶")
        if "token" in content.lower():
            features.append("Tokenç»Ÿè®¡")
        if "error" in content.lower() or "exception" in content.lower():
            features.append("é”™è¯¯å¤„ç†")
        if "fallback" in content.lower():
            features.append("é™çº§æœºåˆ¶")
        
        if features:
            print(f"   âš¡ ç‰¹æ€§: {', '.join(features)}")

def check_compatibility_issues():
    """æ£€æŸ¥å…¼å®¹æ€§é—®é¢˜"""
    
    print(f"\\n\\nğŸ” å…¼å®¹æ€§é—®é¢˜æ£€æŸ¥")
    print("=" * 70)
    
    adapters_dir = Path("/root/TradingAgents/tradingagents/llm_adapters")
    issues = []
    
    # æ£€æŸ¥å¯¼å…¥é—®é¢˜
    for py_file in adapters_dir.glob("*.py"):
        if py_file.name.startswith("__"):
            continue
        
        content = read_file_safely(py_file)
        if content.startswith("è¯»å–å¤±è´¥"):
            continue
        
        # æ£€æŸ¥å¸¸è§çš„å¯¼å…¥é—®é¢˜
        if "langchain_core" in content and "from langchain_core" in content:
            import_lines = re.findall(r'from langchain_core[^\\n]*', content)
            for import_line in import_lines:
                issues.append(f"{py_file.name}: {import_line}")
    
    if issues:
        print("âš ï¸ å‘ç°çš„æ½œåœ¨å¯¼å…¥é—®é¢˜:")
        for issue in issues[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            print(f"   {issue}")
    else:
        print("âœ… æœªå‘ç°æ˜æ˜¾çš„å¯¼å…¥å…¼å®¹æ€§é—®é¢˜")
    
    # æ£€æŸ¥APIå¯†é’¥é…ç½®
    env_file = Path("/root/TradingAgents/.env")
    if env_file.exists():
        env_content = read_file_safely(env_file)
        
        required_keys = [
            "DASHSCOPE_API_KEY",
            "DEEPSEEK_API_KEY", 
            "GOOGLE_API_KEY",
            "OPENAI_API_KEY"
        ]
        
        print(f"\\nğŸ”‘ APIå¯†é’¥é…ç½®æ£€æŸ¥:")
        for key in required_keys:
            if key in env_content and f"{key}=" in env_content:
                # æ£€æŸ¥æ˜¯å¦æœ‰å€¼
                key_match = re.search(rf'{key}=([^\\n]*)', env_content)
                if key_match and key_match.group(1).strip():
                    print(f"   âœ… {key}: å·²é…ç½®")
                else:
                    print(f"   âš ï¸ {key}: å·²å®šä¹‰ä½†å¯èƒ½ä¸ºç©º")
            else:
                print(f"   âŒ {key}: æœªé…ç½®")

def generate_final_report():
    """ç”Ÿæˆæœ€ç»ˆå…¼å®¹æ€§æŠ¥å‘Š"""
    
    print(f"\\n\\nğŸ“‹ æ¨¡å‹é€‚é…å™¨å…¼å®¹æ€§æœ€ç»ˆæŠ¥å‘Š")
    print("=" * 70)
    
    print("âœ… æ¶æ„å®Œæ•´æ€§è¯„ä¼°:")
    print("   1. ä¸“ç”¨é€‚é…å™¨: 7ä¸ªæ¨¡å‹é’ˆå¯¹æ€§ä¼˜åŒ– âœ“")
    print("   2. é€šç”¨é€‚é…å™¨: æ”¯æŒä¸»æµAIæœåŠ¡API âœ“") 
    print("   3. æ™ºèƒ½è·¯ç”±: ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æ¨¡å‹ âœ“")
    print("   4. é”™è¯¯å¤„ç†: å®Œæ•´çš„é‡è¯•å’Œé™çº§æœºåˆ¶ âœ“")
    print("   5. Tokenç®¡ç†: ç»Ÿä¸€çš„ä½¿ç”¨é‡ç»Ÿè®¡ âœ“")
    
    print(f"\\nğŸ”§ æŠ€æœ¯å®ç°ç‰¹ç‚¹:")
    print("   â€¢ åŸºäºLangChainæ ¸å¿ƒæ¥å£ç»Ÿä¸€å°è£…")
    print("   â€¢ æ”¯æŒOpenAIæ ‡å‡†å’Œå„å‚å•†åŸç”ŸAPI")
    print("   â€¢ é’ˆå¯¹ä»£ç ã€å¯¹è¯ã€æ¨ç†ç­‰ä»»åŠ¡ä¼˜åŒ–")
    print("   â€¢ å…·å¤‡å®Œæ•´çš„é…ç½®å’Œç›‘æ§èƒ½åŠ›")
    
    print(f"\\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("   1. ç¡®ä¿å®‰è£…LangChainç›¸å…³ä¾èµ–")
    print("   2. é…ç½®å„å‚å•†çš„APIå¯†é’¥")
    print("   3. æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©åˆé€‚çš„é€‚é…å™¨")
    print("   4. ç›‘æ§Tokenä½¿ç”¨é‡å’ŒAPIè°ƒç”¨æˆåŠŸç‡")
    
    print(f"\\nğŸ¯ æ€»ç»“:")
    print("   é€‚é…å™¨æ¶æ„è®¾è®¡å®Œå–„ï¼Œæ”¯æŒ9å¤§æ¨¡å‹çš„æ™ºèƒ½è°ƒç”¨ï¼Œ")
    print("   å…·å¤‡å®Œæ•´çš„å…¼å®¹æ€§å’Œå®¹é”™èƒ½åŠ›ï¼Œå¯ä»¥æŠ•å…¥ç”Ÿäº§ä½¿ç”¨ã€‚")

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ TradingAgents æ¨¡å‹é€‚é…å™¨å…¼å®¹æ€§æ·±åº¦åˆ†æ")
    print("=" * 70)
    
    # 1. åˆ†æä¸“ç”¨é€‚é…å™¨
    adapter_results = analyze_specialized_adapters()
    
    # 2. åˆ†æå…¶ä»–é€‚é…å™¨
    analyze_other_adapters()
    
    # 3. æ£€æŸ¥å…¼å®¹æ€§é—®é¢˜
    check_compatibility_issues()
    
    # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    generate_final_report()
    
    # ä¿å­˜åˆ†æç»“æœ
    output_file = "/root/TradingAgents/data/model_adapter_analysis.json"
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(adapter_results, f, ensure_ascii=False, indent=2)
        print(f"\\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        print(f"\\nâš ï¸ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")

if __name__ == "__main__":
    main()