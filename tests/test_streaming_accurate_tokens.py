#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æµå¼å“åº”ä¸‹çš„å‡†ç¡®tok        # 3. åˆå§‹åŒ–OpenAIé€‚é…å™¨ï¼Œå¼ºåˆ¶æµå¼
        llm = ThirdPartyOpenAI(
            model=os.getenv('OPENAI_MODEL_NAME', 'Qwen/Qwen3-235B-A22B-Instruct-2507'),  # ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹å
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=os.getenv('OPENAI_API_BASE', 'https://llm.submodel.ai/v1'),
            temperature=0.7,
            stream=True  # ğŸ”‘ å¼ºåˆ¶ä½¿ç”¨æµå¼
        )æ¡ˆ3ï¼šæµå¼è¯·æ±‚åé€šè¿‡å®Œæ•´å¯¹è¯è·å–100%å‡†ç¡®çš„tokenæ•°é‡
"""

import os
import sys
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from tradingagents.config.config_manager import config_manager, token_tracker
from tradingagents.utils.logging_manager import get_logger
from tradingagents.llm_adapters.third_party_openai import ThirdPartyOpenAI

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)-20s | %(levelname)-8s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

logger = logging.getLogger('stream_test')

def test_streaming_accurate_tokens():
    """æµ‹è¯•æµå¼å“åº”çš„å‡†ç¡®tokenç»Ÿè®¡"""
    
    print("ğŸ¯ æµå¼å“åº”å‡†ç¡®Tokenç»Ÿè®¡æµ‹è¯•")
    print("=" * 50)
    
    try:
        # 1. åˆå§‹åŒ–é…ç½®
        logger.info("âœ… é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. æ˜¾ç¤ºæµ‹è¯•å‰ç»Ÿè®¡
        def display_current_statistics():
            """æ˜¾ç¤ºå½“å‰ç»Ÿè®¡ä¿¡æ¯"""
            try:
                stats = config_manager.get_usage_statistics(7)
                logger.info(f"ğŸ“Š æœ€è¿‘7å¤©ç»Ÿè®¡:")
                logger.info(f"   ğŸ’° æ€»æˆæœ¬: Â¥{stats['total_cost']:.6f}")
                logger.info(f"   ğŸ“ æ€»è¯·æ±‚: {stats['total_requests']}")
                logger.info(f"   ğŸ“¥ è¾“å…¥tokens: {stats['total_input_tokens']:,}")
                logger.info(f"   ğŸ“¤ è¾“å‡ºtokens: {stats['total_output_tokens']:,}")
                
                provider_stats = stats.get('provider_stats', {})
                if provider_stats:
                    logger.info(f"   ğŸ“ˆ ä¾›åº”å•†ç»Ÿè®¡:")
                    for provider, provider_info in provider_stats.items():
                        cost = provider_info.get('cost', 0)
                        requests = provider_info.get('requests', 0)
                        logger.info(f"      {provider}: Â¥{cost:.6f} ({requests}æ¬¡è¯·æ±‚)")
                        
            except Exception as e:
                logger.error(f"âŒ æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        
        print("ğŸ“Š æµ‹è¯•å‰ç»Ÿè®¡ä¿¡æ¯:")
        display_current_statistics()
        before_records = len(config_manager.load_usage_records())
        
        # 3. åˆå§‹åŒ–OpenAIé€‚é…å™¨ï¼Œå¼ºåˆ¶æµå¼
        llm = ThirdPartyOpenAI(
            api_key=os.getenv('OPENAI_API_KEY'),
            api_base=os.getenv('OPENAI_API_BASE', 'https://llm.submodel.ai/v1'),
            model_name=os.getenv('OPENAI_MODEL_NAME', 'Qwen/Qwen3-235B-A22B-Instruct-2507'),
            temperature=0.7,
            stream=True  # ğŸ”‘ å¼ºåˆ¶ä½¿ç”¨æµå¼
        )
        
        logger.info("ğŸš€ åˆå§‹åŒ–æµå¼OpenAIé€‚é…å™¨å®Œæˆ")
        
        # 4. å‘é€æµ‹è¯•è¯·æ±‚ï¼ˆå¼ºåˆ¶æµå¼ï¼‰
        session_id = f"stream_test_{hash(datetime.now())}"
        test_query = "ç®€è¦åˆ†ææ¯”ç‰¹å¸ç›®å‰çš„å¸‚åœºè¶‹åŠ¿ï¼Œä¸è¶…è¿‡50å­—ã€‚"
        
        logger.info(f"ğŸ“ ä¼šè¯ID: {session_id}")
        logger.info(f"ğŸš€ å‘é€æµå¼æµ‹è¯•è¯·æ±‚...")
        logger.info(f"ğŸ“Š æµ‹è¯•å‰è®°å½•æ•°: {before_records}")
        
        # å‘é€è¯·æ±‚
        from langchain_core.messages import HumanMessage
        messages = [HumanMessage(content=test_query)]
        
        response = llm._direct_api_call(
            messages=messages,
            stream=True  # ğŸ”‘ å¼ºåˆ¶æµå¼
        )
        
        logger.info(f"âœ… æ”¶åˆ°æµå¼å“åº”:")
        logger.info(f"   {response}")
        
        # 5. ç­‰å¾…tokenç»Ÿè®¡å®Œæˆï¼ˆå¼‚æ­¥è¿›è¡Œï¼‰
        import time
        time.sleep(3)
        
        # 6. éªŒè¯tokenç»Ÿè®¡ç»“æœ
        after_records = len(config_manager.load_usage_records())
        logger.info(f"ğŸ“Š æµ‹è¯•åè®°å½•æ•°: {after_records}")
        
        if after_records > before_records:
            # è·å–æœ€æ–°è®°å½•
            all_records = config_manager.load_usage_records()
            new_record = all_records[-1]
            logger.info(f"ğŸ“Š æ–°å¢è®°å½•:")
            logger.info(f"   ä¾›åº”å•†: {new_record.provider}")
            logger.info(f"   æ¨¡å‹: {new_record.model_name}")
            logger.info(f"   è¾“å…¥tokens: {new_record.input_tokens}")
            logger.info(f"   è¾“å‡ºtokens: {new_record.output_tokens}")
            logger.info(f"   æ€»tokens: {new_record.total_tokens}")
            logger.info(f"   æˆæœ¬: Â¥{new_record.cost:.6f}")
            logger.info(f"   ä¼šè¯ID: {new_record.session_id}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºçœŸå®tokenæ•°æ®ï¼ˆéé»˜è®¤ä¼°ç®—å€¼ï¼‰
            input_tokens = new_record.input_tokens
            output_tokens = new_record.output_tokens
            
            if input_tokens > 0 and output_tokens > 0:
                if input_tokens != 8000 and output_tokens != 4000:  # éé»˜è®¤ä¼°ç®—å€¼
                    logger.info("âœ… Tokenç”¨é‡æ•°æ®è·å–æˆåŠŸä¸”ä¸ºå‡†ç¡®å€¼ï¼ˆéä¼°ç®—ï¼‰")
                    print("\nğŸ‰ æµå¼å“åº”å‡†ç¡®Tokenç»Ÿè®¡æµ‹è¯•æˆåŠŸï¼")
                    print("âœ… ç³»ç»Ÿèƒ½å¤Ÿåœ¨æµå¼å“åº”åè·å–100%å‡†ç¡®çš„Tokenç”¨é‡")
                    print(f"ğŸ“Š å‡†ç¡®tokenç»Ÿè®¡: è¾“å…¥{input_tokens}, è¾“å‡º{output_tokens}")
                else:
                    logger.warning("âš ï¸ å¯èƒ½ä½¿ç”¨äº†ä¼°ç®—å€¼")
                    print("\nâš ï¸ æµ‹è¯•ç»“æœï¼šä½¿ç”¨äº†ä¼°ç®—å€¼ï¼Œé100%å‡†ç¡®")
            else:
                logger.error("âŒ Tokenç”¨é‡æ•°æ®å¼‚å¸¸")
                print("\nâŒ æµ‹è¯•å¤±è´¥ï¼šTokenç”¨é‡æ•°æ®å¼‚å¸¸")
        else:
            logger.error("âŒ æœªæ£€æµ‹åˆ°æ–°çš„ç”¨é‡è®°å½•")
            print("\nâŒ æµ‹è¯•å¤±è´¥ï¼šæœªæ£€æµ‹åˆ°æ–°çš„ç”¨é‡è®°å½•")
        
        # 7. æ˜¾ç¤ºæµ‹è¯•åç»Ÿè®¡
        print("\nğŸ“ˆ æµ‹è¯•åç»Ÿè®¡ä¿¡æ¯:")
        display_current_statistics()
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    try:
        # åŠ è½½ç¯å¢ƒå˜é‡
        from dotenv import load_dotenv
        load_dotenv()
        
        if not os.getenv('OPENAI_API_KEY'):
            logger.error("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
            sys.exit(1)
        
        logger.info("âœ… ç¯å¢ƒé…ç½®åŠ è½½æˆåŠŸ")
        
        # è¿è¡Œæµ‹è¯•
        test_result = test_streaming_accurate_tokens()
        
        print("\n" + "=" * 50)
        print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
        if test_result:
            print("ğŸ‰ æµå¼å“åº”å‡†ç¡®Tokenç»Ÿè®¡æµ‹è¯•å®Œæˆï¼")
            print("âœ… éªŒè¯äº†æ–¹æ¡ˆ3ï¼šæµå¼è¯·æ±‚+å®Œæ•´å¯¹è¯tokenç»Ÿè®¡")
        else:
            print("âŒ æµ‹è¯•å¤±è´¥")
        
        print("\nğŸ“š ç›¸å…³æ–‡ä»¶:")
        print("   - é…ç½®: .env")
        print("   - è®°å½•: config/usage.json")
        print("   - ä»£ç : tradingagents/llm_adapters/third_party_openai.py")
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æµ‹è¯•è¢«ç”¨æˆ·å–æ¶ˆ")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()