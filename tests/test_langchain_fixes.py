#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éªŒè¯LangChainæµå¼æ”¯æŒä¿®å¤æ•ˆæžœ
æµ‹è¯•ä¿®å¤åŽçš„å‚æ•°å¤„ç†å’Œæµå¼æ£€æµ‹é€»è¾‘
"""

import os
import sys
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from tradingagents.config.config_manager import config_manager, token_tracker
from tradingagents.utils.logging_manager import get_logger
from tradingagents.llm_adapters.third_party_openai import ThirdPartyOpenAI

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)-20s | %(levelname)-8s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

logger = get_logger('fix_test')

def test_langchain_streaming_fixes():
    """æµ‹è¯•LangChainæµå¼æ”¯æŒä¿®å¤æ•ˆæžœ"""
    
    print("ðŸ”§ LangChainæµå¼æ”¯æŒä¿®å¤æ•ˆæžœéªŒè¯")
    print("=" * 60)
    
    try:
        # 1. æµ‹è¯•å‚æ•°å†²çªä¿®å¤
        print("\n1ï¸âƒ£ æµ‹è¯•å‚æ•°å†²çªä¿®å¤...")
        
        # æµ‹è¯•åŒæ—¶è®¾ç½®streamingå’Œstreamå‚æ•°
        llm_conflict = ThirdPartyOpenAI(
            model=os.getenv('OPENAI_MODEL_NAME', 'Qwen/Qwen3-235B-A22B-Instruct-2507'),
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=os.getenv('OPENAI_API_BASE', 'https://llm.submodel.ai/v1'),
            streaming=True,   # LangChainæ ‡å‡†å‚æ•°
            stream=True,      # å¯èƒ½çš„å†²çªå‚æ•°
            temperature=0.7
        )
        
        logger.info(f"âœ… å‚æ•°å†²çªå¤„ç†å®Œæˆï¼Œæ— å¼‚å¸¸")
        logger.info(f"ðŸ” user_wants_streaming: {getattr(llm_conflict, 'user_wants_streaming', 'N/A')}")
        
        # 2. æµ‹è¯•æµå¼æ£€æµ‹é€»è¾‘
        print("\n2ï¸âƒ£ æµ‹è¯•æµå¼æ£€æµ‹é€»è¾‘...")
        
        from langchain_core.messages import HumanMessage
        test_query = "ç®€è¦è¯´æ˜Žäº‘è®¡ç®—æ ¸å¿ƒä¼˜åŠ¿ï¼Œä¸è¶…è¿‡15å­—ã€‚"
        messages = [HumanMessage(content=test_query)]
        
        logger.info(f"ðŸ“ æµ‹è¯•æ¶ˆæ¯: {test_query}")
        
        before_count = len(config_manager.load_usage_records())
        result = llm_conflict.generate([messages])
        after_count = len(config_manager.load_usage_records())
        
        logger.info(f"âœ… æµå¼å“åº”: {result.generations[0][0].text}")
        logger.info(f"ðŸ“Š æ–°å¢žè®°å½•æ•°: {after_count - before_count}")
        
        # æ£€æŸ¥æœ€æ–°è®°å½•
        if after_count > before_count:
            latest_record = config_manager.load_usage_records()[-1]
            logger.info(f"ðŸ“Š Tokenç»Ÿè®¡: è¾“å…¥={latest_record.input_tokens}, è¾“å‡º={latest_record.output_tokens}")
            
            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨äº†æ–¹æ¡ˆ3ï¼ˆå‡†ç¡®ç»Ÿè®¡ï¼‰
            if latest_record.input_tokens > 0 and latest_record.output_tokens > 0:
                if (10 <= latest_record.input_tokens <= 100) and (5 <= latest_record.output_tokens <= 50):
                    logger.info("âœ… æ–¹æ¡ˆ3æ­£å¸¸å·¥ä½œï¼ŒèŽ·å¾—å‡†ç¡®tokenç»Ÿè®¡")
                else:
                    logger.warning("âš ï¸ Tokenæ•°å€¼å¼‚å¸¸ï¼Œå¯èƒ½æ˜¯ä¼°ç®—å€¼")
            else:
                logger.error("âŒ Tokenç»Ÿè®¡å¤±è´¥")
        
        # 3. æµ‹è¯•éžæµå¼æ¨¡å¼
        print("\n3ï¸âƒ£ æµ‹è¯•éžæµå¼æ¨¡å¼...")
        
        llm_non_stream = ThirdPartyOpenAI(
            model=os.getenv('OPENAI_MODEL_NAME', 'Qwen/Qwen3-235B-A22B-Instruct-2507'),
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=os.getenv('OPENAI_API_BASE', 'https://llm.submodel.ai/v1'),
            streaming=False,  # æ˜Žç¡®å…³é—­æµå¼
            temperature=0.7
        )
        
        test_query2 = "ç®€è¦è¯´æ˜Žå¤§æ•°æ®æŠ€æœ¯ä»·å€¼ï¼Œä¸è¶…è¿‡15å­—ã€‚"
        messages2 = [HumanMessage(content=test_query2)]
        
        logger.info(f"ðŸ“ æµ‹è¯•æ¶ˆæ¯: {test_query2}")
        logger.info(f"ðŸ” user_wants_streaming: {getattr(llm_non_stream, 'user_wants_streaming', 'N/A')}")
        
        before_count2 = len(config_manager.load_usage_records())
        result2 = llm_non_stream.generate([messages2])
        after_count2 = len(config_manager.load_usage_records())
        
        logger.info(f"âœ… éžæµå¼å“åº”: {result2.generations[0][0].text}")
        logger.info(f"ðŸ“Š æ–°å¢žè®°å½•æ•°: {after_count2 - before_count2}")
        
        # 4. æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆ
        print("\n4ï¸âƒ£ æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆ...")
        
        test_configs = [
            {"streaming": True, "stream": False, "expected": True},
            {"streaming": False, "stream": True, "expected": True}, 
            {"streaming": False, "stream": False, "expected": False},
            {"streaming": True, "expected": True},
            {"stream": True, "expected": True},
            {"expected": False}  # é»˜è®¤é…ç½®
        ]
        
        for i, config in enumerate(test_configs, 1):
            expected = config.pop('expected')
            
            try:
                test_llm = ThirdPartyOpenAI(
                    model="test-model",
                    api_key="test-key",
                    **config
                )
                
                actual = getattr(test_llm, 'user_wants_streaming', False)
                status = "âœ…" if actual == expected else "âŒ"
                
                logger.info(f"{status} é…ç½®{i}: {config} â†’ user_wants_streaming={actual} (æœŸæœ›={expected})")
                
            except Exception as e:
                logger.error(f"âŒ é…ç½®{i}æµ‹è¯•å¤±è´¥: {e}")
        
        # 5. éªŒè¯è­¦å‘Šæ¶ˆé™¤
        print("\n5ï¸âƒ£ éªŒè¯è­¦å‘Šæ¶ˆé™¤...")
        
        import warnings
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            
            test_llm = ThirdPartyOpenAI(
                model="test-model",
                api_key="test-key",
                streaming=True,
                stream=True
            )
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰streamå‚æ•°çš„è­¦å‘Š
            stream_warnings = [warning for warning in w if 'stream' in str(warning.message).lower()]
            
            if stream_warnings:
                logger.warning(f"âš ï¸ ä»æœ‰{len(stream_warnings)}ä¸ªstreamç›¸å…³è­¦å‘Š")
                for warning in stream_warnings:
                    logger.warning(f"   {warning.message}")
            else:
                logger.info("âœ… æˆåŠŸæ¶ˆé™¤streamå‚æ•°è­¦å‘Š")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    try:
        # åŠ è½½çŽ¯å¢ƒå˜é‡
        from dotenv import load_dotenv
        load_dotenv()
        
        if not os.getenv('OPENAI_API_KEY'):
            logger.error("âŒ è¯·è®¾ç½® OPENAI_API_KEY çŽ¯å¢ƒå˜é‡")
            sys.exit(1)
        
        logger.info("âœ… çŽ¯å¢ƒé…ç½®åŠ è½½æˆåŠŸ")
        
        # è¿è¡Œæµ‹è¯•
        test_result = test_langchain_streaming_fixes()
        
        print("\n" + "=" * 60)
        print("ðŸ“‹ ä¿®å¤éªŒè¯æ€»ç»“:")
        
        if test_result:
            print("âœ… LangChainæµå¼æ”¯æŒä¿®å¤éªŒè¯å®Œæˆ")
            print("ðŸ”§ ä¿®å¤çš„é—®é¢˜:")
            print("   1. å‚æ•°å†²çª - streaming vs stream")
            print("   2. æµå¼æ£€æµ‹é€»è¾‘ä¼˜åŒ–")
            print("   3. åˆå§‹åŒ–æ—¥å¿—æ”¹è¿›")
            print("   4. è­¦å‘Šæ¶ˆæ¯å¤„ç†")
        else:
            print("âŒ ä¿®å¤éªŒè¯ä¸­å‡ºçŽ°é”™è¯¯")
        
        print(f"\nðŸ“š ç›¸å…³æ–‡ä»¶:")
        print(f"   - ä»£ç : tradingagents/llm_adapters/third_party_openai.py")
        
    except KeyboardInterrupt:
        print(f"\n\nðŸ‘‹ æµ‹è¯•è¢«ç”¨æˆ·å–æ¶ˆ")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()