"""
ç®€åŒ–çš„LLMæ¨¡å‹è·å–å·¥å…·
ä»ç¬¬ä¸‰æ–¹æœåŠ¡è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
"""

import requests
import logging
import streamlit as st
from typing import List, Optional
import os

logger = logging.getLogger(__name__)

class ModelFetcher:
    """ç®€åŒ–çš„LLMæ¨¡å‹è·å–å™¨"""
    
    def __init__(self):
        self.base_url = "https://llm.submodel.ai/v1"
        self.api_key = os.getenv("OPENAI_API_KEY")
        
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹IDåˆ—è¡¨"""
        try:
            if not self.api_key:
                logger.error("âŒ APIå¯†é’¥æœªé…ç½®")
                return ['deepseek-ai/DeepSeek-V3.1']  # è¿”å›é»˜è®¤æ¨¡å‹
            
            # æ£€æŸ¥ç¼“å­˜
            if hasattr(st.session_state, 'cached_models') and st.session_state.cached_models:
                logger.debug(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹åˆ—è¡¨ï¼Œå…±{len(st.session_state.cached_models)}ä¸ªæ¨¡å‹")
                return st.session_state.cached_models
            
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {self.api_key}'
            }
            
            logger.info("ğŸ” æ­£åœ¨è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨...")
            response = requests.get(f"{self.base_url}/models", headers=headers, timeout=20)
            
            if response.status_code == 200:
                data = response.json()
                models = data.get('data', [])
                
                # æå–æ¨¡å‹IDåˆ—è¡¨
                model_ids = []
                for model in models:
                    model_id = model.get('id', '')
                    if model_id:  # ç¡®ä¿æ¨¡å‹IDä¸ä¸ºç©º
                        model_ids.append(model_id)
                
                # æ’åº
                model_ids.sort()
                
                # ç¼“å­˜ç»“æœ
                st.session_state.cached_models = model_ids
                
                logger.info(f"âœ… æˆåŠŸè·å–{len(model_ids)}ä¸ªå¯ç”¨æ¨¡å‹")
                return model_ids
            else:
                logger.error(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: HTTP {response.status_code}")
                return ['deepseek-ai/DeepSeek-V3.1']  # è¿”å›é»˜è®¤æ¨¡å‹
                
        except requests.exceptions.Timeout:
            logger.error("â° è·å–æ¨¡å‹åˆ—è¡¨è¶…æ—¶")
            return ['deepseek-ai/DeepSeek-V3.1']
        except requests.exceptions.RequestException as e:
            logger.error(f"ğŸŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return ['deepseek-ai/DeepSeek-V3.1']
        except Exception as e:
            logger.error(f"ğŸ’¥ è·å–æ¨¡å‹åˆ—è¡¨å‡ºç°å¼‚å¸¸: {e}")
            return ['deepseek-ai/DeepSeek-V3.1']
    
    def get_default_model(self) -> str:
        """è·å–é»˜è®¤æ¨¡å‹"""
        models = self.get_available_models()
        
        # ä¼˜å…ˆé€‰æ‹©DeepSeekæ¨¡å‹
        for model in models:
            if 'deepseek' in model.lower():
                return model
        
        # å¦‚æœæ²¡æœ‰DeepSeekï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
        if models:
            return models[0]
        
        # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨å›ºå®šçš„é»˜è®¤æ¨¡å‹
        return 'deepseek-ai/DeepSeek-V3.1'
    
    def refresh_models(self):
        """åˆ·æ–°æ¨¡å‹åˆ—è¡¨ï¼ˆæ¸…é™¤ç¼“å­˜ï¼‰"""
        if hasattr(st.session_state, 'cached_models'):
            del st.session_state.cached_models
        logger.info("ğŸ”„ å·²æ¸…é™¤æ¨¡å‹åˆ—è¡¨ç¼“å­˜")

# åˆ›å»ºå…¨å±€å®ä¾‹
model_fetcher = ModelFetcher()